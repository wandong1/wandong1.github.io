[{"content":"hexo博客框架的使用 安装node.js 官网： https://nodejs.org/en/\n安装国内淘宝npm npm install -g cnpm --registry=https://registry.npm.taobao.org 安装hexo cnpm install -g hexo-cli\rhexo -v hexo初始化 hexo init 目录 hexo server 新建文章 文章都在source\\_posts目录下\nhexo new \u0026#34;我的第一篇博客文章\u0026#34; 配置后刷新并重启服务 hexo clean hexo g hexo server 更换主题 进入项目目录\ngit clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改项目目录下配置文件_config.yml\ntheme: yilia 修复更换主题后全部文章无法显示问题 # 进入项目目录后 cnpm i hexo-generator-json-content --save # 随后在项目目录下的_config.yml文件后添加内容 jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 刷新并重启项目\n配置yilia主题显示文章目录 进入 themes\\yilia目录下，修改_config.yml文件\ntoc: 2 处理文章图片不显示问题 查看hexo版本，现在我安装的 hexo 版本已经是 4.3.0 了 (2022年)，该方法适用\nhexo version 进入项目目录执行命令\n# 可用cnpm替换 npm install hexo-asset-image --save 插件bug问题，需要做如下修改 进入你博客的根目录，然后下面顺序找到index.js: node_modules \u0026ndash;\u0026gt; hexo-asset-image \u0026ndash;\u0026gt; index.js 用VS Code 或者 记事本打开 index.js 在第 58 行，可以找到这么一行代码： $(this).attr(\u0026#39;src\u0026#39;, config.root + link + src); 把这一行代码改成下面这样 $(this).attr(\u0026#39;src\u0026#39;, src); 随后保存文件\n插入图片的用法 创建文章时，现在插件会自动在文章的_posts目录下创建md文件名相同的目录以存放图片。当然也可以手动创建目录和md文件。\n在文章中写如下内容便可插入图片（注意圆括号内无需写路径，仅写图片的全名即可，前提是图片在对应的文章的目录下）：\n![image-20220809152616143](image-20220809152616143.png) ","permalink":"https://wandong1.github.io/post/hexo%E5%8D%9A%E5%AE%A2%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/","summary":"hexo博客框架的使用 安装node.js 官网： https://nodejs.org/en/\n安装国内淘宝npm npm install -g cnpm --registry=https://registry.npm.taobao.org 安装hexo cnpm install -g hexo-cli\rhexo -v hexo初始化 hexo init 目录 hexo server 新建文章 文章都在source\\_posts目录下\nhexo new \u0026#34;我的第一篇博客文章\u0026#34; 配置后刷新并重启服务 hexo clean hexo g hexo server 更换主题 进入项目目录\ngit clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改项目目录下配置文件_config.yml\ntheme: yilia 修复更换主题后全部文章无法显示问题 # 进入项目目录后 cnpm i hexo-generator-json-content --save # 随后在项目目录下的_config.yml文件后添加内容 jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 刷新并重启项目","title":"hexo博客框架的使用"},{"content":"\r各类别常见状态码：\n2xx （3种） 200 OK：表示从客户端发送给服务器的请求被正常处理并返回；\n204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；\n206 Patial Content：表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求，响应报文中包含由Content-Range指定范围的实体内容。\n3xx （5种） 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；\n302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；\n301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）\n303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；\n302与303的区别：后者明确表示客户端应当采用GET方式获取资源 304 Not Modified：表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；\n307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；\n4xx （4种） 400 Bad Request：表示请求报文中存在语法错误；\n401 Unauthorized：未经许可，需要通过HTTP认证；\n403 Forbidden：服务器拒绝该次访问（访问权限出现问题）\n404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；\n429 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用。\n5xx （2种） 500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；\n502 bad gateway 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；\n","permalink":"https://wandong1.github.io/post/http%E5%B8%B8%E8%A7%81%E7%8A%B6%E6%80%81%E7%A0%81/","summary":"各类别常见状态码：\n2xx （3种） 200 OK：表示从客户端发送给服务器的请求被正常处理并返回；\n204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；\n206 Patial Content：表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求，响应报文中包含由Content-Range指定范围的实体内容。\n3xx （5种） 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；\n302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；\n301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）\n303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；\n302与303的区别：后者明确表示客户端应当采用GET方式获取资源 304 Not Modified：表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；\n307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；\n4xx （4种） 400 Bad Request：表示请求报文中存在语法错误；\n401 Unauthorized：未经许可，需要通过HTTP认证；\n403 Forbidden：服务器拒绝该次访问（访问权限出现问题）\n404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；\n429 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用。\n5xx （2种） 500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；\n502 bad gateway 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；","title":"http常见状态码"},{"content":"下载hugo二进制程序包 下载地址： https://github.com/gohugoio/hugo/releases\n下载后解压、将hugo路径添加到环境变量。先设置hugo变量，然后在path中添加\n验证安装 hugo version 新建站点 hugo new site myblog # 该命令会新建一个文件夹myblog ls ./myblog # archetypes/ config.toml content/ data/ # layouts/ static/ themes/ ##我目前了解如下 #config.toml 进行参数配置，与之后的theme相关 #content 之后博客(.md)的文件都储存在这里 #layout 可个性化修改博客的展示细节，需要懂网络架构知识 #static 储存一些静态文件，比如本地图片，插入到博客中 #themes 主题，接下来会介绍 下载主题（hugo没有默认主题） 有多种hugo主题可供下载：https://themes.gohugo.io/ 推荐主题： https://adityatelange.github.io/hugo-PaperMod/\ncd ./myblog git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod ls ./themes # PaperMod/ ls ./themes/PaperMod/ # LICENSE README.md assets/ go.mod i18n/ images/ layouts/ theme.toml 修改配置 papermod\n通用配置参数查询：https://gohugo.io/getting-started/configuration/\nPaperMod自定义参数查询：https://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n示例配置：https://www.sulvblog.cn/posts/blog/build_hugo/#4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6\n将 config.toml 重命名为 config.yml 然后进行修改\nbaseURL: https://www.sulvblog.cn # baseURL: https://www.sulvblog.cn # 绑定的域名 languageCode: zh-cn # en-us title: 万东的云计算运维博客 theme: hugo-PaperMod # 主题名字，和themes文件夹下的一致 enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false googleAnalytics: UA-123-45 minify: disableXML: true minifyOutput: true params: env: production # to enable google analytics, opengraph, twitter-cards and schema. title: ExampleSite description: \u0026#34;ExampleSite description\u0026#34; keywords: [Blog, Portfolio, PaperMod] author: Me # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;] DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: dark # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false showtoc: true tocopen: true searchHidden: true assets: # disableHLJS: true # to disable highlight.js # disableFingerprinting: true favicon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon16x16: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon32x32: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; apple_touch_icon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; safari_pinned_tab: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; label: text: \u0026#34;万东的云计算运维博客\u0026#34; icon: /apple-touch-icon.png iconHeight: 35 # profile-mode profileMode: enabled: false # needs to be explicitly set title: ExampleSite subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;img location\u0026gt;\u0026#34; imageWidth: 120 imageHeight: 120 imageTitle: my image buttons: - name: 文章 url: posts - name: 标签 url: tags # home-info mode homeInfoParams: Title: \u0026#34;技术学习笔记 \\U0001F44B\u0026#34; Content: 学无止境 socialIcons: - name: github url: \u0026#34;https://github.com/wandong1\u0026#34; analytics: google: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; bing: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; yandex: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: 搜索 name: 搜索 url: search weight: 80 - identifier: 分类 name: 分类 url: /categories/ weight: 10 - identifier: 标签 name: 标签 url: /tags/ weight: 20 - identifier: 归档 name: 归档 url: /archives/ weight: 41 # Read: https://github.com/adityatelange/hugo-PaperMod/wiki/FAQs#using-hugos-syntax-highlighter-chroma pygmentsUseClasses: true markup: highlight: noClasses: false # anchorLineNos: true # codeFences: true # guessSyntax: true # lineNos: true # style: monokai outputs: home: - HTML - RSS - JSON # is necessary 搜索 和 归档功能需要在content目录下创建对应的md文件\n$ cat archive.md\n--- title: \u0026#34;文章归档\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archives/\u0026#34; summary: \u0026#34;archives\u0026#34; --- $ cat search.md\n--- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; --- 个性化修改 转移目录至侧边栏 Pull Request #675 · adityatelange/hugo-PaperMod 提出将文章目录转移至侧边栏，可以轻松实现上下文跳转。截至发文这一特性还未并入主分支，我们可以让主题子模块追踪该远程 PR 分支：\ncd themes/PaperMod git fetch origin pull/675/head:toc-on-the-side --depth=1 git checkout toc-on-the-side cd ../.. 注意：文章名称中的英文字母使用小写，否则图片可能会显示不出来 在本地启动项目 hugo server -D 把更新的博文更新到public目录 hugo --theme=hugo-PaperMod --baseUrl=\u0026#34;https://wandong1.github.io\u0026#34; --buildDrafts ","permalink":"https://wandong1.github.io/post/hugo-%E5%8D%9A%E5%AE%A2%E7%A8%8B%E5%BA%8F%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/","summary":"下载hugo二进制程序包 下载地址： https://github.com/gohugoio/hugo/releases\n下载后解压、将hugo路径添加到环境变量。先设置hugo变量，然后在path中添加\n验证安装 hugo version 新建站点 hugo new site myblog # 该命令会新建一个文件夹myblog ls ./myblog # archetypes/ config.toml content/ data/ # layouts/ static/ themes/ ##我目前了解如下 #config.toml 进行参数配置，与之后的theme相关 #content 之后博客(.md)的文件都储存在这里 #layout 可个性化修改博客的展示细节，需要懂网络架构知识 #static 储存一些静态文件，比如本地图片，插入到博客中 #themes 主题，接下来会介绍 下载主题（hugo没有默认主题） 有多种hugo主题可供下载：https://themes.gohugo.io/ 推荐主题： https://adityatelange.github.io/hugo-PaperMod/\ncd ./myblog git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod ls ./themes # PaperMod/ ls ./themes/PaperMod/ # LICENSE README.md assets/ go.mod i18n/ images/ layouts/ theme.toml 修改配置 papermod\n通用配置参数查询：https://gohugo.io/getting-started/configuration/\nPaperMod自定义参数查询：https://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n示例配置：https://www.sulvblog.cn/posts/blog/build_hugo/#4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6\n将 config.toml 重命名为 config.yml 然后进行修改","title":"Hugo 博客程序搭建教程"},{"content":"生成管理员证书 cat \u0026gt; admin-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 执行生成命令 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kubeconfig文件 # 设置集群参数 kubectl config set-cluster kubernetes \\ --server=https://192.168.0.149:6443 \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --kubeconfig=config # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=cluster-admin \\ --kubeconfig=config # 设置客户端认证参数 kubectl config set-credentials cluster-admin \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --client-key=admin-key.pem \\ --client-certificate=admin.pem \\ --kubeconfig=config # 设置默认上下文 kubectl config use-context default --kubeconfig=config \u0026ndash;certificate-authority： 验证 kube-apiserver 证书的根证书； \u0026ndash;client-certificate、\u0026ndash;client-key： 刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用； \u0026ndash;embed-certs=true： 将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)；\n设置客户端认证参数时 \u0026ndash;certificate-authority=ca.pem ##添加管理员权限，没有这一段则为普通用户\n指定config配置文件执行命令 kubectl --kubeconfig=config get node ","permalink":"https://wandong1.github.io/post/k8s%E6%A0%B9%E6%8D%AE%E7%8E%B0%E6%9C%89%E8%AF%81%E4%B9%A6%E7%94%9F%E6%88%90%E7%AE%A1%E7%90%86%E5%91%98kubeconfig%E6%96%87%E4%BB%B6/","summary":"生成管理员证书 cat \u0026gt; admin-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 执行生成命令 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kubeconfig文件 # 设置集群参数 kubectl config set-cluster kubernetes \\ --server=https://192.168.0.149:6443 \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --kubeconfig=config # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=cluster-admin \\ --kubeconfig=config # 设置客户端认证参数 kubectl config set-credentials cluster-admin \\ --certificate-authority=ca.","title":"K8S根据现有证书生成管理员kubeconfig文件"},{"content":"软件包下载 https://www.mongodb.com/try/download/community wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-5.0.8.tgz mongodb模式介绍 三节点复制集模式 常见的复制集架构由3个成员节点组成，其中存在几种不同的模式。 PSS模式（官方推荐模式） PSS模式由一个主节点和两个备节点所组成，即Primary+Secondary+Secondary。 此模式始终提供数据集的两个完整副本，如果主节点不可用，则复制集选择备节点作为主节 点并继续正常操作。旧的主节点在可用时重新加入复制集。 复制集部署注意事项 关于硬件: 因为正常的复制集节点都有可能成为主节点，它们的地位是一样的，因此硬件配 置上必须一致; 为了保证节点不会同时宕机，各节点使用的硬件必须具有独立性。\n关于软件: 复制集各节点软件版本必须一致，以避免出现不可预知的问题。 增加节点不会增加系统写性能；\n准备配置文件 复制集的每个mongod进程应该位于不同的服务器。我们现在在一台机器上运行3个进程， 因此要为它们各自配置：\n不同的端口 （28017/28018/28019）\n不同的数据目录 mkdir ‐p /data/db{1,2,3} 不同日志文件路径 (例如：/data/db1/mongod.log)\n创建配置文件/data/db1/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db1/mongod.log logAppend: true storage: dbPath: /data/db1 net: bindIp: 0.0.0.0 port: 28017 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db2/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db2/mongod.log logAppend: true storage: dbPath: /data/db2 net: bindIp: 0.0.0.0 port: 28018 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db3/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db3/mongod.log logAppend: true storage: dbPath: /data/db3 net: bindIp: 0.0.0.0 port: 28019 replication: replSetName: rs0 processManagement: fork: true 启动mongdb mongod -f /data/db1/mongod.conf mongod -f /data/db2/mongod.conf mongod -f /data/db3/mongod.conf 初始化配置复制集 mongo --port 28017 rs.initiate({ _id:\u0026#34;rs0\u0026#34;, members:[{ _id:0, host:\u0026#34;localhost:28017\u0026#34; },{ _id:1, host:\u0026#34;localhost:28018\u0026#34; },{ _id:2, host:\u0026#34;localhost:28019\u0026#34; }] }) 开启安全认证 创建认证用户\nuse admin db.createUser( { user: \u0026#34;root\u0026#34;, pwd: \u0026#34;Aliyun2022\u0026#34;, roles: [ { role: \u0026#34;clusterAdmin\u0026#34;, db: \u0026#34;admin\u0026#34; } , { role: \u0026#34;userAdminAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}, { role: \u0026#34;userAdminAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}, { role: \u0026#34;readWriteAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}] }) 生成复制集间通信的认证key文件\nopenssl rand -base64 756 \u0026gt; /data/mongo.key 重新启动mongdb mongod -f /data/db1/mongod.conf --keyFile /data/mongo.key mongod -f /data/db2/mongod.conf --keyFile /data/mongo.key mongod -f /data/db3/mongod.conf --keyFile /data/mongo.key 使用账户密码登录 mongo \u0026ndash;port 28019 -uroot -pAliyun2022 \u0026ndash;authenticationDatabase=admin\n或者用uri方式登录 mongodb://root:Aliyun2022@192.168.65.174:28017,192.168.65.174:28018,192.168.65.174:28019/test?authSource=admin\u0026amp;replicaSet=rs0\n","permalink":"https://wandong1.github.io/post/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","summary":"软件包下载 https://www.mongodb.com/try/download/community wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-5.0.8.tgz mongodb模式介绍 三节点复制集模式 常见的复制集架构由3个成员节点组成，其中存在几种不同的模式。 PSS模式（官方推荐模式） PSS模式由一个主节点和两个备节点所组成，即Primary+Secondary+Secondary。 此模式始终提供数据集的两个完整副本，如果主节点不可用，则复制集选择备节点作为主节 点并继续正常操作。旧的主节点在可用时重新加入复制集。 复制集部署注意事项 关于硬件: 因为正常的复制集节点都有可能成为主节点，它们的地位是一样的，因此硬件配 置上必须一致; 为了保证节点不会同时宕机，各节点使用的硬件必须具有独立性。\n关于软件: 复制集各节点软件版本必须一致，以避免出现不可预知的问题。 增加节点不会增加系统写性能；\n准备配置文件 复制集的每个mongod进程应该位于不同的服务器。我们现在在一台机器上运行3个进程， 因此要为它们各自配置：\n不同的端口 （28017/28018/28019）\n不同的数据目录 mkdir ‐p /data/db{1,2,3} 不同日志文件路径 (例如：/data/db1/mongod.log)\n创建配置文件/data/db1/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db1/mongod.log logAppend: true storage: dbPath: /data/db1 net: bindIp: 0.0.0.0 port: 28017 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db2/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db2/mongod.log logAppend: true storage: dbPath: /data/db2 net: bindIp: 0.0.0.0 port: 28018 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db3/mongod.","title":"mongodb的安装部署"},{"content":"参考地址：https://www.bilibili.com/read/cv15128680\n","permalink":"https://wandong1.github.io/post/navicat15%E9%83%A8%E7%BD%B2/","summary":"参考地址：https://www.bilibili.com/read/cv15128680","title":"navicat15部署"},{"content":"user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } # 4层 stream配置 stream { log_format main \u0026#39;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent\u0026#39;; access_log /var/log/nginx/dingding-access.log main; upstream dingding { server oapi.dingtalk.com:80; } upstream dingding_v2 { server oapi.dingtalk.com:443; } upstream apsoar { server soar.apsoar.com:22; } upstream timor { server timor.tech:443; } upstream timor80 { server timor.tech:80; } server { listen 22; proxy_pass apsoar; } server { listen 7003; proxy_pass timor80; } server { listen 80; proxy_pass dingding; } server { listen 443; proxy_pass dingding_v2; } } # 7层http配置 http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; server { listen 9999 default_server; location / { proxy_pass http://timor.tech; } } server { listen 19999 default_server; location / { proxy_pass https://timor.tech; } } include /etc/nginx/conf.d/*.conf; } ","permalink":"https://wandong1.github.io/post/nginx%E7%9A%84%E5%9B%9B%E5%B1%82%E8%BD%AC%E5%8F%91%E5%92%8C%E4%B8%83%E5%B1%82%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/","summary":"user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } # 4层 stream配置 stream { log_format main \u0026#39;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent\u0026#39;; access_log /var/log/nginx/dingding-access.log main; upstream dingding { server oapi.dingtalk.com:80; } upstream dingding_v2 { server oapi.dingtalk.com:443; } upstream apsoar { server soar.apsoar.com:22; } upstream timor { server timor.tech:443; } upstream timor80 { server timor.tech:80; } server { listen 22; proxy_pass apsoar; } server { listen 7003; proxy_pass timor80; } server { listen 80; proxy_pass dingding; } server { listen 443; proxy_pass dingding_v2; } } # 7层http配置 http { include /etc/nginx/mime.","title":"nginx的四层转发和七层转发配置示例"},{"content":"一、优化nginx进程数以及cpu分布 修改配置文件 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 几核就有几位二进制数，1在哪位就表示在哪个核心上。\n查看nginx worker进程分布在cpu的情况 ps -axo pid,psr,cmd,ni | grep -i \u0026#34;worker process\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sort -n | uniq -c 二、优化文件数\nulimit -n #查看文件数限制 ulimit -SHn 65535 （注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) #有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535 nginx配置修改：\nworker_rlimit_nofile 65535; 三、使用epoll的I/O模型，用这个模型来高效处理异步事件 在events区块中添加 use epoll; 四、每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为 # worker_processes*worker_connections。为理论上最大连接数 在events区块中添加或修改 worker_connections 65535; 五、http连接超时时间 默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！\nkeepalive_timeout 60; 六、linux内核参数优化 cat \u0026gt; /etc/sysctl.d/youhua.conf \u0026lt;\u0026lt; EOF net.ipv4.tcp_syncookies=1 net.core.somaxconn=32768 net.ipv4.tcp_max_syn_backlog=32768 net.ipv4.ip_local_port_range=15000 65000 EOF sysctl --system [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_syncookies=1\u0026#34; [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_tw_reuse=1\u0026#34; [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_tw_recycle=1\u0026#34; 七、AB简单压测命令 yum -y install httpd-tools ab -n 500000 -c 2000 -k http://10.43.152.61/\nsysctl -w net.ipv4.tcp_max_tw_buckets=30000\njmeter压测配置 http请求\n","permalink":"https://wandong1.github.io/post/nginx%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/","summary":"一、优化nginx进程数以及cpu分布 修改配置文件 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 几核就有几位二进制数，1在哪位就表示在哪个核心上。\n查看nginx worker进程分布在cpu的情况 ps -axo pid,psr,cmd,ni | grep -i \u0026#34;worker process\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sort -n | uniq -c 二、优化文件数\nulimit -n #查看文件数限制 ulimit -SHn 65535 （注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) #有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535 nginx配置修改：\nworker_rlimit_nofile 65535; 三、使用epoll的I/O模型，用这个模型来高效处理异步事件 在events区块中添加 use epoll; 四、每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为 # worker_processes*worker_connections。为理论上最大连接数 在events区块中添加或修改 worker_connections 65535; 五、http连接超时时间 默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！","title":"nginx调优方法"},{"content":"一、prometheus监控 https://prometheus.io/download/ ###下载源码解压即可 https://grafana.com/grafana/dashboards ###搜索数据源为prometheus的\n安装docker mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce systemctl enable docker --now 安装grafana wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpm yum localinstall grafana-5.3.4-1.x86_64.rpm systemctl start grafana-server 默认密码 admin/admin prometheus安装 tar -xvzf prometheus-2.34.0.linux-amd64.tar.gz -C /opt/ mv /opt/prometheus-2.34.0.linux-amd64 /opt/prometheus cd /opt/prometheus \u0026amp;\u0026amp; mkdir data # 创建启动脚本 cat \u0026lt;\u0026lt;EOF \u0026gt;start.sh #!/bin/bash ./prometheus --storage.tsdb.path=./data --storage.tsdb.retention.time=744h --web.enable-lifecycle --storage.tsdb.no-lockfile EOF # storage.tsdb.retention.time为数据保存时间 注：创建data目录，最好单独挂载到一块盘\n修改Prometheus配置文件 # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;. rule_files: # - \u0026#34;first_rules.yml\u0026#34; # - \u0026#34;second_rules.yml\u0026#34; # A scrape configuration containing exactly one endpoint to scrape: # Here it\u0026#39;s Prometheus itself. scrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: \u0026#34;prometheus\u0026#34; # metrics_path defaults to \u0026#39;/metrics\u0026#39; # scheme defaults to \u0026#39;http\u0026#39;. static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] - job_name: \u0026#39;K8S-Cluster\u0026#39; file_sd_configs: - refresh_interval: 10s files: - \u0026#34;/opt/prometheus/sd_config/K8S.yml\u0026#34; sd_config K8S.yml模板文件：\n[ {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.50:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.50\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.51:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.51\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.52:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.52\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.56:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.56\u0026#34;}}, ] 启动Prometheus cd /opt/prometheus \u0026amp;\u0026amp; nohup sh start.sh 2\u0026gt;\u0026amp;1 \u0026gt; prometheus.log \u0026amp; 安装监控插件 node主机监控 grafana 大屏 node监控 下载node_exporter-1.3.1.linux-amd64.tar.gz 放至被监控机器的tmp目录。\ntar -xzvf /tmp/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/node_exporter-1.3.1.linux-amd64 /usr/local/node_exporter groupadd prometheus ;useradd -g prometheus -s /sbin/nologin prometheus chown -Rf prometheus:prometheus /usr/local/node_exporter cat \u0026gt; /usr/lib/systemd/system/node_exporter.service \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [Unit] Description=node_exporter Documentation=https://prometheus.io/ After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/node_exporter/node_exporter Restart=on-failure [Install] WantedBy=multi-user.target EOF systemctl enable node_exporter --now 注：在启动项添加 \u0026ndash;web.listen-address=\u0026quot;:9200\u0026quot; 可修改默认端口9100\nwindows主机监控 监控redis grafana 大屏 Redis Dashboard-1562834847307\ndocker run -d -p 9121:9121 -e REDIS_ADDR=\u0026#34;redis://10.43.152.50:10001\u0026#34; -e REDIS_PASSWORD=\u0026#34;f1543f7c\u0026#34; oliver006/redis_exporter 参考配置 ","permalink":"https://wandong1.github.io/post/prometheus%E7%9B%B4%E6%8E%A5%E5%AE%89%E8%A3%85/","summary":"一、prometheus监控 https://prometheus.io/download/ ###下载源码解压即可 https://grafana.com/grafana/dashboards ###搜索数据源为prometheus的\n安装docker mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce systemctl enable docker --now 安装grafana wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpm yum localinstall grafana-5.3.4-1.x86_64.rpm systemctl start grafana-server 默认密码 admin/admin prometheus安装 tar -xvzf prometheus-2.34.0.linux-amd64.tar.gz -C /opt/ mv /opt/prometheus-2.34.0.linux-amd64 /opt/prometheus cd /opt/prometheus \u0026amp;\u0026amp; mkdir data # 创建启动脚本 cat \u0026lt;\u0026lt;EOF \u0026gt;start.sh #!/bin/bash ./prometheus --storage.tsdb.path=./data --storage.tsdb.retention.time=744h --web.enable-lifecycle --storage.tsdb.no-lockfile EOF # storage.","title":"prometheus直接安装"},{"content":"https://www.rainbond.com/docs/ops-guide/storage/deploy-nfsclient\n添加helm仓库 helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond helm repo update nfs或者nas配置文件 nfs-client.yaml\nnfs: server: 1afc54bbca-jna4.cn-chongqing-cqzwy-d01.nas.alinetops.cqzwy.com #nfs server地址 path: / #nfs server 的路径 mountOptions: #添加参数 - hard - vers=4 - nolock - proto=tcp - rsize=1048576 - wsize=1048576 - timeo=600 - retrans=2 - noresvport 部署 helm install nfs-client-provisioner rainbond/nfs-client-provisioner \\ -f nfs-client.yaml \\ --version 1.2.8 创建有状态应用进行绑定 ","permalink":"https://wandong1.github.io/post/rainbond%E5%AF%B9%E6%8E%A5nfs%E6%88%96%E8%80%85nas%E5%AD%98%E5%82%A8/","summary":"https://www.rainbond.com/docs/ops-guide/storage/deploy-nfsclient\n添加helm仓库 helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond helm repo update nfs或者nas配置文件 nfs-client.yaml\nnfs: server: 1afc54bbca-jna4.cn-chongqing-cqzwy-d01.nas.alinetops.cqzwy.com #nfs server地址 path: / #nfs server 的路径 mountOptions: #添加参数 - hard - vers=4 - nolock - proto=tcp - rsize=1048576 - wsize=1048576 - timeo=600 - retrans=2 - noresvport 部署 helm install nfs-client-provisioner rainbond/nfs-client-provisioner \\ -f nfs-client.yaml \\ --version 1.2.8 创建有状态应用进行绑定 ","title":"rainbond对接nfs或者nas存储"},{"content":"https://lequ7.com/guan-yu-paas-ping-tai-rainbond-tong-guo-cha-jian-zheng-he-elkefk-shi-xian-ri-zhi-shou-ji.html\n通过helm部署filebeat采集容器日志 helm repo add elastic https://helm.elastic.co helm pull elastic/filebeat tar -xvzf filebeat-7.17.3.tgz \u0026amp;\u0026amp; cd filebeat hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#39;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; 进入容器内ping es的svc即可获得svc全称\nhelm install filebeat -n 013497775a1b4580924a00009a20c887 ./filebeat helm list -A 或者使用yaml直接不部署\n--- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: # Mounted `filebeat-inputs` configmap: path: ${path.config}/inputs.d/*.yml # Reload inputs configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.enabled: false output.elasticsearch: hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce:9200\u0026#39;] indices: - index: \u0026#34;filebeat-7.15.2-2022.06.02-000001\u0026#34; username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-inputs namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: kubernetes.yml: |- - type: docker containers.ids: - \u0026#34;*\u0026#34; processors: - add_kubernetes_metadata: in_cluster: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: elastic/filebeat:7.15.2 args: [ \u0026#34;-c\u0026#34;, \u0026#34;/etc/filebeat.yml\u0026#34;, \u0026#34;-e\u0026#34;, ] securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 500Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs # data folder stores a registry of read status for all files, so we don\u0026#39;t send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: 013497775a1b4580924a00009a20c887 roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat ","permalink":"https://wandong1.github.io/post/rainbond%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2es-filebeat/","summary":"https://lequ7.com/guan-yu-paas-ping-tai-rainbond-tong-guo-cha-jian-zheng-he-elkefk-shi-xian-ri-zhi-shou-ji.html\n通过helm部署filebeat采集容器日志 helm repo add elastic https://helm.elastic.co helm pull elastic/filebeat tar -xvzf filebeat-7.17.3.tgz \u0026amp;\u0026amp; cd filebeat hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#39;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; 进入容器内ping es的svc即可获得svc全称\nhelm install filebeat -n 013497775a1b4580924a00009a20c887 ./filebeat helm list -A 或者使用yaml直接不部署\n--- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: # Mounted `filebeat-inputs` configmap: path: ${path.config}/inputs.d/*.yml # Reload inputs configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.","title":"rainbond平台部署ES-filebeat"},{"content":"Rancher Rancher 是一套容器管理平台，它可以帮助组织在生产环境中轻松快捷的部署和管理容器。 Rancher 可以轻松地管理各种环境的 Kubernetes，满足 IT 需求并为 DevOps 团队提供支持。\nRancher 四个组成部分 Rancher 由以下四个部分组成：\n1、基础设施编排\nRancher 可以使用任何公有云或者私有云的 Linux 主机资源。Linux 主机可以是虚拟机，也可以是 物理机。\n2、容器编排与调度\n很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher 包含了当前全部主流的编排 调度引擎，例如 Docker Swarm， Kubernetes， 和 Mesos。同一个用户可以创建 Swarm 或者 Kubernetes 集群。并且可以使用原生的 Swarm 或者 Kubernetes 工具管理应用。 除了 Swarm，Kubernetes 和 Mesos 之外，Rancher 还支持自己的 Cattle 容器编排调度引擎。 Cattle 被广泛用于编排 Rancher 自己的基础设施服务以及用于 Swarm 集群，Kubernetes 集群和 Mesos 集群的配置，管理与升级。\n3、应用商店\nRancher 的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应 用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher 提供了一个由 Rancher 社区维 护的应用商店，其中包括了一系列的流行应用。Rancher 的用户也可以创建自己的私有应用商店。\n4、企业级权限管理\nRancher 支持灵活的插件式的用户认证。支持 Active Directory，LDAP， Github 等 认证方 式。\n使用 Rancher 搭建 k8s 集群 初始化安装机环境 关闭selinux和防火墙\nsystemctl stop firewalld.service \u0026amp;\u0026amp; systemctl disable firewalld.service \u0026amp;\u0026amp; iptables -F \u0026amp;\u0026amp;setenforce 0 hostnamectl set-hostname master01\rhostnamectl set-hostname node01\rhostnamectl set-hostname node02 安装 docker 环境依赖\n在线\nyum install -y yum-utils yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install docker-ce docker-ce-cli containerd.io -y systemctl start docker \u0026amp;\u0026amp; systemctl enable docker.service \u0026amp;\u0026amp; systemctl status docker 离线安装docker\n#!/bin/bash\r#解压\rdockers images \u0026gt;\u0026gt;/dev/null\rif [ $? -eq 0 ];then\recho \u0026#34;docker is installed!!\u0026#34;\relse\rtar -xvzf docker-20.10.14.tgz -C /opt/\rchown root:root -R /opt/docker/\rcp /opt/docker/* /usr/bin\rcat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service\r[Unit]\rDescription=Docker Application Container Engine\rDocumentation=https://docs.docker.com\rAfter=network-online.target firewalld.service\rWants=network-online.target\r[Service]\rType=notify\rExecStart=/usr/bin/dockerd\rExecReload=/bin/kill -s HUP \\$MAINPID\rLimitNOFILE=infinity\rLimitNPROC=infinity\rTimeoutStartSec=0\rDelegate=yes\rKillMode=process\rRestart=on-failure\rStartLimitBurst=3\rStartLimitInterval=60s\r[Install]\rWantedBy=multi-user.target\rEOF\rchmod +x /etc/systemd/system/docker.service\r# 加载service配置\rsystemctl daemon-reload \u0026amp;\u0026amp; echo \u0026#34;加载service配置 success!\u0026#34;\r# dockerDamon\rtee /etc/docker/daemon.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39;\r{\r\u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://dockerhub.azk8s.cn\u0026#34;,\u0026#34;http://hubmirror.c.163.com\u0026#34;,\u0026#34;http://qtid6917.mirror.aliyuncs.com\u0026#34;]\r}\rEOF\recho \u0026#34;docker daemon.json edit success!\u0026#34;\r#设置开机启动 并立即启动\rsystemctl enable docker.service --now \u0026amp;\u0026amp; echo \u0026#34;docker start success!\u0026#34;\rfi 镜像加速，每台机器执行\ntee /etc/docker/daemon.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://dockerhub.azk8s.cn\u0026#34;,\u0026#34;http://hubmirror.c.163.com\u0026#34;,\u0026#34;http://qtid6917.mirror.aliyuncs.com\u0026#34;] } EOF 安装 rancher 平台\n离线导入rancher镜像\ndocker run -d --restart=unless-stopped -p 80:80 -p 443:443 -- privileged rancher/rancher 注：\u0026ndash;restart=unless-stopped ，在容器退出时总是重启容器，但是不考虑在 Docker 守护进程 启动时就已经停止了的容器\n安装过程如果失败需要清理后再重新安装K8S节点\n#!/bin/bash #df -h|grep kubelet |awk -F % \u0026#39;{print $2}\u0026#39;|xargs umount sudo rm /var/lib/kubelet/* -rf sudo rm /etc/kubernetes/* -rf sudo rm /etc/cni/* -rf sudo rm /var/lib/rancher/* -rf sudo rm /var/lib/etcd/* -rf sudo rm /var/lib/cni/* -rf sudo rm /opt/cni/* -rf sudo ip link del flannel.1 ip link del cni0 iptables -F \u0026amp;\u0026amp; iptables -t nat -F docker rm -f `docker ps -a -q` docker volume ls|awk \u0026#39;{print $2}\u0026#39;|xargs docker volume rm systemctl restart docker ","permalink":"https://wandong1.github.io/post/rancher/","summary":"Rancher Rancher 是一套容器管理平台，它可以帮助组织在生产环境中轻松快捷的部署和管理容器。 Rancher 可以轻松地管理各种环境的 Kubernetes，满足 IT 需求并为 DevOps 团队提供支持。\nRancher 四个组成部分 Rancher 由以下四个部分组成：\n1、基础设施编排\nRancher 可以使用任何公有云或者私有云的 Linux 主机资源。Linux 主机可以是虚拟机，也可以是 物理机。\n2、容器编排与调度\n很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher 包含了当前全部主流的编排 调度引擎，例如 Docker Swarm， Kubernetes， 和 Mesos。同一个用户可以创建 Swarm 或者 Kubernetes 集群。并且可以使用原生的 Swarm 或者 Kubernetes 工具管理应用。 除了 Swarm，Kubernetes 和 Mesos 之外，Rancher 还支持自己的 Cattle 容器编排调度引擎。 Cattle 被广泛用于编排 Rancher 自己的基础设施服务以及用于 Swarm 集群，Kubernetes 集群和 Mesos 集群的配置，管理与升级。\n3、应用商店\nRancher 的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应 用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher 提供了一个由 Rancher 社区维 护的应用商店，其中包括了一系列的流行应用。Rancher 的用户也可以创建自己的私有应用商店。\n4、企业级权限管理\nRancher 支持灵活的插件式的用户认证。支持 Active Directory，LDAP， Github 等 认证方 式。","title":"rancher的安装和使用"},{"content":"1、缓存击穿，某些场景下，大量的key同时失效，请求直接穿过redis缓存层打到数据库上。 解决方法：对key的失效时间设置随机值避免同时失效。\n2、缓存穿透，请求进来请求本就不存在的数据，redis层找不到数据库也找不到，每个这种请求都会打到数据库造成压力。 解决方法：对请求的后端数据库不存在的数据，设置空缓存，避免恶意请求对后端数据库造成压力。\n3、雪崩，指流量进来打到redis，redis由于某些原因扛不住，流量又会打到数据库，数据库很显然更抗不住。造成系统雪崩。 解决方法：redis采用高可用的集群架构，针对某些bigkey进行打散操作。\n4、redis常见数据类型。 string，hash，list，set ，有序set。\n5、Redis有哪些适合的场景? (1)Session共享(单点登录);(2)页面缓存;(3)队列;(4)排行榜/计数器;(5)发布/订阅;\n(1)LUA脚本：在事务的基础上，假如，需要在服务端一次性的执行更复杂的操作，那么，这个时候lua就可以上场了。\n","permalink":"https://wandong1.github.io/post/redis%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","summary":"1、缓存击穿，某些场景下，大量的key同时失效，请求直接穿过redis缓存层打到数据库上。 解决方法：对key的失效时间设置随机值避免同时失效。\n2、缓存穿透，请求进来请求本就不存在的数据，redis层找不到数据库也找不到，每个这种请求都会打到数据库造成压力。 解决方法：对请求的后端数据库不存在的数据，设置空缓存，避免恶意请求对后端数据库造成压力。\n3、雪崩，指流量进来打到redis，redis由于某些原因扛不住，流量又会打到数据库，数据库很显然更抗不住。造成系统雪崩。 解决方法：redis采用高可用的集群架构，针对某些bigkey进行打散操作。\n4、redis常见数据类型。 string，hash，list，set ，有序set。\n5、Redis有哪些适合的场景? (1)Session共享(单点登录);(2)页面缓存;(3)队列;(4)排行榜/计数器;(5)发布/订阅;\n(1)LUA脚本：在事务的基础上，假如，需要在服务端一次性的执行更复杂的操作，那么，这个时候lua就可以上场了。","title":"redis的常见问题"},{"content":"zookeeper 官网 https://zookeeper.apache.org/ 找download\n一、下载软件包 https://dlcdn.apache.org/zookeeper/zookeeper-3.8.0/apache-zookeeper-3.8.0-bin.tar.gz\n二、集群部署 1、安装JDK centos\nyum install java-1.8.0-openjdk* -y 2、zk配置文件 # The number of milliseconds of each tick\rtickTime=2000\r# The number of ticks that the initial # synchronization phase can take\rinitLimit=10\r# The number of ticks that can pass between # sending a request and getting an acknowledgement\rsyncLimit=5\r# the directory where the snapshot is stored.\r# do not use /tmp for storage, /tmp here is just # example sakes.\rdataDir=/opt/zookeeper/data\r# the port at which the clients will connect\rclientPort=2181\r# the maximum number of client connections.\r# increase this if you need to handle more clients\r#maxClientCnxns=60\r#\r# Be sure to read the maintenance section of the # administrator guide before turning on autopurge.\r#\r# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\r#\r# The number of snapshots to retain in dataDir\r#autopurge.snapRetainCount=3\r# Purge task interval in hours\r# Set to \u0026#34;0\u0026#34; to disable auto purge feature\r#autopurge.purgeInterval=1\r## Metrics Providers\r#\r# https://prometheus.io Metrics Exporter\r#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider\r#metricsProvider.httpHost=0.0.0.0\r#metricsProvider.httpPort=7000\r#metricsProvider.exportJvmInfo=true\rserver.1=10.43.152.67:2888:3888\rserver.2=10.43.152.68:2888:3888\rserver.3=10.43.152.69:2888:3888 注：每个zk节点，的dataDir目录下必须有myid文件，myid文件中的内容必须为对应的 server.【编号】=10.43.152.67:2888:3888 中的编号\n3、解压zk并启动 zookeeper前台启动命令\nbin/zkServer.sh start-foreground conf/zoo.cfg ","permalink":"https://wandong1.github.io/post/zookeeper/","summary":"zookeeper 官网 https://zookeeper.apache.org/ 找download\n一、下载软件包 https://dlcdn.apache.org/zookeeper/zookeeper-3.8.0/apache-zookeeper-3.8.0-bin.tar.gz\n二、集群部署 1、安装JDK centos\nyum install java-1.8.0-openjdk* -y 2、zk配置文件 # The number of milliseconds of each tick\rtickTime=2000\r# The number of ticks that the initial # synchronization phase can take\rinitLimit=10\r# The number of ticks that can pass between # sending a request and getting an acknowledgement\rsyncLimit=5\r# the directory where the snapshot is stored.\r# do not use /tmp for storage, /tmp here is just # example sakes.","title":"zookeeper的安装和使用"},{"content":"Helm Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前\n打包好的yaml文件部署到kubernetes上。\nHelm有3个重要概念：\n• **helm：**一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。\n• **Chart：**应用描述，一系列用于描述 k8s 资源相关文件的集合。\n• **Release：**基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在\nk8s中创建出真实运行的资源对象。\nHelm客户端 使用helm很简单，你只需要下载一个二进制客户端包即可，会通过kubeconfig配置（通常$HOME/.kube/config）来连接Kubernetes。\n项目地址：https://github.com/helm/helm\n下载Helm客户端：\nwget https://get.helm.sh/helm-v3.4.2-linux-amd64.tar.gz tar zxvf helm-v3.4.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ Helm常用命令 Helm管理应用生命周期： • helm create 创建Chart示例\n• helm install 部署\n• helm upgrade 更新\n• helm rollback 回滚\n• helm uninstall 卸载\nHelm基本使用：创建Chart示例 创建chart：\n# 默认示例中部署的是一个nginx服务 helm create mychart 打包chart：\nhelm package mychart • charts：目录里存放这个chart依赖的所有子chart。\n• Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。\n• values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。\n• Templates： 目录里面存放所有yaml模板文件。\n• NOTES.txt ：用于介绍Chart帮助信息， helm install 部署后展示给用户。例如：\n如何使用这个 Chart、列出缺省的设置等。\n• _helpers.tpl：放置模板的地方，可以在整个 chart 中重复使用。\nHelm基本使用：部署 部署Chart：\nhelm install web mychart 查看Release：\nhelm list -n default 查看部署的Pod：\nkubectl get pods,svc Helm基本使用：升级 使用Chart升级应用有两种方法：\n• \u0026ndash;values，-f：指定YAML文件覆盖值\n• \u0026ndash;set：在命令行上指定覆盖值\n注：如果一起使用，\u0026ndash;set优先级高\n例如将nginx服务升级到1.17版本：\n第一种方式： # vi values.yaml #任意路径 image: tag: \u0026#34;1.17“ helm upgrade -f values.yaml web mychart 第二种方式： helm upgrade --set image.tag=1.17 web mychart Helm基本使用：回滚、卸载 回滚到上一个版本：\nhelm rollback web 查看历史版本：\nhelm history web 回滚到指定版本：\nhelm rollback web 2 卸载应用：\nhelm uninstall web Helm工作流程 Helm模板 Helm核心是模板，即模板化K8s YAML文件。\n通过模板实现Chart高效复用，当部署多个应用时，可以将差异化的字段进行模板化，在部署时使用-f或\n者\u0026ndash;set动态覆盖默认值，从而适配多个应用。\nHelm模板由Go Template编写，指令由{{ }}包裹。\n# values.yaml\nreplicaCount: 1 image: repository: nginx tag: \u0026#34;latest\u0026#34; selectorLabels: \u0026#34;nginx\u0026#34; # templates/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: {{ .Release.Name }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Values.selectorLabels }} template: metadata: labels: app: {{ .Values.selectorLabels }} spec: containers: - image: {{ .Values.image.repository }}:{{ .Values.image.tag }} name: web Chart模板：内置对象 在上面示例中，模板文件中.Release、.Values是Helm内置对象，顶级开头写。\n**Release对象：**获取发布记录信息\n**Values对象：**为Chart模板提供值，这个对象的值有4个来源：\n• chart包中的values.yaml文件\n• helm install或者helm upgrade的-f或者\u0026ndash;values参数传入的自定义yaml文件\n• \u0026ndash;set参数传入值\n**Chart对象：**可以通过Chart对象访问Chart.yaml文件的内容，例如：{{ .Chart.AppVersion }}\nChart模板：调试 使用helm install提供了\u0026ndash;dry-run和\u0026ndash;debug调试参数，帮助你验证模板正确性，并把渲染后的模板打印出来，而\n不会真正的去部署。\n# helm install \u0026ndash;dry-run web mychart\n","permalink":"https://wandong1.github.io/post/helm%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","summary":"Helm Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前\n打包好的yaml文件部署到kubernetes上。\nHelm有3个重要概念：\n• **helm：**一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。\n• **Chart：**应用描述，一系列用于描述 k8s 资源相关文件的集合。\n• **Release：**基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在\nk8s中创建出真实运行的资源对象。\nHelm客户端 使用helm很简单，你只需要下载一个二进制客户端包即可，会通过kubeconfig配置（通常$HOME/.kube/config）来连接Kubernetes。\n项目地址：https://github.com/helm/helm\n下载Helm客户端：\nwget https://get.helm.sh/helm-v3.4.2-linux-amd64.tar.gz tar zxvf helm-v3.4.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ Helm常用命令 Helm管理应用生命周期： • helm create 创建Chart示例\n• helm install 部署\n• helm upgrade 更新\n• helm rollback 回滚\n• helm uninstall 卸载\nHelm基本使用：创建Chart示例 创建chart：\n# 默认示例中部署的是一个nginx服务 helm create mychart 打包chart：\nhelm package mychart • charts：目录里存放这个chart依赖的所有子chart。\n• Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。\n• values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。","title":"helm的使用方法介绍"},{"content":"推荐博客 https://www.liwenzhou.com/posts/Go/golang-menu/\nLinux 安装go语言环境 # 下载地址 https://golang.google.cn/dl/ tar -xvzf go1.17.11.linux-386.tar.gz -C /usr/local yum install glibc.i686 -y mkdir -p /root/workspace cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPATH=\u0026#34;$HOME/workspace\u0026#34; EOF source /etc/profile #设置国内代理 go env -w GOPROXY=https://goproxy.cn,direct #查看go env go env Linux下创建一个go项目 #查看GOPATH go env | grep -i gopath #GOPATH=\u0026#34;/root/workspace\u0026#34; cd /root/workspace;mkdir {src,bin,pkg} #进入src目录创建项目 cd src \u0026amp;\u0026amp; mkdir GoRedis #随后编写main.go文件 构建多平台运行代码 go env -w GOOS=linux go env -w GOARCH=amd64 go build go env -w GOOS=windwos go env -w GOARCH=amd64 go build main.go文件内容 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) var ctx = context.Background() func main() { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;10.43.152.50:10001\u0026#34;, Password: \u0026#34;f1543f7c\u0026#34;, // 密码 DB: 1, // 数据库 PoolSize: 20, // 连接池大小 }) pong, err := rdb.Ping(ctx).Result() fmt.Println(pong, err) } # 执行命令初始化mod go mod init GoRedis # 安装三方依赖包 go get github.com/go-redis/redis/v8 # 整个目录编译，编译后会生成对应系统的执行文件 go build # 运行文件 ./GoRedis Golang指针 任何程序数据载入内存后，在内存都有他们的地址，这就是指针。而为了保存一个数据在内存中的地址，我们就需要指针变量。\n比如，“永远不要高估自己”这句话是我的座右铭，我想把它写入程序中，程序一启动这句话是要加载到内存（假设内存地址0x123456），我在程序中把这段话赋值给变量A，把内存地址赋值给变量B。这时候变量B就是一个指针变量。通过变量A和变量B都能找到我的座右铭。\nGo语言中的指针不能进行偏移和运算，因此Go语言中的指针操作非常简单，我们只需要记住两个符号：\u0026amp;（取地址）和*（根据地址取值）。\nx := 1 p := \u0026amp;x fmt.Println(p) // 变量的内存地址 指针 fmt.Println(*p) //输出 1 *p = 2 // 相当于 x = 2 fmt.Println(x) // 输出2 var x,y int fmt.Println(\u0026amp;x == \u0026amp;x,\u0026amp;x == \u0026amp;y, \u0026amp;x == nil) // 输出 true false false 总结： 取地址操作符\u0026amp;和取值操作符*是一对互补操作符，\u0026amp;取出地址，*根据地址取出地址指向的值。\n变量、指针地址、指针变量、取地址、取值的相互关系和特性如下：\n对变量进行取地址（\u0026amp;）操作，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值（*）操作，可以获得指针变量指向的原变量的值。 Golang并发编程 多线程介绍 A. 线程是由操作系统进行管理，也就是处于内核态。\nB. 线程之间进行切换，需要发生用户态到内核态的切换。\nC. 当系统中运行大量线程，系统会变的非常慢。\nD. 用户态的线程，支持大量线程创建。也叫协程或goroutine。\n1、串行、并发与并行 串行：我们都是先读小学，小学毕业后再读初中，读完初中再读高中。\n并发：同一时间段内执行多个任务（你在用微信和两个女朋友聊天）。\n并行：同一时刻执行多个任务（你和你朋友都在用微信和女朋友聊天）。\n2、进程、线程和协程 进程（process）：程序在操作系统中的一次执行过程，系统进行资源分配和调度的一个独立单位。\n线程（thread）：操作系统基于进程开启的轻量级进程，是操作系统调度执行的最小单位。\n协程（coroutine）：非操作系统提供而是由用户自行创建和控制的用户态‘线程’，比线程更轻量级。\n3、并发模型 业界将如何实现并发编程总结归纳为各式各样的并发模型，常见的并发模型有以下几种：\n线程\u0026amp;锁模型\nActor模型\nCSP模型\nFork\u0026amp;Join模型\nGo语言中的并发程序主要是通过基于CSP（communicating sequential processes）的goroutine和channel来实现，当然也支持使用传统的多线程共享内存的并发方式。\npackage main import ( \u0026#34;fmt\u0026#34; ) func hello() { fmt.Println(\u0026#34;hello\u0026#34;) } func main() { go hello() // 开启线程 fmt.Println(\u0026#34;你好\u0026#34;) } 多线程问题 执行以上代码，程序会退出，主线程执行完退出，主线程下开启的所有线程都会结束，hello()还没来得及执行就被打断了。\n解决办法：\n方法一： 在主函数中添加sleep，使用time.Sleep让 main goroutine 等待 hello goroutine执行结束是不优雅的，当然也是不准确的。\nfunc main() { go hello() fmt.Println(\u0026#34;你好\u0026#34;) time.Sleep(time.Second) } 方法二： Go 语言中通过sync包为我们提供了一些常用的并发原语，我们会在后面的小节单独介绍sync包中的内容。在这一小节，我们会先介绍一下 sync 包中的WaitGroup。当你并不关心并发操作的结果或者有其它方式收集并发操作的结果时，WaitGroup是实现等待一组并发操作完成的好方法。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) // 声明全局等待组变量 var wg sync.WaitGroup func hello() { fmt.Println(\u0026#34;hello\u0026#34;) wg.Done() // 告知当前goroutine完成 } func main() { wg.Add(1) // 登记1个goroutine。启动一个goroutine，就需登记一个 go hello() fmt.Println(\u0026#34;你好\u0026#34;) wg.Wait() // 阻塞等待登记的goroutine完成 } 登记多个goroutine\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) // 声明全局等待组变量 var wg sync.WaitGroup func hello(num int) { fmt.Println(\u0026#34;hello\u0026#34;, num) wg.Done() // 告知当前goroutine完成 } func main() { for i := 0; i \u0026lt; 10; i++ { wg.Add(1) // 登记1个goroutine go hello(i) } fmt.Println(\u0026#34;你好\u0026#34;) wg.Wait() // 阻塞等待登记的goroutine完成 } 方法三： 利用channel。。。\npackage main import \u0026#34;fmt\u0026#34; func hello(num int, ch chan bool) { ch \u0026lt;- true fmt.Println(\u0026#34;hellooooo\u0026#34;, num) } func main() { var num = 15 ch := make(chan bool, num) for i := 0; i \u0026lt; num; i++ { go hello(i, ch) } for j := 0; j \u0026lt; num; j++ { \u0026lt;-ch } fmt.Println(\u0026#34;All go routines finished executing\u0026#34;) } GOMAXPROCS Go运行时的调度器使用GOMAXPROCS参数来确定需要使用多少个 OS 线程来同时执行 Go 代码。默认值是机器上的 CPU 核心数。例如在一个 8 核心的机器上，GOMAXPROCS 默认为 8。Go语言中可以通过runtime.GOMAXPROCS函数设置当前程序并发时占用的 CPU逻辑核心数。（Go1.5版本之前，默认使用的是单核心执行。Go1.5 版本之后，默认使用全部的CPU 逻辑核心数。）\nchannel 单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。\n虽然可以使用共享内存进行数据交换，但是共享内存在不同的 goroutine 中容易发生竞态问题。为了保证数据交换的正确性，很多并发模型中必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。\nGo语言采用的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\n如果说 goroutine 是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个 goroutine 发送特定值到另一个 goroutine 的通信机制。\nGo 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。\nchannel类型 channel是 Go 语言中一种特有的类型。声明通道类型变量的格式如下：\nvar 变量名称 chan 元素类型 //元素类型：是指通道中传递元素的类型 //举例 var ch1 chan int // 声明一个传递整型的通道 var ch2 chan bool // 声明一个传递布尔型的通道 var ch3 chan []int // 声明一个传递int切片的通道 channel零值 未初始化的通道类型变量其默认零值是nil。\nvar ch chan int fmt.Println(ch) // \u0026lt;nil\u0026gt; 初始化channel 声明的通道类型变量需要使用内置的make函数初始化之后才能使用。具体格式如下：\nmake(chan 元素类型, [缓冲大小]) //channel的缓冲大小是可选的。 // 举个例子 ch4 := make(chan int) ch5 := make(chan bool, 1) // 声明一个缓冲区大小为1的通道 channel操作 通道共有发送（send）、接收(receive）和关闭（close）三种操作。而发送和接收操作都使用\u0026lt;-符号。\nch := make(chan int) //发送 ch \u0026lt;- 10 // 把10发送到ch中 //接收 x := \u0026lt;- ch // 从ch中接收值并赋值给变量x \u0026lt;-ch // 从ch中接收值，忽略结果 //关闭 close(ch) 关闭后的通道有以下特点：\n对一个关闭的通道再发送值就会导致 panic。 对一个关闭的通道进行接收会一直获取值直到通道为空。 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致 panic。 无缓冲的通道 无缓冲的通道又称为阻塞的通道。我们来看一下如下代码片段。\nfunc main() { ch := make(chan int) ch \u0026lt;- 10 fmt.Println(\u0026#34;发送成功\u0026#34;) } 有缓冲的通道 func main() { ch := make(chan int, 1) // 创建一个容量为1的有缓冲区通道 ch \u0026lt;- 10 fmt.Println(\u0026#34;发送成功\u0026#34;) } 有缓冲通道的死锁情况 package main import ( \u0026#34;fmt\u0026#34; ) func main() { ch := make(chan string, 2) \u0026lt;-ch //死锁 ch \u0026lt;- \u0026#34;hello\u0026#34; ch \u0026lt;- \u0026#34;world\u0026#34; ch \u0026lt;- \u0026#34;!\u0026#34; // 死锁 fmt.Println(\u0026lt;-ch) fmt.Println(\u0026lt;-ch) } 多返回值模式 value, ok := \u0026lt;- ch //value：从通道中取出的值，如果通道被关闭则返回对应类型的零值。 //ok：通道ch关闭时返回 false，否则返回 true。 for range接收值 通常我们会选择使用for range循环从通道中接收值，当通道被关闭后，会在通道内的所有值被接收完毕后会自动退出循环。上面那个示例我们使用for range改写后会很简洁。\nfunc f3(ch chan int) { for v := range ch { fmt.Println(v) } } select多路复用 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) // 打印基数 for i := 1; i \u0026lt;= 10; i++ { select { case x := \u0026lt;-ch: fmt.Println(x) case ch \u0026lt;- i: } } } channel练习 解法1： package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; \u0026#34;math/rand\u0026#34; ) type Result struct { TaskId int Res string } var resultChan chan *Result func wsum(num int) string { needSum := []int{} for { if num \u0026gt;= 10 { numOne := num % 10 needSum = append(needSum, numOne) num_buf := num / 10 num = int(math.Floor(float64(num_buf))) } else { needSum = append(needSum, num) break } } ret_sum := 0 for i := 0; i \u0026lt; len(needSum); i++ { ret_sum += needSum[i] } // for v := range needSum { // ret_sum += v // fmt.Println(ret_sum) // } // fmt.Printf(\u0026#34;%v sum is %d\\n\u0026#34;, needSum, ret_sum) return fmt.Sprintf(\u0026#34;%v sum is %d\\n\u0026#34;, needSum, ret_sum) } func worker(tId int) { new_num := rand.Int() resInfo := Result{ TaskId: tId, Res: fmt.Sprintf(\u0026#34;%v,%v\u0026#34;, new_num, wsum(new_num)), } // fmt.Println(resInfo) resultChan \u0026lt;- \u0026amp;resInfo } func main() { times := 100 resultChan = make(chan *Result, times) for i := 0; i \u0026lt; times; i++ { go worker(i) } // for v := range resultChan { // fmt.Println(v) // } for j := 0; j \u0026lt; times; j++ { fmt.Println(\u0026lt;-resultChan) } } 解法2 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; ) type res struct { Num int EachSum int } var jobChan chan int var resultChan chan *res func computeInt(num int) (sumNum int) { time.Sleep(time.Second) strNum := strconv.Itoa(num) fmt.Printf(\u0026#34;%v is %T\\n\u0026#34;, strNum, strNum) for i := 0; i \u0026lt; len(strNum); i++ { eachNum := string(strNum[i]) eachNumInt, _ := strconv.Atoi(eachNum) sumNum += eachNumInt } return } func computeInt2(num int) (sumNum int) { for num \u0026gt; 10 { sumNum += num % 10 num = num / 10 } time.Sleep(time.Second) return } func makeRandInt(jobChan chan int) { rand.Seed(time.Nanosecond.Nanoseconds()) for { num := rand.Int() jobChan \u0026lt;- num } } func computeEachNumSum(jobChan chan int, resultChan chan *res) { for { num := \u0026lt;-jobChan res := \u0026amp;res{Num: num, EachSum: computeInt2(num)} resultChan \u0026lt;- res } } func main() { jobChan = make(chan int) resultChan = make(chan *res) go makeRandInt(jobChan) for i := 0; i \u0026lt; 24; i++ { go computeEachNumSum(jobChan, resultChan) } for v := range resultChan { fmt.Printf(\u0026#34;%#v\\n\u0026#34;, *v) } } Golang Gin框架 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.io,direct # 安装gin框架 go get -u github.com/gin-gonic/gin # 解决飘红 go mod init gin go mod edit -require github.com/gin-gonic/gin@latest go mod vendor go env -w GO111MODULE=on\rgo env -w GOPROXY=https://goproxy.io,direct 创建项目后，执行初始命令\ngo mod init 项目名 第一个Gin程序 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func main() { r := gin.Default() // func 匿名函数 r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { //输出json结果给调用方 c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 另外一种写法 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func testping(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) } func main() { r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, testping) r.Run() // listen and serve on 0.0.0.0:8080 } GET和POST方法 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func getPing(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;get pong\u0026#34;, }) } func postPing(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;post pong\u0026#34;, }) } func main() { r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, getPing) r.POST(\u0026#34;/ping\u0026#34;, postPing) r.Run() // listen and serve on 0.0.0.0:8080 } Gin框架路由分组 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func login(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func submit(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func main() { //Default返回一个默认的路由引擎 router := gin.Default() // Simple group: v1 v1 := router.Group(\u0026#34;/api/v1\u0026#34;) { v1.POST(\u0026#34;/login\u0026#34;, login) v1.POST(\u0026#34;/submit\u0026#34;, submit) } // Simple group: v2 v2 := router.Group(\u0026#34;/api/v2\u0026#34;) { v2.POST(\u0026#34;/login\u0026#34;, login) v2.POST(\u0026#34;/submit\u0026#34;, submit) } _ = router.Run(\u0026#34;:8080\u0026#34;) } 参数绑定 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) // Binding from JSON type Login struct { User string `form:\u0026#34;user\u0026#34; json:\u0026#34;user\u0026#34; binding:\u0026#34;required\u0026#34;` UserId int `form:\u0026#34;user_id\u0026#34; json:\u0026#34;user_id\u0026#34;` Password string `form:\u0026#34;password\u0026#34; json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } func main() { router := gin.Default() // Example for binding JSON ({\u0026#34;user\u0026#34;: \u0026#34;manu\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;123\u0026#34;}) router.POST(\u0026#34;/loginJSON\u0026#34;, func(c *gin.Context) { var login Login if err := c.ShouldBindJSON(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;user_id\u0026#34;: login.UserId, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // Example for binding a HTML form (user=manu\u0026amp;password=123) router.POST(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login // This will infer what binder to use depending on the content-type header. if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // Example for binding a HTML querystring (user=manu\u0026amp;password=123) router.GET(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) _ = router.Run(\u0026#34;:8080\u0026#34;) } json返回数据渲染 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() // gin.H is a shortcut for map[string]interface{} r.GET(\u0026#34;/someJSON\u0026#34;, func(c *gin.Context) { //第一种方式,自己拼json c.JSON(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;hey\u0026#34;, \u0026#34;status\u0026#34;: http.StatusOK}) }) r.GET(\u0026#34;/moreJSON\u0026#34;, func(c *gin.Context) { // 第二种方式 You also can use a struct var msg struct { Name string `json:\u0026#34;user\u0026#34;` Message string Number int } msg.Name = \u0026#34;Lena\u0026#34; msg.Message = \u0026#34;hey\u0026#34; msg.Number = 123 // Note that msg.Name becomes \u0026#34;user\u0026#34; in the JSON c.JSON(http.StatusOK, msg) }) // Listen and serve on 0.0.0.0:8080 r.Run(\u0026#34;:8080\u0026#34;) } swagger 生成接口文档 go get -u github.com/swaggo/swag/cmd/swag swag init package main import ( _ \u0026#34;GinTest2/docs\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; ginSwagger \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) func login(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func submit(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func main() { //Default返回一个默认的路由引擎 router := gin.Default() // Simple group: v1 v1 := router.Group(\u0026#34;/api/v1\u0026#34;) { v1.POST(\u0026#34;/login\u0026#34;, login) v1.POST(\u0026#34;/submit\u0026#34;, submit) } // Simple group: v2 v2 := router.Group(\u0026#34;/api/v2\u0026#34;) { v2.POST(\u0026#34;/login\u0026#34;, login) v2.POST(\u0026#34;/submit\u0026#34;, submit) } router.GET(\u0026#34;/swagger/*any\u0026#34;, ginSwagger.WrapHandler(swaggerFiles.Handler)) _ = router.Run(\u0026#34;:8080\u0026#34;) } 访问接口文档 http://localhost:8080/swagger/index.html Golang 使用sqlx连接mysql数据库 安装三方包 go get github.com/go-sql-driver/mysql\rgo get github.com/jmoiron/sqlx main.go文件代码 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; ) var db *sqlx.DB type user struct { Id int64 UserName string Pwd string } func initDB() (err error) { dsn := \u0026#34;slbtraffic:1qaz#EDC@tcp(10.47.69.231:7306)/billing?charset=utf8mb4\u0026amp;parseTime=True\u0026#34; // 也可以使用MustConnect连接不成功就panic db, err = sqlx.Connect(\u0026#34;mysql\u0026#34;, dsn) if err != nil { fmt.Printf(\u0026#34;connect DB failed, err:%v\\n\u0026#34;, err) return } db.SetMaxOpenConns(20) db.SetMaxIdleConns(10) return } // 查询单条数据示例 func queryRowDemo() { sqlStr := \u0026#34;select id, username, pwd from users where id=?\u0026#34; var u user err := db.Get(\u0026amp;u, sqlStr, 1) if err != nil { fmt.Printf(\u0026#34;get failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.Id, u.UserName, u.Pwd) } // 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \u0026#34;select id, username, pwd from users\u0026#34; var u []user err := db.Select(\u0026amp;u, sqlStr) if err != nil { fmt.Printf(\u0026#34;get multi row failed, err:%v\\n\u0026#34;, err) return } for _, val := range u { fmt.Printf(\u0026#34;id:%d name:%s pwd:%s\\n\u0026#34;, val.Id, val.UserName, val.Pwd) } } // 插入数据 func insertRowDemo() { sqlStr := \u0026#34;insert into users(username, pwd) values (?,?)\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;沙河小王子\u0026#34;, \u0026#34;1235asd124142\u0026#34;) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\u0026#34;get lastinsert ID failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;insert success, the id is %d.\\n\u0026#34;, theID) } // 更新数据 func updateRowDemo() { sqlStr := \u0026#34;update users set username=? where username=? limit 1\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;沙河小王子sb\u0026#34;, \u0026#34;沙河小王子\u0026#34;) if err != nil { fmt.Printf(\u0026#34;upadte failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;update success, RowsAffected is %d.\\n\u0026#34;, n) } // 删除数据 func deleteRowDemo() { sqlStr := \u0026#34;delete from users where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\u0026#34;delete failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;delete success, affected rows:%d\\n\u0026#34;, n) } func main() { initDB() queryRowDemo() // insertRowDemo() updateRowDemo() deleteRowDemo() queryMultiRowDemo() } Golang GORM教程 安装和连接 官方文档 https://gorm.io/zh_CN/docs/index.html\n# 创建项目 go mod init 项目名称 go get -u gorm.io/gorm go get -u gorm.io/driver/mysql package main import ( \u0026#34;fmt\u0026#34; \u0026#34;gorm.io/driver/mysql\u0026#34; \u0026#34;gorm.io/gorm\u0026#34; ) type Product struct { gorm.Model Code string `gorm:\u0026#34;type:varchar(255)\u0026#34;` Price uint } func main() { dsn := \u0026#34;slbtraffic:1qaz#EDC@tcp(10.47.69.231:7306)/billing?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34; db, err := gorm.Open(mysql.Open(dsn), \u0026amp;gorm.Config{}) if err != nil { panic(\u0026#34;failed to connect database\u0026#34;) } // 迁移 schema // db.AutoMigrate(\u0026amp;Product{}) // // Create // db.Create(\u0026amp;Product{Code: \u0026#34;D43\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D44\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D45\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D46\u0026#34;, Price: 120}) // 查询记录 var productData Product db.First(\u0026amp;productData, 1) fmt.Printf(\u0026#34;productData=%v\\n\u0026#34;, productData) // db.First(\u0026amp;productData, \u0026#34;code = ?\u0026#34;, \u0026#34;D46\u0026#34;) // fmt.Printf(\u0026#34;productData=%#v\\n\u0026#34;, productData) // 修改记录 将D46 修改为D666 // db.Model(\u0026amp;productData).Update(\u0026#34;code\u0026#34;, \u0026#34;D666\u0026#34;) // Update - 更新多个字段 // db.Model(\u0026amp;productData).Updates(Product{Price: 200, Code: \u0026#34;F42\u0026#34;}) // 仅更新非零值字段 // db.Model(\u0026amp;productData).Updates(map[string]interface{}{\u0026#34;Price\u0026#34;: 200, \u0026#34;Code\u0026#34;: \u0026#34;F42\u0026#34;}) // 删除 // db.Delete(\u0026amp;productData, 1) } Golang 操作redis 连接池 安装redis连接库 # 安装三方包 go get github.com/go-redis/redis/v8 # 插件官方 https://github.com/go-redis/redis # 官方文档 https://redis.uptrace.dev/guide/go-redis.html#installation package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; \u0026#34;time\u0026#34; ) var ctx = context.Background() func main() { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;10.43.152.50:10001\u0026#34;, Password: \u0026#34;f1543f7c\u0026#34;, // 密码 DB: 1, // 数据库 PoolSize: 20, // 连接池大小 }) pong, err := rdb.Ping(ctx).Result() fmt.Println(pong, err) // 直接执行命令获取错误 err = rdb.Set(ctx, \u0026#34;key\u0026#34;, 520, time.Hour).Err() // 直接执行命令获取值 value := rdb.Get(ctx, \u0026#34;key\u0026#34;).Val() fmt.Println(value) } Gin 框架 JWT的实现（重要） package main import ( \u0026#34;errors\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v4\u0026#34; ) // jwt 过期时间 const TokenExpireDuration = time.Hour * 2 // CustomSecret 用于加盐的字符串 var CustomSecret = []byte(\u0026#34;夏天夏天悄悄过去\u0026#34;) type UserInfo struct { Username string Password string } type CustomClaims struct { // 可根据需要自行添加字段 Username string `json:\u0026#34;username\u0026#34;` jwt.RegisteredClaims // 内嵌标准的声明 } // GenToken 生成JWT func GenToken(username string) (string, error) { // 创建一个我们自己的声明 claims := CustomClaims{ username, // 自定义字段 jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(TokenExpireDuration)), Issuer: \u0026#34;my-project\u0026#34;, // 签发人 }, } // 使用指定的签名方法创建签名对象 token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) // 使用指定的secret签名并获得完整的编码后的字符串token return token.SignedString(CustomSecret) } func authHandler(c *gin.Context) { // 用户发送用户名和密码过来 var user UserInfo err := c.ShouldBind(\u0026amp;user) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2001, \u0026#34;msg\u0026#34;: \u0026#34;无效的参数\u0026#34;, }) return } // 校验用户名和密码是否正确 if user.Username == \u0026#34;q1mi\u0026#34; \u0026amp;\u0026amp; user.Password == \u0026#34;q1mi123\u0026#34; { // 生成Token tokenString, _ := GenToken(user.Username) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;token\u0026#34;: tokenString}, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2002, \u0026#34;msg\u0026#34;: \u0026#34;鉴权失败\u0026#34;, }) return } // ParseToken 解析JWT func ParseToken(tokenString string) (*CustomClaims, error) { // 解析token // 如果是自定义Claim结构体则需要使用 ParseWithClaims 方法 token, err := jwt.ParseWithClaims(tokenString, \u0026amp;CustomClaims{}, func(token *jwt.Token) (i interface{}, err error) { // 直接使用标准的Claim则可以直接使用Parse方法 //token, err := jwt.Parse(tokenString, func(token *jwt.Token) (i interface{}, err error) { return CustomSecret, nil }) if err != nil { return nil, err } // 对token对象中的Claim进行类型断言 if claims, ok := token.Claims.(*CustomClaims); ok \u0026amp;\u0026amp; token.Valid { // 校验token return claims, nil } return nil, errors.New(\u0026#34;invalid token\u0026#34;) } // JWTAuthMiddleware 基于JWT的认证中间件 func JWTAuthMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带Token有三种方式 1.放在请求头 2.放在请求体 3.放在URI // 这里假设Token放在Header的Authorization中，并使用Bearer开头 // 这里的具体实现方式要依据你的实际业务情况决定 authHeader := c.Request.Header.Get(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2003, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth为空\u0026#34;, }) c.Abort() return } // 按空格分割 parts := strings.SplitN(authHeader, \u0026#34; \u0026#34;, 2) if !(len(parts) == 2 \u0026amp;\u0026amp; parts[0] == \u0026#34;Bearer\u0026#34;) { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2004, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth格式有误\u0026#34;, }) c.Abort() return } // parts[1]是获取到的tokenString，我们使用之前定义好的解析JWT的函数来解析它 mc, err := ParseToken(parts[1]) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2005, \u0026#34;msg\u0026#34;: \u0026#34;无效的Token\u0026#34;, }) c.Abort() return } // 将当前请求的username信息保存到请求的上下文c上 c.Set(\u0026#34;username\u0026#34;, mc.Username) c.Next() // 后续的处理函数可以用过c.Get(\u0026#34;username\u0026#34;)来获取当前请求的用户信息 } } func homeHandler(c *gin.Context) { username := c.MustGet(\u0026#34;username\u0026#34;).(string) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;username\u0026#34;: username}, }) } func main() { r := gin.Default() r.POST(\u0026#34;/auth\u0026#34;, authHandler) r.GET(\u0026#34;/home\u0026#34;, JWTAuthMiddleware(), homeHandler) r.Run() } swagger接口文档 用法步骤 按照swagger要求给接口代码添加声明式注释，具体参照声明式注释格式。 使用swag工具扫描代码自动生成API接口文档数据 使用gin-swagger渲染在线接口文档页面 安装swag工具，在注释编写完成后使用 go get -u github.com/swaggo/swag/cmd/swag swag init # 会生成docs目录 #./docs #├── docs.go #├── swagger.json #└── swagger.yaml 引入gin-swagger import ( _ \u0026#34;GinJwt/docs\u0026#34; // 千万不要忘了导入把你上一步生成的docs \u0026#34;github.com/gin-gonic/gin\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; gs \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) 下载三方包 go get github.com/swaggo/gin-swagger go get github.com/swaggo/files package main import ( \u0026#34;errors\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;GinJwt/docs\u0026#34; // 千万不要忘了导入把你上一步生成的docs \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v4\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; gs \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) // jwt 过期时间 const TokenExpireDuration = time.Hour * 2 // CustomSecret 用于加盐的字符串 var CustomSecret = []byte(\u0026#34;夏天夏天悄悄过去\u0026#34;) type UserInfo struct { Username string Password string } type CustomClaims struct { // 可根据需要自行添加字段 Username string `json:\u0026#34;username\u0026#34;` jwt.RegisteredClaims // 内嵌标准的声明 } // GenToken 生成JWT func GenToken(username string) (string, error) { // 创建一个我们自己的声明 claims := CustomClaims{ username, // 自定义字段 jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(TokenExpireDuration)), Issuer: \u0026#34;my-project\u0026#34;, // 签发人 }, } // 使用指定的签名方法创建签名对象 token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) // 使用指定的secret签名并获得完整的编码后的字符串token return token.SignedString(CustomSecret) } func authHandler(c *gin.Context) { // 用户发送用户名和密码过来 var user UserInfo err := c.ShouldBind(\u0026amp;user) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2001, \u0026#34;msg\u0026#34;: \u0026#34;无效的参数\u0026#34;, }) return } // 校验用户名和密码是否正确 if user.Username == \u0026#34;q1mi\u0026#34; \u0026amp;\u0026amp; user.Password == \u0026#34;q1mi123\u0026#34; { // 生成Token tokenString, _ := GenToken(user.Username) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;token\u0026#34;: tokenString}, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2002, \u0026#34;msg\u0026#34;: \u0026#34;鉴权失败\u0026#34;, }) return } // ParseToken 解析JWT func ParseToken(tokenString string) (*CustomClaims, error) { // 解析token // 如果是自定义Claim结构体则需要使用 ParseWithClaims 方法 token, err := jwt.ParseWithClaims(tokenString, \u0026amp;CustomClaims{}, func(token *jwt.Token) (i interface{}, err error) { // 直接使用标准的Claim则可以直接使用Parse方法 //token, err := jwt.Parse(tokenString, func(token *jwt.Token) (i interface{}, err error) { return CustomSecret, nil }) if err != nil { return nil, err } // 对token对象中的Claim进行类型断言 if claims, ok := token.Claims.(*CustomClaims); ok \u0026amp;\u0026amp; token.Valid { // 校验token return claims, nil } return nil, errors.New(\u0026#34;invalid token\u0026#34;) } // JWTAuthMiddleware 基于JWT的认证中间件 func JWTAuthMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带Token有三种方式 1.放在请求头 2.放在请求体 3.放在URI // 这里假设Token放在Header的Authorization中，并使用Bearer开头 // 这里的具体实现方式要依据你的实际业务情况决定 authHeader := c.Request.Header.Get(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2003, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth为空\u0026#34;, }) c.Abort() return } // 按空格分割 parts := strings.SplitN(authHeader, \u0026#34; \u0026#34;, 2) if !(len(parts) == 2 \u0026amp;\u0026amp; parts[0] == \u0026#34;Bearer\u0026#34;) { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2004, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth格式有误\u0026#34;, }) c.Abort() return } // parts[1]是获取到的tokenString，我们使用之前定义好的解析JWT的函数来解析它 mc, err := ParseToken(parts[1]) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2005, \u0026#34;msg\u0026#34;: \u0026#34;无效的Token\u0026#34;, }) c.Abort() return } // 将当前请求的username信息保存到请求的上下文c上 c.Set(\u0026#34;username\u0026#34;, mc.Username) c.Next() // 后续的处理函数可以用过c.Get(\u0026#34;username\u0026#34;)来获取当前请求的用户信息 } } // homeHandler 测试jwt功能接口 // @Summary 测试jwt功能接口 // @Description 测试jwt功能接口 // @Tags 测试jwt功能接口 // @Accept application/json // @Produce application/json // @Param Authorization header string false \u0026#34;Bearer 用户令牌\u0026#34; // @Security ApiKeyAuth // @Success 200 // @Router /home [get] {\u0026#34;username\u0026#34;: username} func homeHandler(c *gin.Context) { username := c.MustGet(\u0026#34;username\u0026#34;).(string) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;username\u0026#34;: username}, }) } func main() { r := gin.Default() r.POST(\u0026#34;/auth\u0026#34;, authHandler) r.GET(\u0026#34;/home\u0026#34;, JWTAuthMiddleware(), homeHandler) r.GET(\u0026#34;/swagger/*any\u0026#34;, gs.WrapHandler(swaggerFiles.Handler)) r.Run() } 访问地址 http://localhost:8080/swagger/index.html\nGolang三方库 gopsutil psutil是一个跨平台进程和系统监控的Python库，而gopsutil是其Go语言版本的实现。本文介绍了它的基本使用。\nGo语言部署简单、性能好的特点非常适合做一些诸如采集系统信息和监控的服务，本文介绍的gopsutil库是知名Python库：psutil的一个Go语言版本的实现。\n安装 go get github.com/shirou/gopsutil windows额外安装\ngo get github.com/yusufpapurcu/wmi go get golang.org/x/sys/windows 使用 采集CPU相关信息 import \u0026#34;github.com/shirou/gopsutil/cpu\u0026#34; import \u0026#34;github.com/shirou/gopsutil/load\u0026#34; import \u0026#34;github.com/shirou/gopsutil/mem\u0026#34; import \u0026#34;github.com/shirou/gopsutil/host\u0026#34; import \u0026#34;github.com/shirou/gopsutil/disk\u0026#34; import \u0026#34;github.com/shirou/gopsutil/net\u0026#34; // cpu info func getCpuInfo() { cpuInfos, err := cpu.Info() if err != nil { fmt.Printf(\u0026#34;get cpu info failed, err:%v\u0026#34;, err) } for _, ci := range cpuInfos { fmt.Println(ci) } // CPU使用率 for { percent, _ := cpu.Percent(time.Second, false) fmt.Printf(\u0026#34;cpu percent:%v\\n\u0026#34;, percent) } } func getCpuLoad() { info, _ := load.Avg() fmt.Printf(\u0026#34;%v\\n\u0026#34;, info) } // mem info func getMemInfo() { memInfo, _ := mem.VirtualMemory() fmt.Printf(\u0026#34;mem info:%v\\n\u0026#34;, memInfo) } // host info func getHostInfo() { hInfo, _ := host.Info() fmt.Printf(\u0026#34;host info:%v uptime:%v boottime:%v\\n\u0026#34;, hInfo, hInfo.Uptime, hInfo.BootTime) } // disk info func getDiskInfo() { parts, err := disk.Partitions(true) if err != nil { fmt.Printf(\u0026#34;get Partitions failed, err:%v\\n\u0026#34;, err) return } for _, part := range parts { fmt.Printf(\u0026#34;part:%v\\n\u0026#34;, part.String()) diskInfo, _ := disk.Usage(part.Mountpoint) fmt.Printf(\u0026#34;disk info:used:%v free:%v\\n\u0026#34;, diskInfo.UsedPercent, diskInfo.Free) } ioStat, _ := disk.IOCounters() for k, v := range ioStat { fmt.Printf(\u0026#34;%v:%v\\n\u0026#34;, k, v) } } // net IO func getNetInfo() { info, _ := net.IOCounters(true) for index, v := range info { fmt.Printf(\u0026#34;%v:%v send:%v recv:%v\\n\u0026#34;, index, v, v.BytesSent, v.BytesRecv) } } // get ip func GetLocalIP() (ip string, err error) { addrs, err := net.InterfaceAddrs() if err != nil { return } for _, addr := range addrs { ipAddr, ok := addr.(*net.IPNet) if !ok { continue } if ipAddr.IP.IsLoopback() { continue } if !ipAddr.IP.IsGlobalUnicast() { continue } return ipAddr.IP.String(), nil } return } 适用工具，上传下载文件 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var ( FileDir = flag.String(\u0026#34;d\u0026#34;, \u0026#34;D:\\\\golang_project\\\\src\\\\uploadDownload\\\\file_dir\u0026#34;, `-d 绝对路径 指定上传和下载的目录`) Port = flag.Int(\u0026#34;p\u0026#34;, 8080,`-p 监听端口号 指定程序运行端口号`) ) func main() { flag.Parse() router := gin.Default() // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026#34;/upload\u0026#34;, func(c *gin.Context) { // Multipart form form, _ := c.MultipartForm() files := form.File[\u0026#34;file\u0026#34;] for _, file := range files { log.Println(file.Filename) dst := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, *FileDir, file.Filename) fmt.Println(dst) // 上传文件到指定的目录 c.SaveUploadedFile(file, dst) } c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: fmt.Sprintf(\u0026#34;%d files uploaded!\u0026#34;, len(files)), }) }) router.GET(\u0026#34;/listfile\u0026#34;, func(c *gin.Context) { fileInfoList, err := ioutil.ReadDir(*FileDir) var FileList []string for i := range fileInfoList { FileList = append(FileList,fileInfoList[i].Name()) } if err != nil { c.JSON(http.StatusBadRequest, gin.H{ \u0026#34;code\u0026#34;: 400, \u0026#34;msg\u0026#34;: \u0026#34;Error\u0026#34;, }) }else { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 200, \u0026#34;file_list\u0026#34;: FileList, }) } }) router.GET(\u0026#34;/download/:file\u0026#34;, func(c *gin.Context) { filename := c.Param(\u0026#34;file\u0026#34;) filepath := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, *FileDir, filename) c.File(filepath) }) port := fmt.Sprintf(\u0026#34;:%d\u0026#34;,*Port) router.Run(port) } 常用函数 时间和日期相关函数 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 日期和时间相关的函数 now := time.Now() fmt.Printf(\u0026#34;%v \\n\u0026#34;, now) //2022-08-16 17:08:36.0952186 +0800 CST m=+0.007682101 // 获取时间其他相关信息 fmt.Printf(\u0026#34;%v-%v-%v %v:%v:%v\\n\u0026#34;, now.Year(), int(now.Month()), now.Day(), now.Hour(), now.Minute(), now.Second()) // 格式化日期和时间 // 1、printf fmtTime := fmt.Sprintf(\u0026#34;%v-%v-%v %v:%v:%v\u0026#34;, now.Year(), int(now.Month()), now.Day(), now.Hour(), now.Minute(), now.Second()) fmt.Println(fmtTime) // 2、使用官方的format函数 (推荐) fmt.Println(time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;)) fmt.Println(time.Now().Format(\u0026#34;01-02\u0026#34;)) //时间 to 时间戳 loc, _ := time.LoadLocation(\u0026#34;Asia/Shanghai\u0026#34;) //设置时区 tt, _ := time.ParseInLocation(\u0026#34;2006-01-02 15:04:05\u0026#34;, \u0026#34;2018-07-11 15:07:51\u0026#34;, loc) //2006-01-02 15:04:05是转换的格式如php的\u0026#34;Y-m-d H:i:s\u0026#34; fmt.Println(tt.Unix()) // 时间常量 const ( ns = time.Nanosecond us = time.Microsecond ms = time.Millisecond ss = time.Second ) // 时间戳 Unix UnixNano 纳秒 获取随机数字 fmt.Printf(\u0026#34;unix时间戳=%v unixnano时间戳=%v\\n\u0026#34;, now.Unix(), now.UnixNano()) // 时间戳转日期 needConvertUnix := now.Unix() needConvertUnixTime := time.Unix(needConvertUnix, 0).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;时间戳:%v is %v\\n\u0026#34;, needConvertUnix, needConvertUnixTime) // 时间加减 // 一天前的时间 oneDayBefore, _ := time.ParseDuration(\u0026#34;-24h\u0026#34;) d1 := now.Add(oneDayBefore).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;昨天的日期：%v \\n\u0026#34;, d1) d2 := now.AddDate(0, -1, 0).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;上月的日期：%v\u0026#34;, d2) } os标准库 // 遍历所有环境遍历 envs := os.Environ() for _, value := range envs { cache := strings.Split(value, \u0026#34;=\u0026#34;) fmt.Println(cache) } // 获取指定名称的环境变量 golandEnv := os.Getenv(\u0026#34;GoLand\u0026#34;) fmt.Printf(\u0026#34;golandEnv is %s\\n\u0026#34;, golandEnv) // 打印主机名 fmt.Println(os.Hostname()) // 获取当前路径 filePath, _ := os.Getwd() fmt.Println(filePath) bytes标准库 func TestConvertAb(t *testing.T) { var b = []byte(\u0026#34;abcaBSADASD12-2131\u0026#34;) // 转大写 upper := bytes.ToUpper(b) // 转小写 lower := bytes.ToLower(b) fmt.Println(string(b), string(upper), string(lower)) var c = []byte(\u0026#34;ABC\u0026#34;) var d = []byte(\u0026#34;ABc\u0026#34;) // 忽略大小写比较 if bytes.EqualFold(c, d) { fmt.Printf(\u0026#34;c:%v,d:%v c=d成立\\n\u0026#34;, c, d) } } ","permalink":"https://wandong1.github.io/post/golang/","summary":"推荐博客 https://www.liwenzhou.com/posts/Go/golang-menu/\nLinux 安装go语言环境 # 下载地址 https://golang.google.cn/dl/ tar -xvzf go1.17.11.linux-386.tar.gz -C /usr/local yum install glibc.i686 -y mkdir -p /root/workspace cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPATH=\u0026#34;$HOME/workspace\u0026#34; EOF source /etc/profile #设置国内代理 go env -w GOPROXY=https://goproxy.cn,direct #查看go env go env Linux下创建一个go项目 #查看GOPATH go env | grep -i gopath #GOPATH=\u0026#34;/root/workspace\u0026#34; cd /root/workspace;mkdir {src,bin,pkg} #进入src目录创建项目 cd src \u0026amp;\u0026amp; mkdir GoRedis #随后编写main.go文件 构建多平台运行代码 go env -w GOOS=linux go env -w GOARCH=amd64 go build go env -w GOOS=windwos go env -w GOARCH=amd64 go build main.","title":"Golang学习笔记"},{"content":"golang定时任务系统 项目地址： https://github.com/ouqiang/gocron/releases 部署方法 https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-node-v1.5.3-linux-amd64.tar.gz\nhttps://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-v1.5.3-linux-amd64.tar.gz\n项目分为两个包 gocron-node和gocron，gocron-node为任务节点，实际执行任务，gocron为web端\n创建gocron用户 useradd gocron 解压软件包并运行 tar -xvzf gocron-node-v1.5.3-linux-amd64.tar.gz -C /home/gocron tar -xvzf gocron-v1.5.3-linux-amd64.tar.gz -C /home/gocron # 修改权限 chown gocron:gocron -R /home/gocron/ # 运行gocron web端 前台运行，监听5920端口 cd /home/gocron/gocron-linux-amd64 \u0026amp;\u0026amp; su gocron \u0026amp;\u0026amp; ./gocron web # 新起窗口运行gocron node任务节点 前台运行 cd /home/gocron/gocron-node-linux-amd64 su gocron \u0026amp;\u0026amp; ./gocron-node 1、登录web页面，访问 http://localhost:5920\n2、初始化数据库，并创建登录用户，注意数据库要单独使用新库，不能有其他表\n3、然后登录，添加任务节点，添加完成后测试连接\n![image-20220721155135950](D:\\typora Note\\assets\\image-20220721155135950.png)\n4、进入系统管理配置通知配置\n使用钉钉webhook进行通知\n![image-20220721155256643](D:\\typora Note\\assets\\image-20220721155256643.png)\n模板文件写法：\n{ \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;任务ID: {{.TaskId}} 任务名称: {{.TaskName}} 状态: {{.Status}} 执行结果: \\n{{.Result}}\u0026#34; }, \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34; } 5、新增定时任务\n![image-20220721155354483](D:\\typora Note\\assets\\image-20220721155354483.png)\nCrontab时间表达式 Linux-crontab时间表达式语法, 支持秒级任务定义 格式: 秒 分 时 天 月 周 示例： 1 * * * * * 每分钟第一秒运行 */20 * * * * * 每隔20秒运行一次 0 30 21 * * * 每天晚上21:30:00运行一次 0 0 23 * * 6 每周六晚上23:00:00 运行一次 快捷语法: @yearly 每年运行一次 @monthly 每月运行一次 @weekly 每周运行一次 @daily 每天运行一次 @midnight 每天午夜运行一次 @hourly 每小时运行一次 @every 30s 每隔30秒运行一次 @every 1m20s 每隔1分钟20秒运行一次 @every 3h5m10s 每隔3小时5分钟10秒运行一次 执行方式 shell: 在远程主机上执行shell命令 HTTP: 执行HTTP-GET请求 任务超时时间 任务执行超时，强制结束, 默认0，不限制 shell任务执行时间不能超过86400秒 HTTP任务执行时间不能超过300秒.\n任务执行失败重试次数 无法连接远程主机，shell返回值非0, HTTP响应码非200等异常返回, 可再次执行任务, 每次重试间隔时间 = 重试次数 * 1分钟 按1分钟、2分钟、3分钟\u0026hellip;..的间隔进行重试 取值范围1-10 例: 重试次数为2 任务执行失败, 休眠1分钟, 再次执行任务 再次执行失败, 休眠2分钟, 再次执行任务 默认0，不重试.\n开启安全 TLS双向认证（重要） 为了确保数据传输及任务节点gocron-node安全, 强烈建议开启\n下载证书制作工具 wget https://github.com/square/certstrap/releases/download/v1.1.1/certstrap-v1.1.1-linux-amd64 \u0026amp;\u0026amp; mv certstrap-v1.1.1-linux-amd64 /usr/bin/certstrap \u0026amp;\u0026amp; chmod +x /usr/bin/certstrap 生成证书 # 生成CA证书 ./certstrap init --common-name \u0026#34;Root CA\u0026#34; # 生成服务端(gocron-node)证书和私钥 ./certstrap request-cert --ip 10.43.152.50 ./certstrap sign --CA \u0026#34;Root CA\u0026#34; --years 20 10.43.152.50 # 生成客户端(gocron)证书和私钥 ./certstrap request-cert --ip 127.0.0.1 ./certstrap sign --CA \u0026#34;Root CA\u0026#34; --years 20 127.0.0.1 # 生成证书会到当前目录out文件下 # 修改私钥*.key权限,只能被运行gocron-node的用户读取 cd out/ chmod 600 10.43.152.50.key chmod 600 127.0.0.1.key # 拷贝密钥到对应文件夹 mkdir /home/gocron/gocron-linux-amd64/cert/ cp Root_CA.crt 10.43.152.50.crt 10.43.152.50.key /home/gocron/gocron-node-linux-amd64/ cp Root_CA.crt 127.0.0.1.crt 127.0.0.1.key /home/gocron/gocron-linux-amd64/cert/ 启动服务 # 启动node节点（使用非root用户） cd /home/gocron/gocron-node-linux-amd64/ ./gocron-node -enable-tls -ca-file Root_CA.crt -cert-file 10.43.152.50.crt -key-file 10.43.152.50.key # 先修改gocron web端 conf/app.ini配置文件 enable_tls = true ca_file = /home/gocron/gocron-linux-amd64/cert/Root_CA.crt cert_file = /home/gocron/gocron-linux-amd64/cert/127.0.0.1.crt key_file = /home/gocron/gocron-linux-amd64/cert/127.0.0.1.key # 启动gocron web端（使用非root用户） cd /home/gocron/gocron-linux-amd64/ \u0026amp;\u0026amp; ./gocron web 常见报错 docker容器内部发送消息失败\n![image-20220721234104982](D:\\typora Note\\assets\\image-20220721234104982.png)\nx509: certificate signed by unknown authority\n我们在构建 docker 镜像时一般使用的是 linux(centos或者ubuntu等待) 系统，默认是不带 ca-certificates 根证书的，导致无法识别外部 https 携带的数字证书。那么，在访问的时候就会抛出 x509: certificate signed by unknown authority 的错误，导致 docker 容器的接口服务返回 500。\nUbuntu构建镜像时安装ca-certificates包\napt-get -qq install -y --no-install-recommends ca-certificates curl ","permalink":"https://wandong1.github.io/post/golang%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%B3%BB%E7%BB%9F/","summary":"golang定时任务系统 项目地址： https://github.com/ouqiang/gocron/releases 部署方法 https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-node-v1.5.3-linux-amd64.tar.gz\nhttps://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-v1.5.3-linux-amd64.tar.gz\n项目分为两个包 gocron-node和gocron，gocron-node为任务节点，实际执行任务，gocron为web端\n创建gocron用户 useradd gocron 解压软件包并运行 tar -xvzf gocron-node-v1.5.3-linux-amd64.tar.gz -C /home/gocron tar -xvzf gocron-v1.5.3-linux-amd64.tar.gz -C /home/gocron # 修改权限 chown gocron:gocron -R /home/gocron/ # 运行gocron web端 前台运行，监听5920端口 cd /home/gocron/gocron-linux-amd64 \u0026amp;\u0026amp; su gocron \u0026amp;\u0026amp; ./gocron web # 新起窗口运行gocron node任务节点 前台运行 cd /home/gocron/gocron-node-linux-amd64 su gocron \u0026amp;\u0026amp; ./gocron-node 1、登录web页面，访问 http://localhost:5920\n2、初始化数据库，并创建登录用户，注意数据库要单独使用新库，不能有其他表\n3、然后登录，添加任务节点，添加完成后测试连接\n![image-20220721155135950](D:\\typora Note\\assets\\image-20220721155135950.png)\n4、进入系统管理配置通知配置\n使用钉钉webhook进行通知\n![image-20220721155256643](D:\\typora Note\\assets\\image-20220721155256643.png)\n模板文件写法：\n{ \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;任务ID: {{.","title":"golang定时任务系统"},{"content":"golang微服务 1、RPC 简介 ⚫ 远程过程调用（Remote Procedure Call，RPC）是一个计算机通信协议\n⚫ 该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额 外地为这个交互作用编程\n⚫ 如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方 法调用\n2、golang中如何实现RPC ⚫ golang 中实现 RPC 非常简单，官方提供了封装好的库，还有一些第三方的库\n⚫ golang 官方的 net/rpc 库使用 encoding/gob 进行编解码，支持 tcp 和 http 数据传输方 式，由于其他语言不支持 gob 编解码方式，所以 golang 的 RPC 只支持 golang 开发 的服务器与客户端之间的交互\n⚫ 官方还提供了 net/rpc/jsonrpc 库实现 RPC 方法，jsonrpc 采用 JSON 进行数据编解码， 因而支持跨语言调用，目前 jsonrpc 库是基于 tcp 协议实现的，暂不支持 http 传输 方式\n⚫ golang 的 RPC 必须符合 4 个条件才可以\n​\t◼ 结构体字段首字母要大写，要跨域访问，所以大写\n​\t◼ 函数名必须首字母大写（可以序列号导出的）\n​\t◼ 函数第一个参数是接收参数，第二个参数是返回给客户端参数，必须是指针类 型\n​\t◼ 函数必须有一个返回值 error\n⚫ 另外，net/rpc/jsonrpc 库通过 json 格式编解码，支持跨语言调用\n服务端代码： package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/rpc\u0026#34; ) // 服务端，求矩形面积和周长 // Rect 声明矩形对象 type Rect struct { } // Params 生命参数结构体，字段首字母大写 type Params struct { Width, Height int } // Area 求矩形面积的方法 func (r *Rect) Area(p Params, ret *int) error { *ret = p.Width * p.Height return nil } // Perimeter 求矩形面积的方法 func (r *Rect) Perimeter(p Params, ret *int) error { *ret = (p.Width + p.Height) * 2 return nil } func main() { // 1、注册服务 rect := new(Rect) rpc.Register(rect) // 2、把服务处理绑定到http协议上 rpc.HandleHTTP() // 3、监听服务，等待客户端调用 err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(err) } } 服务端代码（jsonrpc调用）： func main() { // 1、注册服务 rpc.Register(new(Rect)) // 2、把服务处理绑定到http协议上 lis,err := net.Listen(\u0026#34;tcp\u0026#34;,\u0026#34;127.0.0.1:8081\u0026#34;) if err != nil { log.Fatal(err) } //循环监听服务 for { conn, err := lis.Accept() if err != nil { continue } // 起协程 go func(conn net.Conn) { fmt.Println(\u0026#34;new a client\u0026#34;) jsonrpc.ServeConn(conn) }(conn) } } 客户端代码： package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/rpc\u0026#34; ) // Params 定义参数结构体 type Params struct { Width, Height int } // 调用服务 func main() { // 1、连接远程rpc服务 //http调用 rp, err := rpc.DialHTTP(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8080\u0026#34;) // json rpc 调用 // rp, err := jsonrpc.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8081\u0026#34;) if err != nil { log.Fatal(err) } // 2、调用远程方法 // 定义接收服务端传回来的计算结果的变量 var ret int // ret := 0 pArgs := Params{Width: 100, Height: 50} // 求面积 err2 := rp.Call(\u0026#34;Rect.Area\u0026#34;, pArgs, \u0026amp;ret) if err2 != nil { log.Fatal(err2) } fmt.Printf(\u0026#34;width: %d,height: %d \u0026#39;s area is %d\\n\u0026#34;, pArgs.Width, pArgs.Height, ret) // 求周长 err3 := rp.Call(\u0026#34;Rect.Perimeter\u0026#34;, pArgs, \u0026amp;ret) if err3 != nil { log.Fatal(err3) } fmt.Printf(\u0026#34;width: %d,height: %d \u0026#39;s perimeter is %d\\n\u0026#34;, pArgs.Width, pArgs.Height, ret) } ","permalink":"https://wandong1.github.io/post/golang%E5%BE%AE%E6%9C%8D%E5%8A%A1/","summary":"golang微服务 1、RPC 简介 ⚫ 远程过程调用（Remote Procedure Call，RPC）是一个计算机通信协议\n⚫ 该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额 外地为这个交互作用编程\n⚫ 如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方 法调用\n2、golang中如何实现RPC ⚫ golang 中实现 RPC 非常简单，官方提供了封装好的库，还有一些第三方的库\n⚫ golang 官方的 net/rpc 库使用 encoding/gob 进行编解码，支持 tcp 和 http 数据传输方 式，由于其他语言不支持 gob 编解码方式，所以 golang 的 RPC 只支持 golang 开发 的服务器与客户端之间的交互\n⚫ 官方还提供了 net/rpc/jsonrpc 库实现 RPC 方法，jsonrpc 采用 JSON 进行数据编解码， 因而支持跨语言调用，目前 jsonrpc 库是基于 tcp 协议实现的，暂不支持 http 传输 方式\n⚫ golang 的 RPC 必须符合 4 个条件才可以\n​\t◼ 结构体字段首字母要大写，要跨域访问，所以大写\n​\t◼ 函数名必须首字母大写（可以序列号导出的）","title":"golang微服务"},{"content":"golang日志框架zap package main import ( \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; \u0026#34;os\u0026#34; ) var logger *zap.Logger var sugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger = zap.New(core) sugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { file, _ := os.Create(\u0026#34;./test.log\u0026#34;) return zapcore.AddSync(file) } func main() { InitLogger() defer logger.Sync() defer sugarLogger.Sync() logger.Info(\u0026#34;日志记录成功\u0026#34;, zap.String(\u0026#34;service\u0026#34;, \u0026#34;logger service\u0026#34;)) logger.Error(\u0026#34;日志记录失败\u0026#34;, zap.String(\u0026#34;service\u0026#34;, \u0026#34;logger service\u0026#34;)) sugarLogger.Infof(\u0026#34;日志记录成功 服务：%s\u0026#34;, \u0026#34;logger service\u0026#34;) sugarLogger.Error(\u0026#34;日志记录失败\u0026#34;, \u0026#34;logger service\u0026#34;) } ","permalink":"https://wandong1.github.io/post/golang%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6zap/","summary":"golang日志框架zap package main import ( \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; \u0026#34;os\u0026#34; ) var logger *zap.Logger var sugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger = zap.New(core) sugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { file, _ := os.Create(\u0026#34;./test.log\u0026#34;) return zapcore.AddSync(file) } func main() { InitLogger() defer logger.Sync() defer sugarLogger.","title":"golang日志框架zap"},{"content":"jdk安装 dockerfile文件\n下载地址：https://www.oracle.com/java/technologies/downloads/#java8 jdk-8u341-linux-x64.tar.gz文件\nFROM ubuntu:latest ADD jdk-8u341-linux-x64.tar.gz /usr/local/ RUN mv /usr/local/jdk1.8.0_341 /usr/local/jdk1.8 ENV JAVA_HOME=/usr/local/jdk1.8 ENV JRE_HOME=${JAVA_HOME}/jre ENV CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH ENV JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin ENV PATH=$PATH:${JAVA_PATH} RUN apt-get update \u0026amp;\u0026amp; apt-get install curl tree iputils-ping net-tools iproute2 vim -y CMD [\u0026#34;java\u0026#34;,\u0026#34;-version\u0026#34;] ","permalink":"https://wandong1.github.io/post/jdk%E5%AE%89%E8%A3%85/","summary":"jdk安装 dockerfile文件\n下载地址：https://www.oracle.com/java/technologies/downloads/#java8 jdk-8u341-linux-x64.tar.gz文件\nFROM ubuntu:latest ADD jdk-8u341-linux-x64.tar.gz /usr/local/ RUN mv /usr/local/jdk1.8.0_341 /usr/local/jdk1.8 ENV JAVA_HOME=/usr/local/jdk1.8 ENV JRE_HOME=${JAVA_HOME}/jre ENV CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH ENV JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin ENV PATH=$PATH:${JAVA_PATH} RUN apt-get update \u0026amp;\u0026amp; apt-get install curl tree iputils-ping net-tools iproute2 vim -y CMD [\u0026#34;java\u0026#34;,\u0026#34;-version\u0026#34;] ","title":"jdk安装教程（容器）"},{"content":"jenkins构建go项目 一、配置jenkins 1、全局工具配置 将go安装包解压后，拷贝至以上的安装目录\n自由风格构建，选go的构建环境，然后就可以在shell中执行go命令了\n","permalink":"https://wandong1.github.io/post/jenkins%E6%9E%84%E5%BB%BAgo%E9%A1%B9%E7%9B%AE/","summary":"jenkins构建go项目 一、配置jenkins 1、全局工具配置 将go安装包解压后，拷贝至以上的安装目录\n自由风格构建，选go的构建环境，然后就可以在shell中执行go命令了","title":"jenkins构建go项目"},{"content":"项目源码介绍 使用nfs-subdir-external-provisioner github地址：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 详细介绍：https://artifacthub.io/packages/helm/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner 镜像地址：https://hub.docker.com/r/eipwork/nfs-subdir-external-provisioner/tags\nNFS服务端安装 # 安装nfs服务端 yum install nfs-utils -y vim /etc/exports /opt/nfsdata 192.168.0.0/24(rw,no_root_squash,no_all_squash,sync) # 刷新并验证 exportfs -rv # 启动nfs服务，共两个服务 systemctl enable rpcbind --now systemctl enable nfs --now 所有客户端也需要安装nfs-utils，安装完成即可，无需启动服务\nyum install nfs-utils -y storageClass插件安装 # 添加helm仓库地址 helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 安装第一个 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata \\ --set image.repository=eipwork/nfs-subdir-external-provisioner # 安装第二个(可选) helm install second-nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata2 \\ --set image.repository=eipwork/nfs-subdir-external-provisioner \\ --set storageClass.name=nfs-client-2 \\ --set storageClass.provisionerName=k8s-sigs.io/second-nfs-subdir-external-provisioner ","permalink":"https://wandong1.github.io/post/%E4%B8%BAk8s%E9%9B%86%E7%BE%A4%E6%B7%BB%E5%8A%A0nfs%E7%B1%BB%E5%9E%8B%E7%9A%84sotrageclass/","summary":"项目源码介绍 使用nfs-subdir-external-provisioner github地址：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 详细介绍：https://artifacthub.io/packages/helm/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner 镜像地址：https://hub.docker.com/r/eipwork/nfs-subdir-external-provisioner/tags\nNFS服务端安装 # 安装nfs服务端 yum install nfs-utils -y vim /etc/exports /opt/nfsdata 192.168.0.0/24(rw,no_root_squash,no_all_squash,sync) # 刷新并验证 exportfs -rv # 启动nfs服务，共两个服务 systemctl enable rpcbind --now systemctl enable nfs --now 所有客户端也需要安装nfs-utils，安装完成即可，无需启动服务\nyum install nfs-utils -y storageClass插件安装 # 添加helm仓库地址 helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 安装第一个 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata \\ --set image.repository=eipwork/nfs-subdir-external-provisioner # 安装第二个(可选) helm install second-nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata2 \\ --set image.","title":"为K8S集群添加nfs类型的sotrageClass"},{"content":"Centos7\nwget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo wget -O /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo ","permalink":"https://wandong1.github.io/post/centos%E6%9B%B4%E6%96%B0%E5%9B%BD%E5%86%85yum%E6%BA%90/","summary":"Centos7\nwget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo wget -O /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo ","title":"Centos更新国内yum源"},{"content":"ElasticSearch快速入门实战 主讲老师：Fox\nES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\n​ 文档：1.ElasticSearch快速入门实战.note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引） 文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）\n假如，我们有一个站内搜索的功能，通过某个关键词来搜索相关的文章，那么这个关键词可能出现在标题中，也可能出现在文章内容中，那我们将会在创建或修改文章的时候，建立一个关键词与文章的对应关系表，这种，我们可以称之为倒排索引。\nlike %java设计模式% java 设计模式\n关键词 文章ID JAVA 1,2 设计模式 1,2 多线程 2 简单理解，正向索引是通过key找value，反向索引则是通过value找key。ES底层在检索时底层使用的就是倒排索引。\nElasticSearch简介 ElasticSearch是什么 ElasticSearch（简称ES）是一个分布式、RESTful 风格的搜索和数据分析引擎，是用Java开发并且是当前最流行的开源的企业级搜索引擎，能够达到近实时搜索，稳定，可靠，快速，安装使用方便。\n客户端支持Java、.NET（C#）、PHP、Python、Ruby等多种语言。\n官方网站: https://www.elastic.co/\n**下载地址：**https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n搜索引擎排名：\n​ 参考网站：https://db-engines.com/en/ranking/search+engine\n起源——Lucene\n基于Java语言开发的搜索引擎库类\n创建于1999年，2005年成为Apache 顶级开源项目\nLucene具有高性能、易扩展的优点\nLucene的局限性︰\n只能基于Java语言开发 类库的接口学习曲线陡峭 原生并不支持水平扩展 Elasticsearch的诞生\nElasticsearch是构建在Apache Lucene之上的开源分布式搜索引擎。\n2004年 Shay Banon 基于Lucene开发了Compass\n2010年 Shay Banon重写了Compass，取名Elasticsearch\n支持分布式，可水平扩展 降低全文检索的学习曲线，可以被任何编程语言调用 ​ Elasticsearch 与 Lucene 核心库竞争的优势在于：\n完美封装了 Lucene 核心库，设计了友好的 Restful-API，开发者无需过多关注底层机制，直接开箱即用。 分片与副本机制，直接解决了集群下性能与高可用问题。 ES Server进程 3节点 raft (奇数节点)\n数据分片 -》lucene实例 分片和副本数 1个ES节点可以有多个lucene实例。也可以指定一个索引的多个分片\n​ ElasticSearch版本特性 5.x新特性\nLucene 6.x， 性能提升，默认打分机制从TF-IDF改为BM 25\n支持Ingest节点/ Painless Scripting / Completion suggested支持/原生的Java REST客户端\nType标记成deprecated， 支持了Keyword的类型\n性能优化\n内部引擎移除了避免同一文档并发更新的竞争锁，带来15% - 20%的性能提升 Instant aggregation,支持分片，上聚合的缓存 新增了Profile API 6.x新特性\nLucene 7.x\n新功能\n跨集群复制(CCR) 索引生命周期管理 SQL的支持 更友好的的升级及数据迁移\n在主要版本之间的迁移更为简化，体验升级 全新的基于操作的数据复制框架，可加快恢复数据 性能优化\n有效存储稀疏字段的新方法，降低了存储成本 在索引时进行排序，可加快排序的查询性能 7.x新特性\nLucene 8.0\n重大改进-正式废除单个索引下多Type的支持\n7.1开始，Security 功能免费使用\nECK - Elasticseach Operator on Kubernetes\n新功能\nNew Cluster coordination Feature——Complete High Level REST Client Script Score Query 性能优化\n默认的Primary Shard数从5改为1,避免Over Sharding 性能优化， 更快的Top K 8.x新特性\nRest API相比较7.x而言做了比较大的改动（比如彻底删除_type） 默认开启安全配置 存储空间优化：对倒排文件使用新的编码集，对于keyword、match_only_text、text类型字段有效，有3.5%的空间优化提升，对于新建索引和segment自动生效。 优化geo_point，geo_shape类型的索引（写入）效率：15%的提升。 技术预览版KNN API发布，（K邻近算法），跟推荐系统、自然语言排名相关。 https://www.elastic.co/guide/en/elastic-stack/current/elasticsearch-breaking-changes.html ElasticSearch vs Solr\nSolr 是第一个基于 Lucene 核心库功能完备的搜索引擎产品，诞生远早于 Elasticsearch。\n当单纯的对已有数据进行搜索时，Solr更快。当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。\n​ ​ 大型互联网公司，实际生产环境测试，将搜索引擎从Solr转到 Elasticsearch以后的平均查询速度有了50倍的提升。\n​ 总结：\nSolr 利用 Zookeeper 进行分布式管理，而Elasticsearch 自身带有分布式协调管理功能。 Solr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。 Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。 Solr 是传统搜索应用的有力解决方案，但 Elasticsearch更适用于新兴的实时搜索应用。 Elastic Stack介绍 在Elastic Stack之前我们听说过ELK，ELK分别是Elasticsearch，Logstash，Kibana这三款软件在一起的简称，在发展的过程中又有新的成员Beats的加入，就形成了Elastic Stack。\n​ Elastic Stack生态圈\n在Elastic Stack生态圈中Elasticsearch作为数据存储和搜索，是生态圈的基石，Kibana在上层提供用户一个可视化及操作的界面，Logstash和Beat可以对数据进行收集。在上图的右侧X-Pack部分则是Elastic公司提供的商业项目。\n指标分析/日志分析：\n​ ElasticSearch应用场景 站内搜索 日志管理与分析 大数据分析 应用性能监控 机器学习 国内现在有大量的公司都在使用 Elasticsearch，包括携程、滴滴、今日头条、饿了么、360安全、小米、vivo等诸多知名公司。除了搜索之外，结合Kibana、Logstash、Beats，Elastic Stack还被广泛运用在大数据近实时分析领域，包括日志分析、指标监控、信息安全等多个领域。它可以帮助你探索海量结构化、非结构化数据，按需创建可视化报表，对监控数据设置报警阈值，甚至通过使用机器学习技术，自动识别异常状况。\n通用数据处理流程：\n​ ElasticSearch快速开始 ElasticSearch安装运行 环境准备\n运行Elasticsearch，需安装并配置JDK\n设置$JAVA_HOME 各个版本对Java的依赖 https://www.elastic.co/support/matrix#matrix_jvm\nElasticsearch 5需要Java 8以上的版本 Elasticsearch 从6.5开始支持Java 11 7.0开始，内置了Java环境 ES比较耗内存，建议虚拟机4G或以上内存，jvm1g以上的内存分配\n可以参考es的环境文件elasticsearch-env.bat\n​ ES的jdk环境生效的优先级配置ES_JAVA_HOME\u0026gt;JAVA_HOME\u0026gt;ES_HOME\n下载并解压ElasticSearch\n下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n选择版本：7.17.3\n​ ElasticSearch文件目录结构\n目录 描述 bin 脚本文件，包括启动elasticsearch，安装插件，运行统计数据等 config 配置文件目录，如elasticsearch配置、角色配置、jvm配置等。 jdk java运行环境 data 默认的数据存放目录，包含节点、分片、索引、文档的所有数据，生产环境需要修改。 lib elasticsearch依赖的Java类库 logs 默认的日志文件存储路径，生产环境需要修改。 modules 包含所有的Elasticsearch模块，如Cluster、Discovery、Indices等。 plugins 已安装插件目录 主配置文件elasticsearch.yml\ncluster.name 当前节点所属集群名称，多个节点如果要组成同一个集群，那么集群名称一定要配置成相同。默认值elasticsearch，生产环境建议根据ES集群的使用目的修改成合适的名字。\nnode.name 当前节点名称，默认值当前节点部署所在机器的主机名，所以如果一台机器上要起多个ES节点的话，需要通过配置该属性明确指定不同的节点名称。\npath.data 配置数据存储目录，比如索引数据等，默认值 $ES_HOME/data，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\npath.logs 配置日志存储目录，比如运行日志和集群健康信息等，默认值 $ES_HOME/logs，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\nbootstrap.memory_lock 配置ES启动时是否进行内存锁定检查，默认值true。\nES对于内存的需求比较大，一般生产环境建议配置大内存，如果内存不足，容易导致内存交换到磁盘，严重影响ES的性能。所以默认启动时进行相应大小内存的锁定，如果无法锁定则会启动失败。\n非生产环境可能机器内存本身就很小，能够供给ES使用的就更小，如果该参数配置为true的话很可能导致无法锁定内存以致ES无法成功启动，此时可以修改为false。\nnetwork.host 配置能够访问当前节点的主机，默认值为当前节点所在机器的本机回环地址127.0.0.1 和[::1]，这就导致默认情况下只能通过当前节点所在主机访问当前节点。可以配置为 0.0.0.0 ，表示所有主机均可访问。\nhttp.port 配置当前ES节点对外提供服务的http端口，默认值 9200\ndiscovery.seed_hosts 配置参与集群节点发现过程的主机列表，说白一点就是集群中所有节点所在的主机列表，可以是具体的IP地址，也可以是可解析的域名。\ncluster.initial_master_nodes 配置ES集群初始化时参与master选举的节点名称列表，必须与node.name配置的一致。ES集群首次构建完成后，应该将集群中所有节点的配置文件中的cluster.initial_master_nodes配置项移除，重启集群或者将新节点加入某个已存在的集群时切记不要设置该配置项。\n​ #ES开启远程访问 network.host: 0.0.0.0\n修改JVM配置\n修改config/jvm.options配置文件，调整jvm堆内存大小\n​ vim jvm.options -Xms4g -Xmx4g\n配置的建议\nXms和Xms设置成—样 Xmx不要超过机器内存的50% 不要超过30GB - https://www.elastic.co/cn/blog/a-heap-of-trouble 启动ElasticSearch服务 Windows\n直接运行elasticsearch.bat\nLinux（centos7）\nES不允许使用root账号启动服务，如果你当前账号是root，则需要创建一个专有账户\n​ #非root用户 bin/elasticsearch # -d 后台启动 bin/elasticsearch -d\n​ 注意：es默认不能用root用户启动，生产环境建议为elasticsearch创建用户。\n​ #为elaticsearch创建用户并赋予相应权限 adduser es passwd es chown -R es:es elasticsearch-17.3\n运行http://localhost:9200/\n​ 如果ES服务启动异常，会有提示：\n​ 启动ES服务常见错误解决方案 [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\nES因为需要大量的创建索引文件，需要大量的打开系统的文件，所以我们需要解除linux系统当中打开文件最大数目的限制，不然ES启动就会抛错\n​ #切换到root用户 vim /etc/security/limits.conf 末尾添加如下配置： *\tsoft nofile 65536 * hard nofile 65536 * soft nproc 4096 *\thard nproc 4096\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n无法创建本地线程问题,用户最大可创建线程数太小\n​ vim /etc/security/limits.d/20-nproc.conf 改为如下配置： * soft nproc 4096\n[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n最大虚拟内存太小,调大系统的虚拟内存\n​ vim /etc/sysctl.conf 追加以下内容： vm.max_map_count=262144 保存退出之后执行如下命令： sysctl -p\n[4]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n缺少默认配置，至少需要配置discovery.seed_hosts/discovery.seed_providers/cluster.initial_master_nodes中的一个参数.\ndiscovery.seed_hosts: 集群主机列表 discovery.seed_providers: 基于配置文件配置集群主机列表 cluster.initial_master_nodes: 启动时初始化的参与选主的node，生产环境必填 ​ vim config/elasticsearch.yml #添加配置 discovery.seed_hosts: [\u0026ldquo;127.0.0.1\u0026rdquo;] cluster.initial_master_nodes: [\u0026ldquo;node-1\u0026rdquo;] #或者 单节点（集群单节点） discovery.type: single-node\n客户端Kibana安装 Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。\n1）下载并解压缩Kibana\n下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana\n选择版本：7.17.3\n​ 2）修改Kibana.yml\n​ vim config/kibana.yml server.port: 5601 server.host: \u0026ldquo;localhost\u0026rdquo; #服务器ip elasticsearch.hosts: [\u0026ldquo;http://localhost:9200\u0026rdquo;] #elasticsearch的访问地址 i18n.locale: \u0026ldquo;zh-CN\u0026rdquo; #Kibana汉化\n3）运行Kibana\n注意：kibana也需要非root用户启动\n​ bin/kibana #后台启动 nohup bin/kibana \u0026amp;\n访问Kibana: http://localhost:5601/\n​ cat API\n​ /_cat/allocation #查看单节点的shard分配整体情况 /_cat/shards #查看各shard的详细情况 /_cat/shards/{index} #查看指定分片的详细情况 /_cat/master #查看master节点信息 /_cat/nodes #查看所有节点信息 /_cat/indices #查看集群中所有index的详细信息 /_cat/indices/{index} #查看集群中指定index的详细信息 /_cat/segments #查看各index的segment详细信息,包括segment名, 所属shard, 内存(磁盘)占用大小, 是否刷盘 /_cat/segments/{index}#查看指定index的segment详细信息 /_cat/count #查看当前集群的doc数量 /_cat/count/{index} #查看指定索引的doc数量 /_cat/recovery #查看集群内每个shard的recovery过程.调整replica。 /_cat/recovery/{index}#查看指定索引shard的recovery过程 /_cat/health #查看集群当前状态：红、黄、绿 /_cat/pending_tasks #查看当前集群的pending task /_cat/aliases #查看集群中所有alias信息,路由配置等 /_cat/aliases/{alias} #查看指定索引的alias信息 /_cat/thread_pool #查看集群各节点内部不同类型的threadpool的统计信息, /_cat/plugins #查看集群各个节点上的plugin信息 /_cat/fielddata #查看当前集群各个节点的fielddata内存使用情况 /_cat/fielddata/{fields} #查看指定field的内存使用情况,里面传field属性对应的值 /_cat/nodeattrs #查看单节点的自定义属性 /_cat/repositories #输出集群中注册快照存储库 /_cat/templates #输出当前正在存在的模板信息\nElasticsearch安装分词插件 Elasticsearch提供插件机制对系统进行扩展\n以安装analysis-icu这个分词插件为例\n在线安装\n​ #查看已安装插件 bin/elasticsearch-plugin list #安装插件 bin/elasticsearch-plugin install analysis-icu #删除插件 bin/elasticsearch-plugin remove analysis-icu\n注意：安装和删除完插件后，需要重启ES服务才能生效。\n测试分词效果\n​ POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;icu_analyzer\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n​ 离线安装\n本地下载相应的插件，解压，然后手动上传到elasticsearch的plugins目录，然后重启ES实例就可以了。\n比如ik中文分词插件：https://github.com/medcl/elasticsearch-analysis-ik\n测试分词效果\n​ #ES的默认分词设置是standard，会单字拆分 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;standard\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; } #ik_smart:会做最粗粒度的拆 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;: \u0026ldquo;ik_smart\u0026rdquo;, \u0026ldquo;text\u0026rdquo;: \u0026ldquo;中华人民共和国\u0026rdquo; } #ik_max_word:会将文本做最细粒度的拆分 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;ik_max_word\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n创建索引时可以指定IK分词器作为默认分词器\n​ PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;analysis.analyzer.default.type\u0026rdquo;: \u0026ldquo;ik_max_word\u0026rdquo; } } }\n​ ElasticSearch基本概念 关系型数据库 VS ElasticSearch 在7.0之前，一个 Index可以设置多个Types\n目前Type已经被Deprecated，7.0开始，一个索引只能创建一个Type - “_doc”\n传统关系型数据库和Elasticsearch的区别:\nElasticsearch- Schemaless /相关性/高性能全文检索 RDMS —事务性/ Join ​ 索引（Index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。\n一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。\n​ 文档（Document） Elasticsearch是面向文档的，文档是所有可搜索数据的最小单位。\n日志文件中的日志项 一本电影的具体信息/一张唱片的详细信息 MP3播放器里的一首歌/一篇PDF文档中的具体内容 文档会被序列化成JSON格式，保存在Elasticsearch中\nJSON对象由字段组成 每个字段都有对应的字段类型(字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID\n可以自己指定ID或者通过Elasticsearch自动生成 一篇文档包含了一系列字段，类似数据库表中的一条记录\nJSON文档，格式灵活，不需要预先定义格式\n字段的类型可以指定或者通过Elasticsearch自动推算 支持数组/支持嵌套 文档元数据\n​ 元数据，用于标注文档的相关信息：\n_index：文档所属的索引名 _type：文档所属的类型名 _id：文档唯—ld _source: 文档的原始Json数据 _version: 文档的版本号，修改删除操作_version都会自增1 _seq_no: 和_version一样，一旦数据发生更改，数据也一直是累计的。Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。 _primary_term: _primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。 ElasticSearch索引操作 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index.html\n创建索引\n索引命名必须小写，不能以下划线开头\n格式: PUT /索引名称\n​ #创建索引 PUT /es_db #创建索引时可以设置分片数和副本数 PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;number_of_shards\u0026rdquo; : 3, \u0026ldquo;number_of_replicas\u0026rdquo; : 2 } } #修改索引配置 PUT /es_db/_settings { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;number_of_replicas\u0026rdquo; : 1 } }\n​ 查询索引\n格式: GET /索引名称\n​ #查询索引 GET /es_db #es_db是否存在 HEAD /es_db\n​ ​\n删除索引\n格式: DELETE /索引名称\n​ DELETE /es_db\nElasticSearch文档操作 示例数据\n​ PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;analysis.analyzer.default.type\u0026rdquo;: \u0026ldquo;ik_max_word\u0026rdquo; } } } PUT /es_db/_doc/1 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; } PUT /es_db/_doc/2 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;李四\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 28, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州荔湾大厦\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java assistant\u0026rdquo; } PUT /es_db/_doc/3 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;王五\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 26, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州白云山公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;php developer\u0026rdquo; } PUT /es_db/_doc/4 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;赵六\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 22, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙橘子洲\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;python assistant\u0026rdquo; } PUT /es_db/_doc/5 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张龙\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 19, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙麓谷企业广场\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java architect assistant\u0026rdquo; }\tPUT /es_db/_doc/6 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;赵虎\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 32, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙麓谷兴工国际产业园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java architect\u0026rdquo; }\n添加（索引）文档\n格式: [PUT | POST] /索引名称/[_doc | _create ]/id ​ # 创建文档,指定id # 如果id不存在，创建新的文档，否则先删除现有文档，再创建新的文档，版本会增加 PUT /es_db/_doc/1 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; }\t#创建文档，ES生成id POST /es_db/_doc { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; }\n​ 注意:POST和PUT都能起到创建/更新的作用，PUT需要对一个具体的资源进行操作也就是要确定id才能进行更新/创建，而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建/更新\n​ Create -如果ID已经存在，会失败\n​ 修改文档\n全量更新，整个json都会替换，格式: [PUT | POST] /索引名称/_doc/id 如果文档存在，现有文档会被删除，新的文档会被索引\n​ # 全量更新，替换整个json PUT /es_db/_doc/1/ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25 } #查询文档 GET /es_db/_doc/1\n​ 使用_update部分更新，格式: POST /索引名称/_update/id update不会删除原来的文档，而是实现真正的数据更新\n​ # 部分更新：在原有文档上更新 # Update -文档必须已经存在，更新只会对相应字段做增量修改 POST /es_db/_update/1 { \u0026ldquo;doc\u0026rdquo;: { \u0026ldquo;age\u0026rdquo;: 28 } } #查询文档 GET /es_db/_doc/1\n​ 使用 _update_by_query 更新文档 ​ POST /es_db/_update_by_query { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;match\u0026rdquo;: { \u0026ldquo;_id\u0026rdquo;: 1 } }, \u0026ldquo;script\u0026rdquo;: { \u0026ldquo;source\u0026rdquo;: \u0026ldquo;ctx._source.age = 30\u0026rdquo; } }\n​ 并发场景下修改文档\n_seq_no和_primary_term是对_version的优化，7.X版本的ES默认使用这种方式控制版本，所以当在高并发环境下使用乐观锁机制修改文档时，要带上当前文档的_seq_no和_primary_term进行更新：\n​ POST /es_db/_doc/2?if_seq_no=21\u0026amp;if_primary_term=6 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;李四xxx\u0026rdquo; }\n如果版本号不对，会抛出版本冲突异常，如下图：\n​ 查询文档\n根据id查询文档，格式: GET /索引名称/_doc/id ​ GET /es_db/_doc/1\n条件查询 _search，格式： /索引名称/_doc/_search ​ # 查询前10条文档 GET /es_db/_doc/_search\nES Search API提供了两种条件查询搜索方式：\nREST风格的请求URI，直接将参数带过去 封装到request body中，这种方式可以定义更加易读的JSON格式 ​ #通过URI搜索，使用“q”指定查询字符串，“query string syntax” KV键值对 #条件查询, 如要查询age等于28岁的 _search?q=:** GET /es_db/_doc/_search?q=age:28 #范围查询, 如要查询age在25至26岁之间的 _search?q=[ TO **] 注意: TO 必须为大写 GET /es_db/_doc/_search?q=age[25 TO 26] #查询年龄小于等于28岁的 :\u0026lt;= GET /es_db/_doc/_search?q=age:\u0026lt;=28 #查询年龄大于28前的 :\u0026gt; GET /es_db/_doc/_search?q=age:\u0026gt;28 #分页查询 from=\u0026amp;size=* GET /es_db/_doc/_search?q=age[25 TO 26]\u0026amp;from=0\u0026amp;size=1 #对查询结果只输出某些字段 _source=字段,字段 GET /es_db/_doc/_search?_source=name,age #对查询结果排序 sort=字段:desc/asc GET /es_db/_doc/_search?sort=age:desc\n通过请求体的搜索方式会在后面课程详细讲解（DSL）\n​ GET /es_db/_search { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;match\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州白云\u0026rdquo; } } }\n删除文档\n格式: DELETE /索引名称/_doc/id\n​ DELETE /es_db/_doc/1\nElasticSearch文档批量操作\n批量操作可以减少网络连接所产生的开销，提升性能\n支持在一次API调用中，对不同的索引进行操作 可以在URI中指定Index，也可以在请求的Payload中进行 操作中单条操作失败，并不会影响其他操作 返回结果包括了每一条操作执行的结果 批量写入\n批量对文档进行写操作是通过_bulk的API来实现的\n请求方式：POST\n请求地址：_bulk\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n第一行参数为指定操作的类型及操作的对象(index,type和id) 第二行参数才是操作的数据 参数类似于：\n​ {\u0026ldquo;actionName\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;indexName\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026ldquo;typeName\u0026rdquo;,\u0026quot;_id\u0026quot;:\u0026ldquo;id\u0026rdquo;}} {\u0026ldquo;field1\u0026rdquo;:\u0026ldquo;value1\u0026rdquo;, \u0026ldquo;field2\u0026rdquo;:\u0026ldquo;value2\u0026rdquo;}\nactionName：表示操作类型，主要有create,index,delete和update 批量创建文档create\n​ POST _bulk {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;fox老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;fox老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;id\u0026rdquo;:4,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;mark老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;mark老师NB\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530}\n普通创建或全量替换index\n​ POST _bulk {\u0026ldquo;index\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;图灵徐庶老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;图灵学院徐庶老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;index\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;id\u0026rdquo;:4,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;图灵诸葛老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;图灵学院诸葛老师NB\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530}\n如果原文档不存在，则是创建 如果原文档存在，则是替换(全量修改原文档) 批量删除delete\n​ POST _bulk {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}}\n批量修改update\n​ POST _bulk {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;title\u0026rdquo;:\u0026ldquo;ES大法必修内功\u0026rdquo;}} {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;create_time\u0026rdquo;:1554018421008}}\n组合应用\n​ POST _bulk {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;fox老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;fox老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;create_time\u0026rdquo;:1554018421008}}\n批量读取\nes的批量查询可以使用mget和msearch两种。其中mget是需要我们知道它的id，可以指定不同的index，也可以指定返回值source。msearch可以通过字段查询来进行一个批量的查找。\n_mget\n​ #可以通过ID批量获取不同index和type的数据 GET _mget { \u0026ldquo;docs\u0026rdquo;: [ { \u0026ldquo;_index\u0026rdquo;: \u0026ldquo;es_db\u0026rdquo;, \u0026ldquo;_id\u0026rdquo;: 1 }, { \u0026ldquo;_index\u0026rdquo;: \u0026ldquo;article\u0026rdquo;, \u0026ldquo;_id\u0026rdquo;: 4 } ] } #可以通过ID批量获取es_db的数据 GET /es_db/_mget { \u0026ldquo;docs\u0026rdquo;: [ { \u0026ldquo;_id\u0026rdquo;: 1 }, { \u0026ldquo;_id\u0026rdquo;: 4 } ] } #简化后 GET /es_db/_mget { \u0026ldquo;ids\u0026rdquo;:[\u0026ldquo;1\u0026rdquo;,\u0026ldquo;2\u0026rdquo;] }\n​ _msearch\n在_msearch中，请求格式和bulk类似。查询一条数据需要两个对象，第一个设置index和type，第二个设置查询语句。查询语句和search相同。如果只是查询一个index，我们可以在url中带上index，这样，如果查该index可以直接用空对象表示。\n​ GET /es_db/_msearch {} {\u0026ldquo;query\u0026rdquo; : {\u0026ldquo;match_all\u0026rdquo; : {}}, \u0026ldquo;from\u0026rdquo; : 0, \u0026ldquo;size\u0026rdquo; : 2} {\u0026ldquo;index\u0026rdquo; : \u0026ldquo;article\u0026rdquo;} {\u0026ldquo;query\u0026rdquo; : {\u0026ldquo;match_all\u0026rdquo; : {}}}\n​ Logstash与FileBeat详解以及ELK整合 ​ 文档：6. Logstash与FileBeat详解以及ELK整合\u0026hellip; 链接：http://note.youdao.com/noteshare?id=cd88d72a1c76d18efcf7fe767e8c2d20\u0026amp;sub=D7819084A43243FFA52E8A8741795414\n注意：本节课的命令和配置文件不要再pdf文件中复制，为存在格式问题，保存到有道云笔记后再操作\n​ 背景\n​ ELK架构\n​ 经典的ELK\n​ 整合消息队列+Nginx架构\n​ 什么是Logstash\n​ Logstash核心概念\n​ Logstash数据传输原理\n​ Logstash配置文件结构\n​ Logstash Queue\n​ Logstash导入数据到ES\n​ 同步数据库数据到Elasticsearch\n​ 什么是Beats\n​ FileBeat简介\n​ FileBeat的工作原理\n​ logstash vs FileBeat\n​ Filebeat安装\n​ ELK整合实战\n​ 案例：采集tomcat服务器日志\n​ 使用FileBeats将日志发送到Logstash\n​ 配置Logstash接收FileBeat收集的数据并打印\n​ Logstash输出数据到Elasticsearch\n​ 利用Logstash过滤器解析日志\n​ 输出到Elasticsearch指定索引\n背景 日志管理的挑战：\n关注点很多，任何一个点都有可能引起问题 日志分散在很多机器，出了问题时，才发现日志被删了 很多运维人员是消防员，哪里有问题去哪里 ​ 集中化日志管理思路：\n日志收集 ——》格式化分析 ——》检索和可视化 ——》 风险告警\nELK架构\nELK架构分为两种，一种是经典的ELK，另外一种是加上消息队列（Redis或Kafka或RabbitMQ）和Nginx结构。\n经典的ELK\n经典的ELK主要是由Filebeat + Logstash + Elasticsearch + Kibana组成，如下图：（早期的ELK只有Logstash + Elasticsearch + Kibana）\n​ 此架构主要适用于数据量小的开发环境，存在数据丢失的危险。\n整合消息队列+Nginx架构 这种架构，主要加上了Redis或Kafka或RabbitMQ做消息队列，保证了消息的不丢失。\n​ 此种架构，主要用在生产环境，可以处理大数据量，并且不会丢失数据。\n什么是Logstash\nLogstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的存储库中。\nhttps://www.elastic.co/cn/logstash/\n应用：ETL工具 / 数据采集处理引擎\n​ Logstash核心概念 Pipeline\n包含了input—filter-output三个阶段的处理流程 插件生命周期管理 队列管理 Logstash Event\n数据在内部流转时的具体表现形式。数据在input 阶段被转换为Event，在 output被转化成目标格式数据 Event 其实是一个Java Object，在配置文件中，对Event 的属性进行增删改查 Codec (Code / Decode)\n将原始数据decode成Event;将Event encode成目标数据\n​ Logstash数据传输原理 数据采集与输入：Logstash支持各种输入选择，能够以连续的流式传输方式，轻松地从日志、指标、Web应用以及数据存储中采集数据。 实时解析和数据转换：通过Logstash过滤器解析各个事件，识别已命名的字段来构建结构，并将它们转换成通用格式，最终将数据从源端传输到存储库中。 存储与数据导出：Logstash提供多种输出选择，可以将数据发送到指定的地方。 Logstash通过管道完成数据的采集与处理，管道配置中包含input、output和filter（可选）插件，input和output用来配置输入和输出数据源、filter用来对数据进行过滤或预处理。\n​ Logstash配置文件结构 参考：https://www.elastic.co/guide/en/logstash/7.17/configuration.html\nLogstash的管道配置文件对每种类型的插件都提供了一个单独的配置部分，用于处理管道事件。\n​ input { stdin { } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{COMBINEDAPACHELOG}\u0026rdquo; } } date { match =\u0026gt; [ \u0026ldquo;timestamp\u0026rdquo; , \u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026ldquo;localhost:9200\u0026rdquo;]} stdout { codec =\u0026gt; rubydebug } }\n每个配置部分可以包含一个或多个插件。例如，指定多个filter插件，Logstash会按照它们在配置文件中出现的顺序进行处理。\n​ #运行 bin/logstash -f logstash-demo.conf\nInput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/input-plugins.html\n一个 Pipeline可以有多个input插件\nStdin / File\nBeats / Log4J /Elasticsearch / JDBC / Kafka /Rabbitmq /Redis\nJMX/ HTTP / Websocket / UDP / TCP\nGoogle Cloud Storage / S3\nGithub / Twitter\nOutput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/output-plugins.html\n将Event发送到特定的目的地，是 Pipeline 的最后一个阶段。\n常见 Output Plugins：\nElasticsearch Email / Pageduty Influxdb / Kafka / Mongodb / Opentsdb / Zabbix Http / TCP / Websocket Filter Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/filter-plugins.html\n处理Event\n内置的Filter Plugins:\nMutate 一操作Event的字段 Metrics — Aggregate metrics Ruby 一执行Ruby 代码 Codec Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/codec-plugins.html\n将原始数据decode成Event;将Event encode成目标数据\n内置的Codec Plugins:\nLine / Multiline JSON / Avro / Cef (ArcSight Common Event Format) Dots / Rubydebug Logstash Queue\nIn Memory Queue 进程Crash，机器宕机，都会引起数据的丢失\nPersistent Queue 机器宕机，数据也不会丢失; 数据保证会被消费; 可以替代 Kafka等消息队列缓冲区的作用\n​ queue.type: persisted (默认是memory) queue.max_bytes: 4gb\n​ Logstash安装 logstash官方文档: https://www.elastic.co/guide/en/logstash/7.17/installing-logstash.html\n1）下载并解压logstash\n下载地址： https://www.elastic.co/cn/downloads/past-releases#logstash\n选择版本：7.17.3\n​ 2）测试：运行最基本的logstash管道\n​ cd logstash-7.17.3 #linux #-e选项表示，直接把配置放在命令中，这样可以有效快速进行测试 bin/logstash -e \u0026lsquo;input { stdin { } } output { stdout {} }\u0026rsquo; #windows .\\bin\\logstash.bat -e \u0026ldquo;input { stdin { } } output { stdout {} }\u0026rdquo;\n测试结果：\n​ window版本的logstash-7.17.3的bug:\nwindows出现错误提示could not find java; set JAVA_HOME or ensure java is in PATH\n​ 修改setup.bat\n​ ​ Codec Plugin测试\n​ # single line bin/logstash -e \u0026ldquo;input{stdin{codec=\u0026gt;line}}output{stdout{codec=\u0026gt; rubydebug}}\u0026rdquo; bin/logstash -e \u0026ldquo;input{stdin{codec=\u0026gt;json}}output{stdout{codec=\u0026gt; rubydebug}}\u0026rdquo;\nCodec Plugin —— Multiline\n设置参数:\npattern: 设置行匹配的正则表达式\nwhat : 如果匹配成功，那么匹配行属于上一个事件还是下一个事件\nprevious / next negate : 是否对pattern结果取反\ntrue / false ​ # 多行数据，异常 Exception in thread \u0026ldquo;main\u0026rdquo; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) # multiline-exception.conf input { stdin { codec =\u0026gt; multiline { pattern =\u0026gt; \u0026ldquo;^\\s\u0026rdquo; what =\u0026gt; \u0026ldquo;previous\u0026rdquo; } } } filter {} output { stdout { codec =\u0026gt; rubydebug } } #执行管道 bin/logstash -f multiline-exception.conf\nInput Plugin —— File\n支持从文件中读取数据，如日志文件 文件读取需要解决的问题：只被读取一次。重启后需要从上次读取的位置继续(通过sincedb 实现) 读取到文件新内容，发现新文件 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取 Filter Plugin\nFilter Plugin可以对Logstash Event进行各种处理，例如解析，删除字段，类型转换\nDate: 日期解析 Dissect: 分割符解析 Grok: 正则匹配解析 Mutate: 处理字段。重命名，删除，替换 Ruby: 利用Ruby 代码来动态修改Event Filter Plugin - Mutate\n对字段做各种操作:\nConvert : 类型转换 Gsub : 字符串替换 Split / Join /Merge: 字符串切割，数组合并字符串，数组合并数组 Rename: 字段重命名 Update / Replace: 字段内容更新替换 Remove_field: 字段删除 Logstash导入数据到ES\n1）测试数据集下载：https://grouplens.org/datasets/movielens/\n​ 2）准备logstash-movie.conf配置文件\n​ input { file { path =\u0026gt; \u0026ldquo;/home/es/logstash-7.17.3/dataset/movies.csv\u0026rdquo; start_position =\u0026gt; \u0026ldquo;beginning\u0026rdquo; sincedb_path =\u0026gt; \u0026ldquo;/dev/null\u0026rdquo; } } filter { csv { separator =\u0026gt; \u0026ldquo;,\u0026rdquo; columns =\u0026gt; [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;content\u0026rdquo;,\u0026ldquo;genre\u0026rdquo;] } mutate { split =\u0026gt; { \u0026ldquo;genre\u0026rdquo; =\u0026gt; \u0026ldquo;|\u0026rdquo; } remove_field =\u0026gt; [\u0026ldquo;path\u0026rdquo;, \u0026ldquo;host\u0026rdquo;,\u0026quot;@timestamp\u0026quot;,\u0026ldquo;message\u0026rdquo;] } mutate { split =\u0026gt; [\u0026ldquo;content\u0026rdquo;, \u0026ldquo;(\u0026rdquo;] add_field =\u0026gt; { \u0026ldquo;title\u0026rdquo; =\u0026gt; \u0026ldquo;%{[content][0]}\u0026rdquo;} add_field =\u0026gt; { \u0026ldquo;year\u0026rdquo; =\u0026gt; \u0026ldquo;%{[content][1]}\u0026rdquo;} } mutate { convert =\u0026gt; { \u0026ldquo;year\u0026rdquo; =\u0026gt; \u0026ldquo;integer\u0026rdquo; } strip =\u0026gt; [\u0026ldquo;title\u0026rdquo;] remove_field =\u0026gt; [\u0026ldquo;path\u0026rdquo;, \u0026ldquo;host\u0026rdquo;,\u0026quot;@timestamp\u0026quot;,\u0026ldquo;message\u0026rdquo;,\u0026ldquo;content\u0026rdquo;] } } output { elasticsearch { hosts =\u0026gt; \u0026ldquo;http://localhost:9200\u0026rdquo; index =\u0026gt; \u0026ldquo;movies\u0026rdquo; document_id =\u0026gt; \u0026ldquo;%{id}\u0026rdquo; user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout {} }\n3）运行logstash\n​ # linux bin/logstash -f logstash-movie.conf\n同步数据库数据到Elasticsearch 需求: 将数据库中的数据同步到ES，借助ES的全文搜索,提高搜索速度\n需要把新增用户信息同步到Elasticsearch中 用户信息Update 后，需要能被更新到Elasticsearch 支持增量更新 用户注销后，不能被ES所搜索到 实现思路\n基于canal同步数据（项目实战中讲解）\n借助JDBC Input Plugin将数据从数据库读到Logstash\n需要自己提供所需的 JDBC Driver； JDBC Input Plugin 支持定时任务 Scheduling，其语法来自 Rufus-scheduler，其扩展了 Cron，使用 Cron 的语法可以完成任务的触发； JDBC Input Plugin 支持通过 Tracking_column / sql_last_value 的方式记录 State，最终实现增量的更新； https://www.elastic.co/cn/blog/logstash-jdbc-input-plugin JDBC Input Plugin实现步骤\n1）拷贝jdbc依赖到logstash-7.17.3/drivers目录下\n2）准备mysql-demo.conf配置文件\n​ input { jdbc { jdbc_driver_library =\u0026gt; \u0026ldquo;/home/es/logstash-7.17.3/drivers/mysql-connector-java-5.1.49.jar\u0026rdquo; jdbc_driver_class =\u0026gt; \u0026ldquo;com.mysql.jdbc.Driver\u0026rdquo; jdbc_connection_string =\u0026gt; \u0026ldquo;jdbc:mysql://localhost:3306/test?useSSL=false\u0026rdquo; jdbc_user =\u0026gt; \u0026ldquo;root\u0026rdquo; jdbc_password =\u0026gt; \u0026ldquo;123456\u0026rdquo; #启用追踪，如果为true，则需要指定tracking_column use_column_value =\u0026gt; true #指定追踪的字段， tracking_column =\u0026gt; \u0026ldquo;last_updated\u0026rdquo; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =\u0026gt; \u0026ldquo;numeric\u0026rdquo; #记录最后一次运行的结果 record_last_run =\u0026gt; true #上面运行结果的保存位置 last_run_metadata_path =\u0026gt; \u0026ldquo;jdbc-position.txt\u0026rdquo; statement =\u0026gt; \u0026ldquo;SELECT * FROM user where last_updated \u0026gt;:sql_last_value;\u0026rdquo; schedule =\u0026gt; \u0026quot; * * * * * *\u0026quot; } } output { elasticsearch { document_id =\u0026gt; \u0026ldquo;%{id}\u0026rdquo; document_type =\u0026gt; \u0026ldquo;_doc\u0026rdquo; index =\u0026gt; \u0026ldquo;users\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n3）运行logstash\n​ bin/logstash -f mysql-demo.conf\n​ 测试\n​ #user表 CREATE TABLE user ( id int NOT NULL AUTO_INCREMENT, name varchar(50) DEFAULT NULL, address varchar(50) CHARACTER DEFAULT NULL, last_updated bigint DEFAULT NULL, is_deleted int DEFAULT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; #插入数据 INSERT INTO user(name,address,last_updated,is_deleted) VALUES(\u0026ldquo;张三\u0026rdquo;,\u0026ldquo;广州天河\u0026rdquo;,unix_timestamp(NOW()),0)\n​ ​ # 更新 update user set address=\u0026ldquo;广州白云山\u0026rdquo;,last_updated=unix_timestamp(NOW()) where name=\u0026ldquo;张三\u0026rdquo;\n​ ​ #删除 update user set is_deleted=1,last_updated=unix_timestamp(NOW()) where name=\u0026ldquo;张三\u0026rdquo;\n​ ​ #ES中查询 # 创建 alias，只显示没有被标记 deleted的用户 POST /_aliases { \u0026ldquo;actions\u0026rdquo;: [ { \u0026ldquo;add\u0026rdquo;: { \u0026ldquo;index\u0026rdquo;: \u0026ldquo;users\u0026rdquo;, \u0026ldquo;alias\u0026rdquo;: \u0026ldquo;view_users\u0026rdquo;, \u0026ldquo;filter\u0026rdquo; : { \u0026ldquo;term\u0026rdquo; : { \u0026ldquo;is_deleted\u0026rdquo; : 0} } } } ] } # 通过 Alias查询，查不到被标记成 deleted的用户 POST view_users/_search POST view_users/_search { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;term\u0026rdquo;: { \u0026ldquo;name.keyword\u0026rdquo;: { \u0026ldquo;value\u0026rdquo;: \u0026ldquo;张三\u0026rdquo; } } } }\n什么是Beats 轻量型数据采集器，文档地址： https://www.elastic.co/guide/en/beats/libbeat/7.17/index.html\nBeats 是一个免费且开放的平台，集合了多种单一用途的数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。\n​ FileBeat简介 FileBeat专门用于转发和收集日志数据的轻量级采集工具。它可以作为代理安装在服务器上，FileBeat监视指定路径的日志文件，收集日志数据，并将收集到的日志转发到Elasticsearch或者Logstash。\nFileBeat的工作原理 启动FileBeat时，会启动一个或者多个输入（Input），这些Input监控指定的日志数据位置。FileBeat会针对每一个文件启动一个Harvester（收割机）。Harvester读取每一个文件的日志，将新的日志发送到libbeat，libbeat将数据收集到一起，并将数据发送给输出（Output）。\n​ logstash vs FileBeat\nLogstash是在jvm上运行的，资源消耗比较大。而FileBeat是基于golang编写的，功能较少但资源消耗也比较小，更轻量级。 Logstash 和Filebeat都具有日志收集功能，Filebeat更轻量，占用资源更少 Logstash 具有Filter功能，能过滤分析日志 一般结构都是Filebeat采集日志，然后发送到消息队列、Redis、MQ中，然后Logstash去获取，利用Filter功能过滤分析，然后存储到Elasticsearch中 FileBeat和Logstash配合，实现背压机制。当将数据发送到Logstash或 Elasticsearch时，Filebeat使用背压敏感协议，以应对更多的数据量。如果Logstash正在忙于处理数据，则会告诉Filebeat 减慢读取速度。一旦拥堵得到解决，Filebeat就会恢复到原来的步伐并继续传输数据。 Filebeat安装 https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html\n1）下载并解压Filebeat\n下载地址：https://www.elastic.co/cn/downloads/past-releases#filebeat\n选择版本：7.17.3\n​ 2）编辑配置\n修改 filebeat.yml 以设置连接信息：\n​ output.elasticsearch: hosts: [\u0026ldquo;192.168.65.174:9200\u0026rdquo;,\u0026ldquo;192.168.65.192:9200\u0026rdquo;,\u0026ldquo;192.168.65.204:9200\u0026rdquo;] username: \u0026ldquo;elastic\u0026rdquo; password: \u0026ldquo;123456\u0026rdquo; setup.kibana: host: \u0026ldquo;192.168.65.174:5601\u0026rdquo;\n3) 启用和配置数据收集模块\n从安装目录中，运行：\n​ # 查看可以模块列表 ./filebeat modules list #启用nginx模块 ./filebeat modules enable nginx #如果需要更改nginx日志路径,修改modules.d/nginx.yml - module: nginx access: var.paths: [\u0026quot;/var/log/nginx/access.log*\u0026quot;] #启用 Logstash 模块 ./filebeat modules enable logstash #在 modules.d/logstash.yml 文件中修改设置 - module: logstash log: enabled: true var.paths: [\u0026quot;/home/es/logstash-7.17.3/logs/*.log\u0026quot;]\n4）启动 Filebeat\n​ # setup命令加载Kibana仪表板。 如果仪表板已经设置，则忽略此命令。 ./filebeat setup # 启动Filebeat ./filebeat -e\nELK整合实战 案例：采集tomcat服务器日志\nTomcat服务器运行过程中产生很多日志信息，通过Logstash采集并存储日志信息至ElasticSearch中\n使用FileBeats将日志发送到Logstash\n1）创建配置文件filebeat-logstash.yml，配置FileBeats将数据发送到Logstash\n​ vim filebeat-logstash.yml chmod 644 filebeat-logstash.yml #因为Tomcat的web log日志都是以IP地址开头的，所以我们需要修改下匹配字段。 # 不以ip地址开头的行追加到上一行 filebeat.inputs: - type: log enabled: true paths: - /home/es/apache-tomcat-8.5.33/logs/access.* multiline.pattern: \u0026lsquo;^\\d+\\.\\d+\\.\\d+\\.\\d+ \u0026rsquo; multiline.negate: true multiline.match: after output.logstash: enabled: true hosts: [\u0026ldquo;192.168.65.204:5044\u0026rdquo;]\npattern：正则表达式 negate：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行 match：after 或 before，合并到上一行的末尾或开头 2）启动FileBeat，并指定使用指定的配置文件\n​ ./filebeat -e -c filebeat-logstash.yml\n可能出现的异常：\n异常1：Exiting: error loading config file: config file (\u0026ldquo;filebeat-logstash.yml\u0026rdquo;) can only be writable by the owner but the permissions are \u0026ldquo;-rw-rw-r\u0026ndash;\u0026rdquo; (to fix the permissions use: \u0026lsquo;chmod go-w /home/es/filebeat-7.17.3-linux-x86_64/filebeat-logstash.yml\u0026rsquo;)\n因为安全原因不要其他用户写的权限，去掉写的权限就可以了\n​ chmod 644 filebeat-logstash.yml\n异常2：Failed to connect to backoff(async(tcp://192.168.65.204:5044)): dial tcp 192.168.65.204:5044: connect: connection refused\nFileBeat将尝试建立与Logstash监听的IP和端口号进行连接。但此时，我们并没有开启并配置Logstash，所以FileBeat是无法连接到Logstash的。\n配置Logstash接收FileBeat收集的数据并打印\n​ vim config/filebeat-console.conf # 配置从FileBeat接收数据 input { beats { port =\u0026gt; 5044 } } output { stdout { codec =\u0026gt; rubydebug } }\n测试logstash配置是否正确\n​ bin/logstash -f config/filebeat-console.conf \u0026ndash;config.test_and_exit\n启动logstash\n​ # reload.automatic：修改配置文件时自动重新加载 bin/logstash -f config/filebeat-console.conf \u0026ndash;config.reload.automatic\n测试访问tomcat，logstash是否接收到了Filebeat传过来的tomcat日志\nLogstash输出数据到Elasticsearch\n如果我们需要将数据输出值ES而不是控制台的话，我们修改Logstash的output配置。\n​ vim config/filebeat-elasticSearch.conf input { beats { port =\u0026gt; 5044 } } output { elasticsearch { hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n启动logstash\n​ bin/logstash -f config/filebeat-elasticSearch.conf \u0026ndash;config.reload.automatic\nES中会生成一个以logstash开头的索引，测试日志是否保存到了ES。\n思考：日志信息都保证在message字段中，是否可以把日志进行解析一个个的字段？例如：IP字段、时间、请求方式、请求URL、响应结果。\n利用Logstash过滤器解析日志\n从日志文件中收集到的数据包含了很多有效信息，比如IP、时间等，在Logstash中可以配置过滤器Filter对采集到的数据进行过滤处理，Logstash中有大量的插件可以供我们使用。\n​ 查看Logstash已经安装的插件 bin/logstash-plugin list\nGrok插件\nGrok是一种将非结构化日志解析为结构化的插件。这个工具非常适合用来解析系统日志、Web服务器日志、MySQL或者是任意其他的日志格式。\nhttps://www.elastic.co/guide/en/logstash/7.17/plugins-filters-grok.html\nGrok语法\nGrok是通过模式匹配的方式来识别日志中的数据,可以把Grok插件简单理解为升级版本的正则表达式。它拥有更多的模式，默认Logstash拥有120个模式。如果这些模式不满足我们解析日志的需求，我们可以直接使用正则表达式来进行匹配。\ngrok模式的语法是：\n​ %{SYNTAX:SEMANTIC}\nSYNTAX（语法）指的是Grok模式名称，SEMANTIC（语义）是给模式匹配到的文本字段名。例如：\n​ %{NUMBER:duration} %{IP:client} duration表示：匹配一个数字，client表示匹配一个IP地址。\n默认在Grok中，所有匹配到的的数据类型都是字符串，如果要转换成int类型（目前只支持int和float），可以这样：%{NUMBER:duration:int} %{IP:client}\n常用的Grok模式\nhttps://help.aliyun.com/document_detail/129387.html?scm=20140722.184.2.173\n用法\n​ filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026rdquo; } } }\n比如，tomacat日志\n​ 192.168.65.103 - - [23/Jun/2022:22:37:23 +0800] \u0026ldquo;GET /docs/images/docs-stylesheet.css HTTP/1.1\u0026rdquo; 200 5780\n解析后的字段\n字段名 说明 client IP 浏览器端IP timestamp 请求的时间戳 method 请求方式（GET/POST） uri 请求的链接地址 status 服务器端响应状态 length 响应的数据长度 grok模式\n​ %{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status} %{INT:length}\n为了方便测试，我们可以使用Kibana来进行Grok开发：\n​ 修改Logstash配置文件\n​ vim config/filebeat-console.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status:int} %{INT:length:int}\u0026rdquo; } } } output { stdout { codec =\u0026gt; rubydebug } }\n启动logstash测试\n​ bin/logstash -f config/filebeat-console.conf \u0026ndash;config.reload.automatic\n使用mutate插件过滤掉不需要的字段\n​ mutate { enable_metric =\u0026gt; \u0026ldquo;false\u0026rdquo; remove_field =\u0026gt; [\u0026ldquo;message\u0026rdquo;, \u0026ldquo;log\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;input\u0026rdquo;, \u0026ldquo;agent\u0026rdquo;, \u0026ldquo;host\u0026rdquo;, \u0026ldquo;ecs\u0026rdquo;, \u0026ldquo;@version\u0026rdquo;] }\n要将日期格式进行转换，我们可以使用Date插件来实现。该插件专门用来解析字段中的日期，官方说明文档：https://www.elastic.co/guide/en/logstash/7.17/plugins-filters-date.html\n用法如下：\n​ 将date字段转换为「年月日 时分秒」格式。默认字段经过date插件处理后，会输出到@timestamp字段，所以，我们可以通过修改target属性来重新定义输出字段。\n​ date { match =\u0026gt; [\u0026ldquo;date\u0026rdquo;,\u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo;,\u0026ldquo;yyyy-MM-dd HH:mm:ss\u0026rdquo;] target =\u0026gt; \u0026ldquo;date\u0026rdquo; }\n输出到Elasticsearch指定索引\nindex来指定索引名称，默认输出的index名称为：logstash-%{+yyyy.MM.dd}。但注意，要在index中使用时间格式化，filter的输出必须包含 @timestamp字段，否则将无法解析日期。\n​ output { elasticsearch { index =\u0026gt; \u0026ldquo;tomcat_web_log_%{+YYYY-MM}\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n注意：index名称中，不能出现大写字符\n完整的Logstash配置文件\n​ vim config/filebeat-filter-es.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status:int} %{INT:length:int}\u0026rdquo; } } mutate { enable_metric =\u0026gt; \u0026ldquo;false\u0026rdquo; remove_field =\u0026gt; [\u0026ldquo;message\u0026rdquo;, \u0026ldquo;log\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;input\u0026rdquo;, \u0026ldquo;agent\u0026rdquo;, \u0026ldquo;host\u0026rdquo;, \u0026ldquo;ecs\u0026rdquo;, \u0026ldquo;@version\u0026rdquo;] } date { match =\u0026gt; [\u0026ldquo;date\u0026rdquo;,\u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo;,\u0026ldquo;yyyy-MM-dd HH:mm:ss\u0026rdquo;] target =\u0026gt; \u0026ldquo;date\u0026rdquo; } } output { stdout { codec =\u0026gt; rubydebug } elasticsearch { index =\u0026gt; \u0026ldquo;tomcat_web_log_%{+YYYY-MM}\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } }\n启动logstash\n​ bin/logstash -f config/filebeat-filter-es.conf \u0026ndash;config.reload.automatic\n","permalink":"https://wandong1.github.io/post/elasticsearch/","summary":"ElasticSearch快速入门实战 主讲老师：Fox\nES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\n​ 文档：1.ElasticSearch快速入门实战.note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:","title":"ElasticSearch快速入门实战"},{"content":"Git Git 是一个开源的分布式版本控制软件,用以有效、高速的处理从很小到非常大的项目版本管理。 Git 最初是由Linus Torvalds设计开发的，用于管理Linux内核开发。Git 是根据GNU通用公共许可证版本2的条款分发的自由/免费软件，安装参见：http://git-scm.com/\n打开git bash，初始化配置 git config --global user.name \u0026#34;wandong\u0026#34; git config --global user.email \u0026#34;993696910@qq.com\u0026#34; # 对已存在的目录进行git的初始化 git init # 添加远程仓库地址 git remote add origin http://git.cqzwymgmt.com/root/gin-project-orm.git # git add . git commit -m \u0026#34;Initial commit\u0026#34; # 推送到远程仓库 master分支 git push -u origin master 在新的环境拉取代码，进行开发 git clone http://git.cqzwymgmt.com/root/gin-project-orm.git # 创建新的分支继续开发 git branch dev # 列出所有分支 git branch # 切换分支 git checkout dev # 可以开始开发新功能了，尽量开发新的文件，避免合并的时候出现冲突进而解决冲突。 git add . git commit -m \u0026#34;change log function\u0026#34; # 推送到远程仓库 dev分支 git push -u origin dev 更新本地代码 # 拉取最新的dev分支代码，如果本地没有该分支，先创建 git branch dev # 使用pull命令更新分支代码的时候，要先处于该分支，不然会被合并 git branch dev git checkout dev git pull origin dev # 查看dev分支代码和master代码区别 将dev分支合并到master分支 git merge dev # 或者 git rebase dev 删除本地和远程仓库的分支 # 删除分支前先切换其他分支 git branch -d dev git push origin --delete dev 将你的仓库和你的gitee合并了，用填充的方法，即： git pull --rebase origin master ","permalink":"https://wandong1.github.io/post/git/","summary":"Git Git 是一个开源的分布式版本控制软件,用以有效、高速的处理从很小到非常大的项目版本管理。 Git 最初是由Linus Torvalds设计开发的，用于管理Linux内核开发。Git 是根据GNU通用公共许可证版本2的条款分发的自由/免费软件，安装参见：http://git-scm.com/\n打开git bash，初始化配置 git config --global user.name \u0026#34;wandong\u0026#34; git config --global user.email \u0026#34;993696910@qq.com\u0026#34; # 对已存在的目录进行git的初始化 git init # 添加远程仓库地址 git remote add origin http://git.cqzwymgmt.com/root/gin-project-orm.git # git add . git commit -m \u0026#34;Initial commit\u0026#34; # 推送到远程仓库 master分支 git push -u origin master 在新的环境拉取代码，进行开发 git clone http://git.cqzwymgmt.com/root/gin-project-orm.git # 创建新的分支继续开发 git branch dev # 列出所有分支 git branch # 切换分支 git checkout dev # 可以开始开发新功能了，尽量开发新的文件，避免合并的时候出现冲突进而解决冲突。 git add . git commit -m \u0026#34;change log function\u0026#34; # 推送到远程仓库 dev分支 git push -u origin dev 更新本地代码 # 拉取最新的dev分支代码，如果本地没有该分支，先创建 git branch dev # 使用pull命令更新分支代码的时候，要先处于该分支，不然会被合并 git branch dev git checkout dev git pull origin dev # 查看dev分支代码和master代码区别 将dev分支合并到master分支 git merge dev # 或者 git rebase dev 删除本地和远程仓库的分支 # 删除分支前先切换其他分支 git branch -d dev git push origin --delete dev 将你的仓库和你的gitee合并了，用填充的方法，即： git pull --rebase origin master ","title":"git的使用方法"},{"content":"ES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\nElasticSearch快速入门实战 note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引）\n文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）\n假如，我们有一个站内搜索的功能，通过某个关键词来搜索相关的文章，那么这个关键词可能出现在标题中，也可能出现在文章内容中，那我们将会在创建或修改文章的时候，建立一个关键词与文章的对应关系表，这种，我们可以称之为倒排索引。\nlike %java设计模式% java 设计模式\n关键词 文章ID JAVA 1,2 设计模式 1,2 多线程 2 简单理解，正向索引是通过key找value，反向索引则是通过value找key。ES底层在检索时底层使用的就是倒排索引。\nElasticSearch简介 ElasticSearch是什么 ElasticSearch（简称ES）是一个分布式、RESTful 风格的搜索和数据分析引擎，是用Java开发并且是当前最流行的开源的企业级搜索引擎，能够达到近实时搜索，稳定，可靠，快速，安装使用方便。\n客户端支持Java、.NET（C#）、PHP、Python、Ruby等多种语言。\n官方网站: https://www.elastic.co/\n**下载地址：**https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n搜索引擎排名：\n​ 参考网站：https://db-engines.com/en/ranking/search+engine\n起源——Lucene 基于Java语言开发的搜索引擎库类\n创建于1999年，2005年成为Apache 顶级开源项目\nLucene具有高性能、易扩展的优点\nLucene的局限性︰\n只能基于Java语言开发 类库的接口学习曲线陡峭 原生并不支持水平扩展 Elasticsearch的诞生 Elasticsearch是构建在Apache Lucene之上的开源分布式搜索引擎。\n2004年 Shay Banon 基于Lucene开发了Compass\n2010年 Shay Banon重写了Compass，取名Elasticsearch\n支持分布式，可水平扩展 降低全文检索的学习曲线，可以被任何编程语言调用 ​ Elasticsearch 与 Lucene 核心库竞争的优势在于：\n完美封装了 Lucene 核心库，设计了友好的 Restful-API，开发者无需过多关注底层机制，直接开箱即用。 分片与副本机制，直接解决了集群下性能与高可用问题。 ES Server进程 3节点 raft (奇数节点)\n数据分片 -》lucene实例 分片和副本数 1个ES节点可以有多个lucene实例。也可以指定一个索引的多个分片\n​ ElasticSearch版本特性 5.x新特性\nLucene 6.x， 性能提升，默认打分机制从TF-IDF改为BM 25\n支持Ingest节点/ Painless Scripting / Completion suggested支持/原生的Java REST客户端\nType标记成deprecated， 支持了Keyword的类型\n性能优化\n内部引擎移除了避免同一文档并发更新的竞争锁，带来15% - 20%的性能提升 Instant aggregation,支持分片，上聚合的缓存 新增了Profile API 6.x新特性\nLucene 7.x\n新功能\n跨集群复制(CCR) 索引生命周期管理 SQL的支持 更友好的的升级及数据迁移\n在主要版本之间的迁移更为简化，体验升级 全新的基于操作的数据复制框架，可加快恢复数据 性能优化\n有效存储稀疏字段的新方法，降低了存储成本 在索引时进行排序，可加快排序的查询性能 7.x新特性\nLucene 8.0\n重大改进-正式废除单个索引下多Type的支持\n7.1开始，Security 功能免费使用\nECK - Elasticseach Operator on Kubernetes\n新功能\nNew Cluster coordination Feature——Complete High Level REST Client Script Score Query 性能优化\n默认的Primary Shard数从5改为1,避免Over Sharding 性能优化， 更快的Top K 8.x新特性\nRest API相比较7.x而言做了比较大的改动（比如彻底删除_type） 默认开启安全配置 存储空间优化：对倒排文件使用新的编码集，对于keyword、match_only_text、text类型字段有效，有3.5%的空间优化提升，对于新建索引和segment自动生效。 优化geo_point，geo_shape类型的索引（写入）效率：15%的提升。 技术预览版KNN API发布，（K邻近算法），跟推荐系统、自然语言排名相关。 https://www.elastic.co/guide/en/elastic-stack/current/elasticsearch-breaking-changes.html ElasticSearch vs Solr Solr 是第一个基于 Lucene 核心库功能完备的搜索引擎产品，诞生远早于 Elasticsearch。\n当单纯的对已有数据进行搜索时，Solr更快。当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。\n​ ​ 大型互联网公司，实际生产环境测试，将搜索引擎从Solr转到 Elasticsearch以后的平均查询速度有了50倍的提升。\n​ 总结：\nSolr 利用 Zookeeper 进行分布式管理，而Elasticsearch 自身带有分布式协调管理功能。 Solr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。 Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。 Solr 是传统搜索应用的有力解决方案，但 Elasticsearch更适用于新兴的实时搜索应用。 Elastic Stack介绍 在Elastic Stack之前我们听说过ELK，ELK分别是Elasticsearch，Logstash，Kibana这三款软件在一起的简称，在发展的过程中又有新的成员Beats的加入，就形成了Elastic Stack。\n​ Elastic Stack生态圈\n在Elastic Stack生态圈中Elasticsearch作为数据存储和搜索，是生态圈的基石，Kibana在上层提供用户一个可视化及操作的界面，Logstash和Beat可以对数据进行收集。在上图的右侧X-Pack部分则是Elastic公司提供的商业项目。\n指标分析/日志分析：\n​ ElasticSearch应用场景 站内搜索 日志管理与分析 大数据分析 应用性能监控 机器学习 国内现在有大量的公司都在使用 Elasticsearch，包括携程、滴滴、今日头条、饿了么、360安全、小米、vivo等诸多知名公司。除了搜索之外，结合Kibana、Logstash、Beats，Elastic Stack还被广泛运用在大数据近实时分析领域，包括日志分析、指标监控、信息安全等多个领域。它可以帮助你探索海量结构化、非结构化数据，按需创建可视化报表，对监控数据设置报警阈值，甚至通过使用机器学习技术，自动识别异常状况。\n通用数据处理流程：\n​ ElasticSearch快速开始 安装JDK\n1、yum install -y java-1.8.0-openjdk* # 或者 mkdir /opt/jdk;tar -xvzf jdk-8u333-linux-x64.tar.gz -C /opt/jdk/; mv /opt/jdk/jdk1.8.0_333/ /opt/jdk/jdk1.8 ; cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile JAVA_HOME=/opt/jdk/jdk1.8 CLASSPATH=$JAVA_HOME/lib/ PATH=$PATH:$JAVA_HOME/bin export PATH JAVA_HOME CLASSPATH EOF source /etc/profile ElasticSearch安装运行 环境准备 运行Elasticsearch，需安装并配置JDK\n设置$JAVA_HOME 各个版本对Java的依赖 https://www.elastic.co/support/matrix#matrix_jvm\nElasticsearch 5需要Java 8以上的版本 Elasticsearch 从6.5开始支持Java 11 7.0开始，内置了Java环境 ES比较耗内存，建议虚拟机4G或以上内存，jvm1g以上的内存分配\n可以参考es的环境文件elasticsearch-env.bat\n​ ES的jdk环境生效的优先级配置ES_JAVA_HOME\u0026gt;JAVA_HOME\u0026gt;ES_HOME\n下载并解压ElasticSearch 下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n选择版本：7.17.3\n​ ElasticSearch文件目录结构\n目录 描述 bin 脚本文件，包括启动elasticsearch，安装插件，运行统计数据等 config 配置文件目录，如elasticsearch配置、角色配置、jvm配置等。 jdk java运行环境 data 默认的数据存放目录，包含节点、分片、索引、文档的所有数据，生产环境需要修改。 lib elasticsearch依赖的Java类库 logs 默认的日志文件存储路径，生产环境需要修改。 modules 包含所有的Elasticsearch模块，如Cluster、Discovery、Indices等。 plugins 已安装插件目录 主配置文件elasticsearch.yml\ncluster.name 当前节点所属集群名称，多个节点如果要组成同一个集群，那么集群名称一定要配置成相同。默认值elasticsearch，生产环境建议根据ES集群的使用目的修改成合适的名字。\nnode.name 当前节点名称，默认值当前节点部署所在机器的主机名，所以如果一台机器上要起多个ES节点的话，需要通过配置该属性明确指定不同的节点名称。\npath.data 配置数据存储目录，比如索引数据等，默认值 $ES_HOME/data，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\npath.logs 配置日志存储目录，比如运行日志和集群健康信息等，默认值 $ES_HOME/logs，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\nbootstrap.memory_lock 配置ES启动时是否进行内存锁定检查，默认值true。\nES对于内存的需求比较大，一般生产环境建议配置大内存，如果内存不足，容易导致内存交换到磁盘，严重影响ES的性能。所以默认启动时进行相应大小内存的锁定，如果无法锁定则会启动失败。\n非生产环境可能机器内存本身就很小，能够供给ES使用的就更小，如果该参数配置为true的话很可能导致无法锁定内存以致ES无法成功启动，此时可以修改为false。\nnetwork.host 配置能够访问当前节点的主机，默认值为当前节点所在机器的本机回环地址127.0.0.1 和[::1]，这就导致默认情况下只能通过当前节点所在主机访问当前节点。可以配置为 0.0.0.0 ，表示所有主机均可访问。\nhttp.port 配置当前ES节点对外提供服务的http端口，默认值 9200\ndiscovery.seed_hosts 配置参与集群节点发现过程的主机列表，说白一点就是集群中所有节点所在的主机列表，可以是具体的IP地址，也可以是可解析的域名。\ncluster.initial_master_nodes 配置ES集群初始化时参与master选举的节点名称列表，必须与node.name配置的一致。ES集群首次构建完成后，应该将集群中所有节点的配置文件中的cluster.initial_master_nodes配置项移除，重启集群或者将新节点加入某个已存在的集群时切记不要设置该配置项。\n​ #ES开启远程访问 network.host: 0.0.0.0\n修改JVM配置 修改config/jvm.options配置文件，调整jvm堆内存大小\n​ vim jvm.options -Xms4g -Xmx4g\n配置的建议\nXms和Xms设置成—样 Xmx不要超过机器内存的50% 不要超过30GB - https://www.elastic.co/cn/blog/a-heap-of-trouble 启动ElasticSearch服务 Windows\n直接运行elasticsearch.bat\nLinux（centos7）\nES不允许使用root账号启动服务，如果你当前账号是root，则需要创建一个专有账户\n​ #非root用户 bin/elasticsearch # -d 后台启动 bin/elasticsearch -d\n​ 注意：es默认不能用root用户启动，生产环境建议为elasticsearch创建用户。\n​ #为elaticsearch创建用户并赋予相应权限 adduser es passwd es chown -R es:es elasticsearch-17.3\n运行http://localhost:9200/\n​ 如果ES服务启动异常，会有提示：\n​ 启动ES服务常见错误解决方案\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\nES因为需要大量的创建索引文件，需要大量的打开系统的文件，所以我们需要解除linux系统当中打开文件最大数目的限制，不然ES启动就会抛错\n#切换到root用户 vim /etc/security/limits.conf 末尾添加如下配置： *\tsoft nofile 65536 * hard nofile 65536 * soft nproc 4096 *\thard nproc 4096 ​\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n无法创建本地线程问题,用户最大可创建线程数太小\nvim /etc/security/limits.d/20-nproc.conf 改为如下配置： * soft nproc 4096 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n最大虚拟内存太小,调大系统的虚拟内存\n​\nvim /etc/sysctl.conf 追加以下内容： vm.max_map_count=262144 保存退出之后执行如下命令： sysctl -p [4]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n缺少默认配置，至少需要配置discovery.seed_hosts/discovery.seed_providers/cluster.initial_master_nodes中的一个参数.\ndiscovery.seed_hosts: 集群主机列表 discovery.seed_providers: 基于配置文件配置集群主机列表 cluster.initial_master_nodes: 启动时初始化的参与选主的node，生产环境必填 ​\nvim config/elasticsearch.yml #添加配置 discovery.seed_hosts: [\u0026#34;127.0.0.1\u0026#34;] cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;] #或者 单节点（集群单节点） discovery.type: single-node 客户端Kibana安装 Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。\n1）下载并解压缩Kibana\n下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana\n选择版本：7.17.3\n​ 2）修改Kibana.yml\nvim config/kibana.yml server.port: 5601 server.host: \u0026#34;0.0.0.0\u0026#34; #服务器ip elasticsearch.hosts: [\u0026#34;http://localhost:9200\u0026#34;] #elasticsearch的访问地址 i18n.locale: \u0026#34;zh-CN\u0026#34; #Kibana汉化 3）运行Kibana\n# 注意：kibana也需要非root用户启动 bin/kibana # 后台启动 nohup bin/kibana \u0026amp; # 或者 sudo -H -u es /bin/bash -c \u0026#34;nohup bin/kibana \u0026amp;\u0026#34; ​\n访问Kibana: http://localhost:5601/\n​ cat API\n/_cat/allocation #查看单节点的shard分配整体情况 /_cat/shards #查看各shard的详细情况 /_cat/shards/{index} #查看指定分片的详细情况 /_cat/master #查看master节点信息 /_cat/nodes #查看所有节点信息 /_cat/indices #查看集群中所有index的详细信息 /_cat/indices/{index} #查看集群中指定index的详细信息 /_cat/segments #查看各index的segment详细信息,包括segment名, 所属shard, 内存(磁盘)占用大小, 是否刷盘 /_cat/segments/{index}#查看指定index的segment详细信息 /_cat/count #查看当前集群的doc数量 /_cat/count/{index} #查看指定索引的doc数量 /_cat/recovery #查看集群内每个shard的recovery过程.调整replica。 /_cat/recovery/{index}#查看指定索引shard的recovery过程 /_cat/health #查看集群当前状态：红、黄、绿 /_cat/pending_tasks #查看当前集群的pending task /_cat/aliases #查看集群中所有alias信息,路由配置等 /_cat/aliases/{alias} #查看指定索引的alias信息 /_cat/thread_pool #查看集群各节点内部不同类型的threadpool的统计信息, /_cat/plugins #查看集群各个节点上的plugin信息 /_cat/fielddata #查看当前集群各个节点的fielddata内存使用情况 /_cat/fielddata/{fields} #查看指定field的内存使用情况,里面传field属性对应的值 /_cat/nodeattrs #查看单节点的自定义属性 /_cat/repositories #输出集群中注册快照存储库 /_cat/templates #输出当前正在存在的模板信息 Elasticsearch安装分词插件 Elasticsearch提供插件机制对系统进行扩展\n以安装analysis-icu这个分词插件为例\n在线安装\n​\n#查看已安装插件 bin/elasticsearch-plugin list #安装插件 bin/elasticsearch-plugin install analysis-icu #删除插件 bin/elasticsearch-plugin remove analysis-icu 注意：安装和删除完插件后，需要重启ES服务才能生效。\n测试分词效果\n​ POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;icu_analyzer\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n​ 离线安装\n本地下载相应的插件，解压，然后手动上传到elasticsearch的plugins目录，然后重启ES实例就可以了。\n比如ik中文分词插件：https://github.com/medcl/elasticsearch-analysis-ik\nelasticsearch-analysis-ik-7.15.2.zip\n必须对应es版本\n测试分词效果\n#ES的默认分词设置是standard，会单字拆分 POST _analyze { \u0026#34;analyzer\u0026#34;:\u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;中华人民共和国\u0026#34; } #ik_smart:会做最粗粒度的拆 POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国\u0026#34; } #ik_max_word:会将文本做最细粒度的拆分 POST _analyze { \u0026#34;analyzer\u0026#34;:\u0026#34;ik_max_word\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;中华人民共和国\u0026#34; } ​\n创建索引时可以指定IK分词器作为默认分词器\nPUT /es_db { \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;analysis.analyzer.default.type\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } ​ ElasticSearch基本概念 关系型数据库 VS ElasticSearch 在7.0之前，一个 Index可以设置多个Types\n目前Type已经被Deprecated，7.0开始，一个索引只能创建一个Type - “_doc”\n传统关系型数据库和Elasticsearch的区别:\nElasticsearch- Schemaless /相关性/高性能全文检索 RDMS —事务性/ Join ​ 索引（Index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。\n一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。\n​ 文档（Document） Elasticsearch是面向文档的，文档是所有可搜索数据的最小单位。\n日志文件中的日志项 一本电影的具体信息/一张唱片的详细信息 MP3播放器里的一首歌/一篇PDF文档中的具体内容 文档会被序列化成JSON格式，保存在Elasticsearch中\nJSON对象由字段组成 每个字段都有对应的字段类型(字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID\n可以自己指定ID或者通过Elasticsearch自动生成 一篇文档包含了一系列字段，类似数据库表中的一条记录\nJSON文档，格式灵活，不需要预先定义格式\n字段的类型可以指定或者通过Elasticsearch自动推算 支持数组/支持嵌套 文档元数据\n​ 元数据，用于标注文档的相关信息：\n_index：文档所属的索引名 _type：文档所属的类型名 _id：文档唯—ld _source: 文档的原始Json数据 _version: 文档的版本号，修改删除操作_version都会自增1 _seq_no: 和_version一样，一旦数据发生更改，数据也一直是累计的。Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。 _primary_term: _primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。 ElasticSearch索引操作 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index.html\n创建索引 索引命名必须小写，不能以下划线开头\n格式: PUT /索引名称\n​ #创建索引 PUT /es_db #创建索引时可以设置分片数和副本数 PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;number_of_shards\u0026rdquo; : 3, \u0026ldquo;number_of_replicas\u0026rdquo; : 2 } } #修改索引配置 PUT /es_db/_settings { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;number_of_replicas\u0026rdquo; : 1 } }\n​ 查询索引 格式: GET /索引名称\n​ #查询索引 GET /es_db #es_db是否存在 HEAD /es_db\n​ ​\n删除索引 格式: DELETE /索引名称\n​ DELETE /es_db\nElasticSearch文档操作 示例数据\nPUT /es_db { \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;analysis.analyzer.default.type\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } PUT /es_db/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; } PUT /es_db/_doc/2 { \u0026#34;name\u0026#34;: \u0026#34;李四\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 28, \u0026#34;address\u0026#34;: \u0026#34;广州荔湾大厦\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java assistant\u0026#34; } PUT /es_db/_doc/3 { \u0026#34;name\u0026#34;: \u0026#34;王五\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 26, \u0026#34;address\u0026#34;: \u0026#34;广州白云山公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;php developer\u0026#34; } PUT /es_db/_doc/4 { \u0026#34;name\u0026#34;: \u0026#34;赵六\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 22, \u0026#34;address\u0026#34;: \u0026#34;长沙橘子洲\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;python assistant\u0026#34; } PUT /es_db/_doc/5 { \u0026#34;name\u0026#34;: \u0026#34;张龙\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 19, \u0026#34;address\u0026#34;: \u0026#34;长沙麓谷企业广场\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java architect assistant\u0026#34; }\tPUT /es_db/_doc/6 { \u0026#34;name\u0026#34;: \u0026#34;赵虎\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 32, \u0026#34;address\u0026#34;: \u0026#34;长沙麓谷兴工国际产业园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java architect\u0026#34; }\t​\n添加（索引）文档 格式: [PUT | POST] /索引名称/[_doc | _create ]/id ​\n# 创建文档,指定id # 如果id不存在，创建新的文档，否则先删除现有文档，再创建新的文档，版本会增加 PUT /es_db/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; }\t#创建文档，ES生成id POST /es_db/_doc { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; } ​\n​ 注意:POST和PUT都能起到创建/更新的作用，PUT需要对一个具体的资源进行操作也就是要确定id才能进行更新/创建，而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建/更新\n​ Create -如果ID已经存在，会失败\n​ 修改文档 全量更新，整个json都会替换，格式: [PUT | POST] /索引名称/_doc/id 如果文档存在，现有文档会被删除，新的文档会被索引\n​\n# 全量更新，替换整个json PUT /es_db/_doc/1/ { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25 } #查询文档 GET /es_db/_doc/1 ​\n​ 使用_update部分更新，格式: POST /索引名称/_update/id update不会删除原来的文档，而是实现真正的数据更新\n​\n# 部分更新：在原有文档上更新 # Update -文档必须已经存在，更新只会对相应字段做增量修改 POST /es_db/_update/1 { \u0026#34;doc\u0026#34;: { \u0026#34;age\u0026#34;: 28 } } #查询文档 GET /es_db/_doc/1 ​\n​ 使用 _update_by_query 更新文档 ​\nPOST /es_db/_update_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;_id\u0026#34;: 1 } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age = 30\u0026#34; } } ​\n​ 并发场景下修改文档 _seq_no和_primary_term是对_version的优化，7.X版本的ES默认使用这种方式控制版本，所以当在高并发环境下使用乐观锁机制修改文档时，要带上当前文档的_seq_no和_primary_term进行更新：\n​\nPOST /es_db/_doc/2?if_seq_no=21\u0026amp;if_primary_term=6 { \u0026#34;name\u0026#34;: \u0026#34;李四xxx\u0026#34; } 如果版本号不对，会抛出版本冲突异常，如下图：\n​ 查询文档 根据id查询文档，格式: GET /索引名称/_doc/id GET /es_db/_doc/1 条件查询 _search，格式： /索引名称/_doc/_search # 查询前10条文档 GET /es_db/_doc/_search ​\nES Search API提供了两种条件查询搜索方式：\nREST风格的请求URI，直接将参数带过去 封装到request body中，这种方式可以定义更加易读的JSON格式 ​\n#通过URI搜索，使用“q”指定查询字符串，“query string syntax” KV键值对 #条件查询, 如要查询age等于28岁的 _search?q=*:*** GET /es_db/_doc/_search?q=age:28 #范围查询, 如要查询age在25至26岁之间的 _search?q=***[** TO **] 注意: TO 必须为大写 GET /es_db/_doc/_search?q=age[25 TO 26] #查询年龄小于等于28岁的 :\u0026lt;= GET /es_db/_doc/_search?q=age:\u0026lt;=28 #查询年龄大于28前的 :\u0026gt; GET /es_db/_doc/_search?q=age:\u0026gt;28 #分页查询 from=*\u0026amp;size=* GET /es_db/_doc/_search?q=age[25 TO 26]\u0026amp;from=0\u0026amp;size=1 #对查询结果只输出某些字段 _source=字段,字段 GET /es_db/_doc/_search?_source=name,age #对查询结果排序 sort=字段:desc/asc GET /es_db/_doc/_search?sort=age:desc 通过请求体的搜索方式会在后面课程详细讲解（DSL）\n​\nGET /es_db/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;广州白云\u0026#34; } } } 删除文档 格式: DELETE /索引名称/_doc/id\nDELETE /es_db/_doc/1 ElasticSearch文档批量操作 批量操作可以减少网络连接所产生的开销，提升性能\n支持在一次API调用中，对不同的索引进行操作 可以在URI中指定Index，也可以在请求的Payload中进行 操作中单条操作失败，并不会影响其他操作 返回结果包括了每一条操作执行的结果 批量写入 批量对文档进行写操作是通过_bulk的API来实现的\n请求方式：POST\n请求地址：_bulk\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n第一行参数为指定操作的类型及操作的对象(index,type和id) 第二行参数才是操作的数据 参数类似于：\n{\u0026#34;actionName\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;indexName\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;typeName\u0026#34;,\u0026#34;_id\u0026#34;:\u0026#34;id\u0026#34;}} {\u0026#34;field1\u0026#34;:\u0026#34;value1\u0026#34;, \u0026#34;field2\u0026#34;:\u0026#34;value2\u0026#34;} actionName：表示操作类型，主要有create,index,delete和update 批量创建文档create\nPOST _bulk {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;fox老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;fox老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;id\u0026#34;:4,\u0026#34;title\u0026#34;:\u0026#34;mark老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;mark老师NB\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} 普通创建或全量替换index\nPOST _bulk {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;图灵徐庶老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;图灵学院徐庶老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;id\u0026#34;:4,\u0026#34;title\u0026#34;:\u0026#34;图灵诸葛老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;图灵学院诸葛老师NB\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} 如果原文档不存在，则是创建 如果原文档存在，则是替换(全量修改原文档) 批量删除delete\n​\nPOST _bulk {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} 批量修改update\n​\nPOST _bulk {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;doc\u0026#34;:{\u0026#34;title\u0026#34;:\u0026#34;ES大法必修内功\u0026#34;}} {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;doc\u0026#34;:{\u0026#34;create_time\u0026#34;:1554018421008}} 组合应用\n​\nPOST _bulk {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;fox老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;fox老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;doc\u0026#34;:{\u0026#34;create_time\u0026#34;:1554018421008}} 批量读取 es的批量查询可以使用mget和msearch两种。其中mget是需要我们知道它的id，可以指定不同的index，也可以指定返回值source。msearch可以通过字段查询来进行一个批量的查找。\n_mget\n​\n#可以通过ID批量获取不同index和type的数据 GET _mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;es_db\u0026#34;, \u0026#34;_id\u0026#34;: 1 }, { \u0026#34;_index\u0026#34;: \u0026#34;article\u0026#34;, \u0026#34;_id\u0026#34;: 4 } ] } #可以通过ID批量获取es_db的数据 GET /es_db/_mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_id\u0026#34;: 1 }, { \u0026#34;_id\u0026#34;: 4 } ] } #简化后 GET /es_db/_mget { \u0026#34;ids\u0026#34;:[\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;] } ​\n​ _msearch\n在_msearch中，请求格式和bulk类似。查询一条数据需要两个对象，第一个设置index和type，第二个设置查询语句。查询语句和search相同。如果只是查询一个index，我们可以在url中带上index，这样，如果查该index可以直接用空对象表示。\n​\nGET /es_db/_msearch {} {\u0026#34;query\u0026#34; : {\u0026#34;match_all\u0026#34; : {}}, \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 2} {\u0026#34;index\u0026#34; : \u0026#34;article\u0026#34;} {\u0026#34;query\u0026#34; : {\u0026#34;match_all\u0026#34; : {}}} ​ Logstash与FileBeat详解以及ELK整合 链接：http://note.youdao.com/noteshare?id=cd88d72a1c76d18efcf7fe767e8c2d20\u0026amp;sub=D7819084A43243FFA52E8A8741795414\n背景 日志管理的挑战：\n关注点很多，任何一个点都有可能引起问题 日志分散在很多机器，出了问题时，才发现日志被删了 很多运维人员是消防员，哪里有问题去哪里 ​ 集中化日志管理思路：\n日志收集 ——》格式化分析 ——》检索和可视化 ——》 风险告警\nELK架构 ELK架构分为两种，一种是经典的ELK，另外一种是加上消息队列（Redis或Kafka或RabbitMQ）和Nginx结构。\n经典的ELK 经典的ELK主要是由Filebeat + Logstash + Elasticsearch + Kibana组成，如下图：（早期的ELK只有Logstash + Elasticsearch + Kibana）\n​ 此架构主要适用于数据量小的开发环境，存在数据丢失的危险。\n整合消息队列+Nginx架构 这种架构，主要加上了Redis或Kafka或RabbitMQ做消息队列，保证了消息的不丢失。\n​ 此种架构，主要用在生产环境，可以处理大数据量，并且不会丢失数据。\n什么是Logstash Logstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的存储库中。\nhttps://www.elastic.co/cn/logstash/\n应用：ETL工具 / 数据采集处理引擎\n​ Logstash核心概念 Pipeline\n包含了input—filter-output三个阶段的处理流程 插件生命周期管理 队列管理 Logstash Event\n数据在内部流转时的具体表现形式。数据在input 阶段被转换为Event，在 output被转化成目标格式数据 Event 其实是一个Java Object，在配置文件中，对Event 的属性进行增删改查 Codec (Code / Decode)\n将原始数据decode成Event;将Event encode成目标数据\n​ Logstash数据传输原理 数据采集与输入：Logstash支持各种输入选择，能够以连续的流式传输方式，轻松地从日志、指标、Web应用以及数据存储中采集数据。 实时解析和数据转换：通过Logstash过滤器解析各个事件，识别已命名的字段来构建结构，并将它们转换成通用格式，最终将数据从源端传输到存储库中。 存储与数据导出：Logstash提供多种输出选择，可以将数据发送到指定的地方。 Logstash通过管道完成数据的采集与处理，管道配置中包含input、output和filter（可选）插件，input和output用来配置输入和输出数据源、filter用来对数据进行过滤或预处理。\n​ Logstash配置文件结构 参考：https://www.elastic.co/guide/en/logstash/7.17/configuration.html\nLogstash的管道配置文件对每种类型的插件都提供了一个单独的配置部分，用于处理管道事件。\ninput { stdin { } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{COMBINEDAPACHELOG}\u0026#34; } } date { match =\u0026gt; [ \u0026#34;timestamp\u0026#34; , \u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;]} stdout { codec =\u0026gt; rubydebug } } 每个配置部分可以包含一个或多个插件。例如，指定多个filter插件，Logstash会按照它们在配置文件中出现的顺序进行处理。\n#运行 bin/logstash -f logstash-demo.conf Input Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/input-plugins.html\n一个 Pipeline可以有多个input插件\nStdin / File\nBeats / Log4J /Elasticsearch / JDBC / Kafka /Rabbitmq /Redis\nJMX/ HTTP / Websocket / UDP / TCP\nGoogle Cloud Storage / S3\nGithub / Twitter\nOutput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/output-plugins.html\n将Event发送到特定的目的地，是 Pipeline 的最后一个阶段。\n常见 Output Plugins：\nElasticsearch Email / Pageduty Influxdb / Kafka / Mongodb / Opentsdb / Zabbix Http / TCP / Websocket Filter Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/filter-plugins.html\n处理Event\n内置的Filter Plugins:\nMutate 一操作Event的字段 Metrics — Aggregate metrics Ruby 一执行Ruby 代码 Codec Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/codec-plugins.html\n将原始数据decode成Event;将Event encode成目标数据\n内置的Codec Plugins:\nLine / Multiline JSON / Avro / Cef (ArcSight Common Event Format) Dots / Rubydebug Logstash Queue In Memory Queue 进程Crash，机器宕机，都会引起数据的丢失\nPersistent Queue 机器宕机，数据也不会丢失; 数据保证会被消费; 可以替代 Kafka等消息队列缓冲区的作用\nqueue.type: persisted (默认是memory) queue.max_bytes: 4gb ​ Logstash安装 logstash官方文档: https://www.elastic.co/guide/en/logstash/7.17/installing-logstash.html\n1）下载并解压logstash 下载地址： https://www.elastic.co/cn/downloads/past-releases#logstash\n选择版本：7.17.3\n​ 2）测试：运行最基本的logstash管道 ​\ncd logstash-7.17.3 #linux #-e选项表示，直接把配置放在命令中，这样可以有效快速进行测试 bin/logstash -e \u0026#39;input { stdin { } } output { stdout {} }\u0026#39; #windows .\\bin\\logstash.bat -e \u0026#34;input { stdin { } } output { stdout {} }\u0026#34; ​\n测试结果：\n​ window版本的logstash-7.17.3的bug:\nwindows出现错误提示could not find java; set JAVA_HOME or ensure java is in PATH\n​ 修改setup.bat\n​ ​ Codec Plugin测试\n# single line bin/logstash -e \u0026#34;input{stdin{codec=\u0026gt;line}}output{stdout{codec=\u0026gt; rubydebug}}\u0026#34; bin/logstash -e \u0026#34;input{stdin{codec=\u0026gt;json}}output{stdout{codec=\u0026gt; rubydebug}}\u0026#34; ​\nCodec Plugin —— Multiline\n设置参数:\npattern: 设置行匹配的正则表达式\nwhat : 如果匹配成功，那么匹配行属于上一个事件还是下一个事件\nprevious / next negate : 是否对pattern结果取反\ntrue / false ​\n# 多行数据，异常 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) # multiline-exception.conf input { stdin { codec =\u0026gt; multiline { pattern =\u0026gt; \u0026#34;^\\s\u0026#34; what =\u0026gt; \u0026#34;previous\u0026#34; } } } filter {} output { stdout { codec =\u0026gt; rubydebug } } #执行管道 bin/logstash -f multiline-exception.conf Input Plugin —— File\n支持从文件中读取数据，如日志文件 文件读取需要解决的问题：只被读取一次。重启后需要从上次读取的位置继续(通过sincedb 实现) 读取到文件新内容，发现新文件 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取 Filter Plugin\nFilter Plugin可以对Logstash Event进行各种处理，例如解析，删除字段，类型转换\nDate: 日期解析 Dissect: 分割符解析 Grok: 正则匹配解析 Mutate: 处理字段。重命名，删除，替换 Ruby: 利用Ruby 代码来动态修改Event Filter Plugin - Mutate\n对字段做各种操作:\nConvert : 类型转换 Gsub : 字符串替换 Split / Join /Merge: 字符串切割，数组合并字符串，数组合并数组 Rename: 字段重命名 Update / Replace: 字段内容更新替换 Remove_field: 字段删除 Logstash导入数据到ES 1）测试数据集下载：https://grouplens.org/datasets/movielens/\n​ 2）准备logstash-movie.conf配置文件\ninput { file { path =\u0026gt; \u0026#34;/home/es/logstash-7.17.3/dataset/movies.csv\u0026#34; start_position =\u0026gt; \u0026#34;beginning\u0026#34; sincedb_path =\u0026gt; \u0026#34;/dev/null\u0026#34; } } filter { csv { separator =\u0026gt; \u0026#34;,\u0026#34; columns =\u0026gt; [\u0026#34;id\u0026#34;,\u0026#34;content\u0026#34;,\u0026#34;genre\u0026#34;] } mutate { split =\u0026gt; { \u0026#34;genre\u0026#34; =\u0026gt; \u0026#34;|\u0026#34; } remove_field =\u0026gt; [\u0026#34;path\u0026#34;, \u0026#34;host\u0026#34;,\u0026#34;@timestamp\u0026#34;,\u0026#34;message\u0026#34;] } mutate { split =\u0026gt; [\u0026#34;content\u0026#34;, \u0026#34;(\u0026#34;] add_field =\u0026gt; { \u0026#34;title\u0026#34; =\u0026gt; \u0026#34;%{[content][0]}\u0026#34;} add_field =\u0026gt; { \u0026#34;year\u0026#34; =\u0026gt; \u0026#34;%{[content][1]}\u0026#34;} } mutate { convert =\u0026gt; { \u0026#34;year\u0026#34; =\u0026gt; \u0026#34;integer\u0026#34; } strip =\u0026gt; [\u0026#34;title\u0026#34;] remove_field =\u0026gt; [\u0026#34;path\u0026#34;, \u0026#34;host\u0026#34;,\u0026#34;@timestamp\u0026#34;,\u0026#34;message\u0026#34;,\u0026#34;content\u0026#34;] } } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://localhost:9200\u0026#34; index =\u0026gt; \u0026#34;movies\u0026#34; document_id =\u0026gt; \u0026#34;%{id}\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout {} } ​\n3）运行logstash\n​\n# linux bin/logstash -f logstash-movie.conf ​\n同步数据库数据到Elasticsearch 需求: 将数据库中的数据同步到ES，借助ES的全文搜索,提高搜索速度\n需要把新增用户信息同步到Elasticsearch中 用户信息Update 后，需要能被更新到Elasticsearch 支持增量更新 用户注销后，不能被ES所搜索到 实现思路\n基于canal同步数据（项目实战中讲解）\n借助JDBC Input Plugin将数据从数据库读到Logstash\n需要自己提供所需的 JDBC Driver； JDBC Input Plugin 支持定时任务 Scheduling，其语法来自 Rufus-scheduler，其扩展了 Cron，使用 Cron 的语法可以完成任务的触发； JDBC Input Plugin 支持通过 Tracking_column / sql_last_value 的方式记录 State，最终实现增量的更新； https://www.elastic.co/cn/blog/logstash-jdbc-input-plugin JDBC Input Plugin实现步骤\n1）拷贝jdbc依赖到logstash-7.17.3/drivers目录下\n2）准备mysql-demo.conf配置文件\ninput { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;/home/es/logstash-7.17.3/drivers/mysql-connector-java-5.1.49.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://localhost:3306/test?useSSL=false\u0026#34; jdbc_user =\u0026gt; \u0026#34;root\u0026#34; jdbc_password =\u0026gt; \u0026#34;123456\u0026#34; #启用追踪，如果为true，则需要指定tracking_column use_column_value =\u0026gt; true #指定追踪的字段， tracking_column =\u0026gt; \u0026#34;last_updated\u0026#34; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =\u0026gt; \u0026#34;numeric\u0026#34; #记录最后一次运行的结果 record_last_run =\u0026gt; true #上面运行结果的保存位置 last_run_metadata_path =\u0026gt; \u0026#34;jdbc-position.txt\u0026#34; statement =\u0026gt; \u0026#34;SELECT * FROM user where last_updated \u0026gt;:sql_last_value;\u0026#34; schedule =\u0026gt; \u0026#34; * * * * * *\u0026#34; } } output { elasticsearch { document_id =\u0026gt; \u0026#34;%{id}\u0026#34; document_type =\u0026gt; \u0026#34;_doc\u0026#34; index =\u0026gt; \u0026#34;users\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 3）运行logstash\nbin/logstash -f mysql-demo.conf 测试\n#user表 CREATE TABLE `user` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `address` varchar(50) CHARACTER DEFAULT NULL, `last_updated` bigint DEFAULT NULL, `is_deleted` int DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; #插入数据 INSERT INTO user(name,address,last_updated,is_deleted) VALUES(\u0026#34;张三\u0026#34;,\u0026#34;广州天河\u0026#34;,unix_timestamp(NOW()),0) ​ # 更新 update user set address=\u0026#34;广州白云山\u0026#34;,last_updated=unix_timestamp(NOW()) where name=\u0026#34;张三\u0026#34; ​\n​ #删除 update user set is_deleted=1,last_updated=unix_timestamp(NOW()) where name=\u0026#34;张三\u0026#34; ​ #ES中查询 # 创建 alias，只显示没有被标记 deleted的用户 POST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;view_users\u0026#34;, \u0026#34;filter\u0026#34; : { \u0026#34;term\u0026#34; : { \u0026#34;is_deleted\u0026#34; : 0} } } } ] } # 通过 Alias查询，查不到被标记成 deleted的用户 POST view_users/_search POST view_users/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;name.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;张三\u0026#34; } } } } ​\n什么是Beats 轻量型数据采集器，文档地址： https://www.elastic.co/guide/en/beats/libbeat/7.17/index.html\nBeats 是一个免费且开放的平台，集合了多种单一用途的数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。\n​ FileBeat简介 FileBeat专门用于转发和收集日志数据的轻量级采集工具。它可以作为代理安装在服务器上，FileBeat监视指定路径的日志文件，收集日志数据，并将收集到的日志转发到Elasticsearch或者Logstash。\nFileBeat的工作原理 启动FileBeat时，会启动一个或者多个输入（Input），这些Input监控指定的日志数据位置。FileBeat会针对每一个文件启动一个Harvester（收割机）。Harvester读取每一个文件的日志，将新的日志发送到libbeat，libbeat将数据收集到一起，并将数据发送给输出（Output）。\n​ logstash vs FileBeat Logstash是在jvm上运行的，资源消耗比较大。而FileBeat是基于golang编写的，功能较少但资源消耗也比较小，更轻量级。 Logstash 和Filebeat都具有日志收集功能，Filebeat更轻量，占用资源更少 Logstash 具有Filter功能，能过滤分析日志 一般结构都是Filebeat采集日志，然后发送到消息队列、Redis、MQ中，然后Logstash去获取，利用Filter功能过滤分析，然后存储到Elasticsearch中 FileBeat和Logstash配合，实现背压机制。当将数据发送到Logstash或 Elasticsearch时，Filebeat使用背压敏感协议，以应对更多的数据量。如果Logstash正在忙于处理数据，则会告诉Filebeat 减慢读取速度。一旦拥堵得到解决，Filebeat就会恢复到原来的步伐并继续传输数据。 Filebeat安装 https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html\n1）下载并解压Filebeat\n下载地址：https://www.elastic.co/cn/downloads/past-releases#filebeat\n选择版本：7.17.3\n​ 2）编辑配置\n修改 filebeat.yml 以设置连接信息：\n​\noutput.elasticsearch: hosts: [\u0026#34;192.168.65.174:9200\u0026#34;,\u0026#34;192.168.65.192:9200\u0026#34;,\u0026#34;192.168.65.204:9200\u0026#34;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;123456\u0026#34; setup.kibana: host: \u0026#34;192.168.65.174:5601\u0026#34; ​\n3) 启用和配置数据收集模块\n从安装目录中，运行：\n# 查看可以模块列表 ./filebeat modules list #启用nginx模块 ./filebeat modules enable nginx #如果需要更改nginx日志路径,修改modules.d/nginx.yml - module: nginx access: var.paths: [\u0026#34;/var/log/nginx/access.log*\u0026#34;] #启用 Logstash 模块 ./filebeat modules enable logstash #在 modules.d/logstash.yml 文件中修改设置 - module: logstash log: enabled: true var.paths: [\u0026#34;/home/es/logstash-7.17.3/logs/*.log\u0026#34;] 4）启动 Filebeat\n# setup命令加载Kibana仪表板。 如果仪表板已经设置，则忽略此命令。 ./filebeat setup # 启动Filebeat ./filebeat -e ELK整合实战 案例：采集tomcat服务器日志 Tomcat服务器运行过程中产生很多日志信息，通过Logstash采集并存储日志信息至ElasticSearch中\n使用FileBeats将日志发送到Logstash 1）创建配置文件filebeat-logstash.yml，配置FileBeats将数据发送到Logstash\nvim filebeat-logstash.yml chmod 644 filebeat-logstash.yml #因为Tomcat的web log日志都是以IP地址开头的，所以我们需要修改下匹配字段。 # 不以ip地址开头的行追加到上一行 filebeat.inputs: - type: log enabled: true paths: - /home/es/apache-tomcat-8.5.33/logs/*access*.* multiline.pattern: \u0026#39;^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+ \u0026#39; multiline.negate: true multiline.match: after output.logstash: enabled: true hosts: [\u0026#34;192.168.65.204:5044\u0026#34;] ​\npattern：正则表达式 negate：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行 match：after 或 before，合并到上一行的末尾或开头 2）启动FileBeat，并指定使用指定的配置文件\n./filebeat -e -c filebeat-logstash.yml 可能出现的异常：\n异常1：Exiting: error loading config file: config file (\u0026ldquo;filebeat-logstash.yml\u0026rdquo;) can only be writable by the owner but the permissions are \u0026ldquo;-rw-rw-r\u0026ndash;\u0026rdquo; (to fix the permissions use: \u0026lsquo;chmod go-w /home/es/filebeat-7.17.3-linux-x86_64/filebeat-logstash.yml\u0026rsquo;)\n因为安全原因不要其他用户写的权限，去掉写的权限就可以了\n​ chmod 644 filebeat-logstash.yml\n异常2：Failed to connect to backoff(async(tcp://192.168.65.204:5044)): dial tcp 192.168.65.204:5044: connect: connection refused\nFileBeat将尝试建立与Logstash监听的IP和端口号进行连接。但此时，我们并没有开启并配置Logstash，所以FileBeat是无法连接到Logstash的。\n配置Logstash接收FileBeat收集的数据并打印 vim config/filebeat-console.conf # 配置从FileBeat接收数据 input { beats { port =\u0026gt; 5044 } } output { stdout { codec =\u0026gt; rubydebug } } 测试logstash配置是否正确\nbin/logstash -f config/filebeat-console.conf --config.test_and_exit 启动logstash\n# reload.automatic：修改配置文件时自动重新加载 bin/logstash -f config/filebeat-console.conf --config.reload.automatic ​\n测试访问tomcat，logstash是否接收到了Filebeat传过来的tomcat日志\nLogstash输出数据到Elasticsearch 如果我们需要将数据输出值ES而不是控制台的话，我们修改Logstash的output配置。\nvim config/filebeat-elasticSearch.conf input { beats { port =\u0026gt; 5044 } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 启动logstash\nbin/logstash -f config/filebeat-elasticSearch.conf --config.reload.automatic ​\nES中会生成一个以logstash开头的索引，测试日志是否保存到了ES。\n思考：日志信息都保证在message字段中，是否可以把日志进行解析一个个的字段？例如：IP字段、时间、请求方式、请求URL、响应结果。\n利用Logstash过滤器解析日志 从日志文件中收集到的数据包含了很多有效信息，比如IP、时间等，在Logstash中可以配置过滤器Filter对采集到的数据进行过滤处理，Logstash中有大量的插件可以供我们使用。\n#查看Logstash已经安装的插件 bin/logstash-plugin list Grok插件\nGrok是一种将非结构化日志解析为结构化的插件。这个工具非常适合用来解析系统日志、Web服务器日志、MySQL或者是任意其他的日志格式。\nhttps://www.elastic.co/guide/en/logstash/7.17/plugins-filters-grok.html\nGrok语法\nGrok是通过模式匹配的方式来识别日志中的数据,可以把Grok插件简单理解为升级版本的正则表达式。它拥有更多的模式，默认Logstash拥有120个模式。如果这些模式不满足我们解析日志的需求，我们可以直接使用正则表达式来进行匹配。\ngrok模式的语法是：\n%{SYNTAX:SEMANTIC} SYNTAX（语法）指的是Grok模式名称，SEMANTIC（语义）是给模式匹配到的文本字段名。例如：\n%{NUMBER:duration} %{IP:client} duration表示：匹配一个数字，client表示匹配一个IP地址。 默认在Grok中，所有匹配到的的数据类型都是字符串，如果要转换成int类型（目前只支持int和float），可以这样：%{NUMBER:duration:int} %{IP:client}\n常用的Grok模式\nhttps://help.aliyun.com/document_detail/129387.html?scm=20140722.184.2.173\n用法\nfilter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026#34; } } } 比如，tomacat日志\n192.168.65.103 - - [23/Jun/2022:22:37:23 +0800] \u0026#34;GET /docs/images/docs-stylesheet.css HTTP/1.1\u0026#34; 200 5780 解析后的字段\n字段名 说明 client IP 浏览器端IP timestamp 请求的时间戳 method 请求方式（GET/POST） uri 请求的链接地址 status 服务器端响应状态 length 响应的数据长度 grok模式\n​\n%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status} %{INT:length} ​\n为了方便测试，我们可以使用Kibana来进行Grok开发：\n​ 修改Logstash配置文件\nvim config/filebeat-console.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status:int} %{INT:length:int}\u0026#34; } } } output { stdout { codec =\u0026gt; rubydebug } } 启动logstash测试\nbin/logstash -f config/filebeat-console.conf --config.reload.automatic 使用mutate插件过滤掉不需要的字段\nmutate { enable_metric =\u0026gt; \u0026#34;false\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;, \u0026#34;log\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;input\u0026#34;, \u0026#34;agent\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;@version\u0026#34;] } 要将日期格式进行转换，我们可以使用Date插件来实现。该插件专门用来解析字段中的日期，官方说明文档：https://www.elastic.co/guide/en/logstash/7.17/plugins-filters-date.html\n用法如下：\n​ 将date字段转换为「年月日 时分秒」格式。默认字段经过date插件处理后，会输出到@timestamp字段，所以，我们可以通过修改target属性来重新定义输出字段。\ndate { match =\u0026gt; [\u0026#34;date\u0026#34;,\u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] target =\u0026gt; \u0026#34;date\u0026#34; } ​\n输出到Elasticsearch指定索引 index来指定索引名称，默认输出的index名称为：logstash-%{+yyyy.MM.dd}。但注意，要在index中使用时间格式化，filter的输出必须包含 @timestamp字段，否则将无法解析日期。\noutput { elasticsearch { index =\u0026gt; \u0026#34;tomcat_web_log_%{+YYYY-MM}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 注意：index名称中，不能出现大写字符\n完整的Logstash配置文件\nvim config/filebeat-filter-es.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status:int} %{INT:length:int}\u0026#34; } } mutate { enable_metric =\u0026gt; \u0026#34;false\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;, \u0026#34;log\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;input\u0026#34;, \u0026#34;agent\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;@version\u0026#34;] } date { match =\u0026gt; [\u0026#34;date\u0026#34;,\u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] target =\u0026gt; \u0026#34;date\u0026#34; } } output { stdout { codec =\u0026gt; rubydebug } elasticsearch { index =\u0026gt; \u0026#34;tomcat_web_log_%{+YYYY-MM}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } } 启动logstash\nbin/logstash -f config/filebeat-filter-es.conf --config.reload.automatic input { redis { host=\u0026gt; \u0026#34;localhost\u0026#34; port =\u0026gt; \u0026#34;6379\u0026#34; password =\u0026gt; \u0026#34;9ed99d6b\u0026#34; key =\u0026gt; \u0026#34;filebeat\u0026#34; type =\u0026gt; \u0026#34;redis-input\u0026#34; data_type =\u0026gt; \u0026#34;list\u0026#34; threads =\u0026gt;4 batch_count =\u0026gt; 10 db =\u0026gt; 0 } } filter { } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#34; index =\u0026gt; \u0026#34;netlog\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; } } logstash 问题处理\nhttps://blog.csdn.net/King_weng/article/details/106506996\nELFK整合实战2 filebeat =\u0026gt; redis =\u0026gt; logstash =\u0026gt; elasticsearch =\u0026gt; kinbana\nfilebeat的配置 filebeat.yml\nfilebeat.inputs: - type: filestream paths: - /var/log/data/10.42.76.202/*.log - /var/log/data/10.42.76.201/*.log - /var/log/data/10.42.76.204/*.log encoding: gbk # 对非utf-8的数据进行转码 ignore_older: 5m #只采集5分钟内更新的文件 - type: filestream paths: - /var/log/data/10.42.76.206/*.log - /var/log/data/10.42.76.207/*.log ignore_older: 5m output.redis: hosts: [\u0026#34;10.43.152.65:6379\u0026#34;] password: \u0026#34;9ed99d6b\u0026#34; key: \u0026#34;filebeat\u0026#34; db: 0 timeout: 5 filebeat 启动命令\nnohup ./filebeat -e -c filebeat.yml \u0026gt;/dev/null \u0026amp; logstash的配置 logstash_run.yml\ninput { redis { host=\u0026gt; \u0026#34;localhost\u0026#34; port =\u0026gt; \u0026#34;6379\u0026#34; password =\u0026gt; \u0026#34;9ed99d6b\u0026#34; key =\u0026gt; \u0026#34;filebeat\u0026#34; # 对应redis中的key名称 type =\u0026gt; \u0026#34;redis-input\u0026#34; data_type =\u0026gt; \u0026#34;list\u0026#34; # key的类型 threads =\u0026gt;4 batch_count =\u0026gt; 10 db =\u0026gt; 0 } } filter { } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#34; index =\u0026gt; \u0026#34;netlog\u0026#34; # 在es中的索引名称 user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; } } 通过jvm.options文件修改jvm参数\n制作logstash镜像(可选) start.sh启动脚本\n#!/bin/bash # -f 后指定的配置文件需是新的文件 bin/logstash -f config/logstash_run.yml --config.reload.automatic Dockerfile文件\nFROM centos:7 MAINTAINER wandong RUN yum install -y wget java-1.8.0-openjdk curl unzip iproute net-tools \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum/* ADD logstash-7.15.2-linux-x86_64.tar.gz /opt/ WORKDIR /opt/logstash-7.15.2 COPY start.sh /opt/logstash-7.15.2 EXPOSE 5044 9600 CMD [\u0026#34;sh\u0026#34;,\u0026#34;start.sh\u0026#34;] log.file.path : 10.42.76.201 and message : 39.144.219.132\n","permalink":"https://wandong1.github.io/post/elasticsearch%E5%85%A5%E9%97%A8/","summary":"ES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\nElasticSearch快速入门实战 note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引）\n文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）","title":"离线安装docker"},{"content":"离线安装docker https://download.docker.com/linux/static/stable/x86_64/docker-20.10.14.tgz\n#解压 tar -xvzf docker-20.10.14.tgz -C /opt/ chown root:root -R /opt/docker/ cp /opt/docker/* /usr/bin cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP \\$MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target EOF chmod +x /etc/systemd/system/docker.service # 加载service配置 systemctl daemon-reload #设置开机启动 并立即启动 systemctl enable docker.service --now ","permalink":"https://wandong1.github.io/post/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85docker/","summary":"离线安装docker https://download.docker.com/linux/static/stable/x86_64/docker-20.10.14.tgz\n#解压 tar -xvzf docker-20.10.14.tgz -C /opt/ chown root:root -R /opt/docker/ cp /opt/docker/* /usr/bin cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP \\$MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target EOF chmod +x /etc/systemd/system/docker.service # 加载service配置 systemctl daemon-reload #设置开机启动 并立即启动 systemctl enable docker.service --now ","title":"离线安装docker"},{"content":"随笔 技术的变革，一定是思想先行，云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。\n符合云原生架构的应用程序应该是：采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。\n云原生的四要素 微服务：几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用，微服务有理论基础，那就是康威定律，指导服务怎么切分，很玄乎，凡是能称为理论定律的都简单明白不了，不然就忒没b格，大概意思是组织架构决定产品形态，不知道跟马克思的生产关系影响生产力有无关系。\n微服务架构的好处就是按function切了之后，服务解耦，内聚更强，变更更易；另一个划分服务的技巧据说是依据DDD来搞。\n容器化：Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用，是基于LXC技术搞的，容器化为微服务提供实施保障，起到应用隔离作用，K8S是容器编排系统，用于容器管理，容器间的负载均衡，谷歌搞的，Docker和K8S都采用Go编写，都是好东西。\nDevOps：这是个组合词，Dev+Ops，就是开发和运维合体，不像开发和产品，经常刀刃相见，实际上DevOps应该还包括测试，DevOps是一个敏捷思维，是一个沟通文化，也是组织形式，为云原生提供持续交付能力。\n持续交付：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。\n如何云原生？ 首先，云原生借了云计算的东风，没有云计算，自然没有云原生，云计算是云原生的基础。\n随着虚拟化技术的成熟和分布式框架的普及，在容器技术、可持续交付、编排系统等开源社区的推动下，以及微服务等开发理念的带动下，应用上云已经是不可逆转的趋势。\n云计算的3层划分，即基础设施即服务(IaaS)、平台即服务(PaaS)、软件即服务(SaaS)为云原生提供了技术基础和方向指引，真正的云化不仅仅是基础设施和平台的变化，应用也需要做出改变，摈弃传统的土方法，在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点，重新设计，从而建设全新的云化的应用，即云原生应用。\n1.本地部署的传统应用往往采用c/c++、企业级java编写，而云原生应用则需要用以网络为中心的go、node.js等新兴语言编写。\n2.本地部署的传统应用可能需要停机更新，而云原生应用应该始终是最新的，需要支持频繁变更，持续交付，蓝绿部署。\n3.本地部署的传统应用无法动态扩展，往往需要冗余资源以抵抗流量高峰，而云原生应用利用云的弹性自动伸缩，通过共享降本增效。\n4.本地部署的传统应用对网络资源，比如ip、端口等有依赖，甚至是硬编码，而云原生应用对网络和存储都没有这种限制。\n5.本地部署的传统应用通常人肉部署手工运维，而云原生应用这一切都是自动化的。\n6.本地部署的传统应用通常依赖系统环境，而云原生应用不会硬连接到任何系统环境，而是依赖抽象的基础架构，从而获得良好移植性。\n7.本地部署的传统应用有些是单体(巨石)应用，或者强依赖，而基于微服务架构的云原生应用，纵向划分服务，模块化更合理。\n可见，要转向云原生应用需要以新的云原生方法开展工作，云原生包括很多方面：基础架构服务、虚拟化、容器化、容器编排、微服务。幸运的是，开源社区在云原生应用方面做出了大量卓有成效的工作，很多开源的框架和设施可以通过拿来主义直接用，2013年Docker推出并很快成为容器事实标准，随后围绕容器编排的混战中，2017年诞生的k8s很快脱颖而出，而这些技术极大的降低了开发云原生应用的技术门槛。\n","permalink":"https://wandong1.github.io/post/%E9%9A%8F%E7%AC%94/","summary":"随笔 技术的变革，一定是思想先行，云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。\n符合云原生架构的应用程序应该是：采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。\n云原生的四要素 微服务：几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用，微服务有理论基础，那就是康威定律，指导服务怎么切分，很玄乎，凡是能称为理论定律的都简单明白不了，不然就忒没b格，大概意思是组织架构决定产品形态，不知道跟马克思的生产关系影响生产力有无关系。\n微服务架构的好处就是按function切了之后，服务解耦，内聚更强，变更更易；另一个划分服务的技巧据说是依据DDD来搞。\n容器化：Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用，是基于LXC技术搞的，容器化为微服务提供实施保障，起到应用隔离作用，K8S是容器编排系统，用于容器管理，容器间的负载均衡，谷歌搞的，Docker和K8S都采用Go编写，都是好东西。\nDevOps：这是个组合词，Dev+Ops，就是开发和运维合体，不像开发和产品，经常刀刃相见，实际上DevOps应该还包括测试，DevOps是一个敏捷思维，是一个沟通文化，也是组织形式，为云原生提供持续交付能力。\n持续交付：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。\n如何云原生？ 首先，云原生借了云计算的东风，没有云计算，自然没有云原生，云计算是云原生的基础。\n随着虚拟化技术的成熟和分布式框架的普及，在容器技术、可持续交付、编排系统等开源社区的推动下，以及微服务等开发理念的带动下，应用上云已经是不可逆转的趋势。\n云计算的3层划分，即基础设施即服务(IaaS)、平台即服务(PaaS)、软件即服务(SaaS)为云原生提供了技术基础和方向指引，真正的云化不仅仅是基础设施和平台的变化，应用也需要做出改变，摈弃传统的土方法，在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点，重新设计，从而建设全新的云化的应用，即云原生应用。\n1.本地部署的传统应用往往采用c/c++、企业级java编写，而云原生应用则需要用以网络为中心的go、node.js等新兴语言编写。\n2.本地部署的传统应用可能需要停机更新，而云原生应用应该始终是最新的，需要支持频繁变更，持续交付，蓝绿部署。\n3.本地部署的传统应用无法动态扩展，往往需要冗余资源以抵抗流量高峰，而云原生应用利用云的弹性自动伸缩，通过共享降本增效。\n4.本地部署的传统应用对网络资源，比如ip、端口等有依赖，甚至是硬编码，而云原生应用对网络和存储都没有这种限制。\n5.本地部署的传统应用通常人肉部署手工运维，而云原生应用这一切都是自动化的。\n6.本地部署的传统应用通常依赖系统环境，而云原生应用不会硬连接到任何系统环境，而是依赖抽象的基础架构，从而获得良好移植性。\n7.本地部署的传统应用有些是单体(巨石)应用，或者强依赖，而基于微服务架构的云原生应用，纵向划分服务，模块化更合理。\n可见，要转向云原生应用需要以新的云原生方法开展工作，云原生包括很多方面：基础架构服务、虚拟化、容器化、容器编排、微服务。幸运的是，开源社区在云原生应用方面做出了大量卓有成效的工作，很多开源的框架和设施可以通过拿来主义直接用，2013年Docker推出并很快成为容器事实标准，随后围绕容器编排的混战中，2017年诞生的k8s很快脱颖而出，而这些技术极大的降低了开发云原生应用的技术门槛。","title":"随笔"},{"content":"基于Docker和Kubernetes的企业级DevOps实践训练营 课程准备 离线镜像包\n百度：https://pan.baidu.com/s/1N1AYGCYftYGn6L0QPMWIMw 提取码：ev2h\n天翼云：https://cloud.189.cn/t/ENjUbmRR7FNz\nCentOS7.4版本以上 虚拟机3台（4C+8G+50G），内网互通，可连外网\n课件文档\n《训练营课件》 《安装手册》 git仓库\nhttps://gitee.com/agagin/python-demo.git python demo项目\nhttps://gitee.com/agagin/demo-resources.git demo项目演示需要的资源文件\n关于本人 李永信\n2012-2017，云平台开发工程师，先后对接过Vmware、OpenStack、Docker平台\n2017-2019， 运维开发工程师，Docker+Kubernetes的Paas平台运维开发\n2019至今，DevOps工程师\n8年多的时间，积攒了一定的开发和运维经验，跟大家分享。\n课程安排 2020.4.11 Docker + kubernetes\n2020.4.18 DevOps平台实践\n2天的时间，节奏会相对快一些\n小调研：\nA : 只听过docker，几乎没有docker的使用经验 B：有一定的docker实践经验，不熟悉或者几乎没用过k8s C：对于docker和k8s都有一定的实践经验，想更多了解如何基于docker+k8s构建devops平台 D：其他 课程介绍 最近的三年多时间，关注容器圈的话应该会知道这么几个事情：\n容器技术持续火爆\nKubernetes(k8s)成为容器编排管理的标准\n国内外厂商均已开始了全面拥抱Kubernetes的转型， 无数中小型企业已经落地 Kubernetes，或正走落地的道路上 。基于目前的发展趋势可以预见，未来几年以kubernetes平台为核心的容器运维管理、DevOps等将迎来全面的发展。\n本着实践为核心的思想，本课程使用企业常见的基于Django + uwsgi + Nginx架构的Python Demo项目，分别讲述三个事情：\n项目的容器化\n教大家如何把公司的项目做成容器，并且运行在docker环境中\n使用Kubernetes集群来管理容器化的项目\n带大家一步一步部署k8s集群，并把容器化后的demo项目使用k8s来管理起来\n使用Jenkins和Kubernetes集成，实现demo项目的持续集成/持续交付(CI/CD)\n会使用k8s管理应用生命周期后，还差最后的环节，就是如何把开发、测试、部署的流程使用自动化工具整合起来，最后一部分呢，课程会教会大家如何优雅的使用gitlab+Jenkins+k8s构建企业级的DevOps平台\n流程示意 你将学到哪些 Docker相关\n如何使用Dockerfile快速构建镜像 Docker镜像、容器、仓库的常用操作 Docker容器的网络（Bridge下的SNAT、DNAT） Kubernetes相关\n集群的快速搭建 kubernetes的架构及工作流程 使用Pod控制器管理业务应用的生命周期 使用CoreDNS、Service和Ingress实现服务发现、负载均衡及四层、七层网络的访问 Kubernetes的认证授权体系 使用EFK构建集群业务应用的日志收集系统\n基于Gitlab+Jenkins+k8s构建DevOps平台\nJenkins介绍及流水线的使用 Jenkinsfile及多分支流水线的实际应用 Jenkins集成sonarQube、Docker、Kubernetes 使用groovy编写sharedLibrary，实现CI/CD流程的优化 第一章 走进Docker的世界 介绍docker的前世今生，了解docker的实现原理，以Django项目为例，带大家如何编写最佳的Dockerfile构建镜像。通过本章的学习，大家会知道docker的概念及基本操作，并学会构建自己的业务镜像，并通过抓包的方式掌握Docker最常用的bridge网络模式的通信。\n认识docker 怎么出现的 轻量、高效的虚拟化\nDocker 公司位于旧金山,原名dotCloud，底层利用了Linux容器技术（在操作系统中实现资源隔离与限制）。为了方便创建和管理这些容器，dotCloud 开发了一套内部工具，之后被命名为“Docker”。Docker就是这样诞生的。\n（思考为啥要用Linux容器技术？）\nHypervisor： 一种运行在基础物理服务器和操作系统之间的中间软件层，可允许多个操作系统和应用共享硬件 。常见的VMware的 Workstation 、ESXi、微软的Hyper-V或者思杰的XenServer。\nContainer Runtime：通过Linux内核虚拟化能力管理多个容器，多个容器共享一套操作系统内核。因此摘掉了内核占用的空间及运行所需要的耗时，使得容器极其轻量与快速。\n软件交付过程中的环境依赖\n几个知识点 可以把应用程序代码及运行依赖环境打包成镜像，作为交付介质，在各环境部署\n可以将镜像（image）启动成为容器(container)，并且提供多容器的生命周期进行管理（启、停、删）\ncontainer容器之间相互隔离，且每个容器可以设置资源限额\n提供轻量级虚拟化功能，容器就是在宿主机中的一个个的虚拟的空间，彼此相互隔离，完全独立\nCS架构的软件产品\n版本管理 Docker 引擎主要有两个版本：企业版（EE）和社区版（CE） 每个季度(1-3,4-6,7-9,10-12)，企业版和社区版都会发布一个稳定版本(Stable)。社区版本会提供 4 个月的支持，而企业版本会提供 12 个月的支持 每个月社区版还会通过 Edge 方式发布月度版 从 2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年 6 月第一次发布的社区版本为 18.06.0-ce 发展史 13年成立，15年开始，迎来了飞速发展。\nDocker 1.8之前，使用LXC，Docker在上层做了封装， 把LXC复杂的容器创建与使用方式简化为自己的一套命令体系。\n之后，为了实现跨平台等复杂的场景，Docker抽出了libcontainer项目，把对namespace、cgroup的操作封装在libcontainer项目里，支持不同的平台类型。\n2015年6月，Docker牵头成立了 OCI（Open Container Initiative开放容器计划）组织，这个组织的目的是建立起一个围绕容器的通用标准 。 容器格式标准是一种不受上层结构绑定的协议，即不限于某种特定操作系统、硬件、CPU架构、公有云等 ， 允许任何人在遵循该标准的情况下开发应用容器技术，这使得容器技术有了一个更广阔的发展空间。\nOCI成立后，libcontainer 交给OCI组织来维护，但是libcontainer中只包含了与kernel交互的库，因此基于libcontainer项目，后面又加入了一个CLI工具，并且项目改名为runC (https://github.com/opencontainers/runc )， 目前runC已经成为一个功能强大的runtime工具。\nDocker也做了架构调整。将容器运行时相关的程序从docker daemon剥离出来，形成了containerd。containerd向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化，确保原有接口向下兼容。向下通过containerd-shim结合runC，使得引擎可以独立升级，避免之前Docker Daemon升级会导致所有容器不可用的问题。\n也就是说\nrunC（libcontainer）是符合OCI标准的一个实现，与底层系统交互 containerd是实现了OCI之上的容器的高级功能，比如镜像管理、容器执行的调用等 Dockerd目前是最上层与CLI交互的进程，接收cli的请求并与containerd协作 小结 为了解决软件交付过程中的环境依赖，同时提供一种更加轻量的虚拟化技术，Docker出现了 Docker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理 docker-ce，每季度发布stable版本。18.06，18.09，19.03 发展至今，docker已经通过制定OCI标准对最初的项目做了拆分，其中runC和containerd是docker的核心项目，理解docker整个请求的流程，对我们深入理解docker有很大的帮助 安装 配置宿主机网卡转发 ## 配置网卡转发,看值是否为1 $ sysctl -a |grep -w net.ipv4.ip_forward net.ipv4.ip_forward = 1 ## 若未配置，需要执行如下 $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/docker.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 EOF $ sysctl -p /etc/sysctl.d/docker.conf Yum安装配置docker ## 下载阿里源repo文件 $ curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo $ curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo $ yum clean all \u0026amp;\u0026amp; yum makecache ## yum安装 $ yum install -y docker-ce ## 查看源中可用版本 $ yum list docker-ce --showduplicates | sort -r ## 安装指定版本 ##yum install -y docker-ce-18.09.9 ## 配置源加速 ## https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors mkdir -p /etc/docker vi /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34; : [ \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://ot2k4d59.mirror.aliyuncs.com/\u0026#34; ] } ## 设置开机自启 systemctl enable docker systemctl daemon-reload ## 启动docker systemctl start docker ## 查看docker信息 docker info ## docker-client which docker ## docker daemon ps aux |grep docker 核心要素及常用操作详解 三大核心要素：镜像(Image)、容器(Container)、仓库(Registry)\n（先整体看下流程，再逐个演示）\n镜像（Image） 打包了业务代码及运行环境的包，是静态的文件，不能直接对外提供服务。\n容器（Container） 镜像的运行时，可以对外提供服务。本质上讲是利用namespace和cgroup等技术在宿主机中创建的独立的虚拟空间。\n仓库（Registry） 公有仓库，Docker Hub，阿里，网易\u0026hellip; 私有仓库，企业内部搭建 Docker Registry，Docker官方提供的镜像仓库存储服务 Harbor, 是Docker Registry的更高级封装，它除了提供友好的Web UI界面，角色和用户权限管理，用户操作审计等功能 镜像访问地址形式 registry.devops.com/demo/hello:latest,若没有前面的url地址，则默认寻找Docker Hub中的镜像，若没有tag标签，则使用latest作为标签 公有的仓库中，一般存在这么几类镜像 操作系统基础镜像（centos，ubuntu，suse，alpine） 中间件（nginx，redis，mysql，tomcat） 语言编译环境（python，java，golang） 业务镜像（django-demo\u0026hellip;） 操作演示 解压离线包\n为了保证镜像下载的速度，因此提前在一台节点下载了离线镜像包，做解压：\n$ tar zxf registry.tar.gz -C /opt $ ll /opt/registry-data total 25732 drwxr-xr-x 3 root root 4096 Apr 9 20:11 registry -rw------- 1 root root 26344448 Apr 9 22:15 registry-v2.tar 查看所有镜像：\n$ docker images 拉取镜像: $ docker pull nginx:alpine 如何唯一确定镜像: image_id repository:tag $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx alpine 377c0837328f 2 weeks ago 19.7MB 导出镜像到文件中\n$ docker save -o nginx-alpine.tar nginx:alpine\n5. 从文件中加载镜像\r```powershell\r$ docker load -i nginx-alpine.tar 部署镜像仓库\nhttps://docs.docker.com/registry/\n## 使用docker镜像启动镜像仓库服务 $ docker run -d -p 5000:5000 --restart always -v /opt/registry-data/registry:/var/lib/registry --name registry registry:2 ## 默认仓库不带认证，若需要认证，参考https://docs.docker.com/registry/deploying/#restricting-access 假设启动镜像仓库服务的主机地址为172.21.32.6，该目录中已存在的镜像列表：\n现镜像仓库地址 原镜像仓库地址 172.21.32.6:5000/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 172.21.32.6:5000/mysql:5.7 mysql:5.7 172.21.32.6:5000/nginx:alpine nginx:alpine 172.21.32.6:5000/centos:centos7.5.1804 centos:centos7.5.1804 172.21.32.6:5000/elasticsearch/elasticsearch:7.4.2 docker.elastic.co/elasticsearch/elasticsearch:7.4.2 172.21.32.6:5000/fluentd-es-root:v1.6.2-1.0 gcr.io/google_containers/fluentd-elasticsearch:v2.4.0 172.21.32.6:5000/kibana/kibana:7.4.2 docker.elastic.co/kibana/kibana:7.4.2 172.21.32.6:5000/kubernetesui/dashboard:v2.0.0-beta5 kubernetesui/dashboard:v2.0.0-beta5 172.21.32.6:5000/kubernetesui/metrics-scraper:v1.0.1 kubernetesui/metrics-scraper:v1.0.1 172.21.32.6:5000/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 推送本地镜像到镜像仓库中\n$ docker tag nginx:alpine localhost:5000/nginx:alpine $ docker push localhost:5000/nginx:alpine ## 我的镜像仓库给外部访问，不能通过localhost，尝试使用内网地址172.21.16.3:5000/nginx:alpine $ docker tag nginx:alpine 172.21.16.3:5000/nginx:alpine $ docker push 172.21.16.3:5000/nginx:alpine The push refers to repository [172.21.16.3:5000/nginx] Get https://172.21.16.3:5000/v2/: http: server gave HTTP response to HTTPS client ## docker默认不允许向http的仓库地址推送，如何做成https的，参考：https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry ## 我们没有可信证书机构颁发的证书和域名，自签名证书需要在每个节点中拷贝证书文件，比较麻烦，因此我们通过配置daemon的方式，来跳过证书的验证： $ cat /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34; ], \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;172.21.16.3:5000\u0026#34; ] } $ systemctl restart docker $ docker push 172.21.16.3:5000/nginx:alpine $ docker images\t# IMAGE ID相同，等于起别名或者加快捷方式 REPOSITORY TAG IMAGE ID CREATED SIZE 172.21.16.3:5000/nginx alpine 377c0837328f 4 weeks ago nginx alpine 377c0837328f 4 weeks ago localhost:5000/nginx alpine 377c0837328f 4 weeks ago registry 2 708bc6af7e5e 2 months ago 删除镜像\ndocker rmi nginx:alpine 查看容器列表\n## 查看运行状态的容器列表 $ docker ps ## 查看全部状态的容器列表 $ docker ps -a 启动容器\n## 后台启动 $ docker run --name nginx -d nginx:alpine ##查看run流程# ##查看容器进程 ## 等同于在虚拟机中开辟了一块隔离的独立的虚拟空间 ## 启动容器的同时进入容器，-ti与/bin/sh或者/bin/bash配套使用，意思未分配一个tty终端 $ docker run --name nginx -ti nginx:alpine /bin/sh （注意：退出容器后，该容器会变成退出状态，因为容器内部的1号进程退出） ## 实际上，在运行容器的时候，镜像地址后面跟的命令等于是覆盖了原有的容器的CMD命令，因此，执行的这些命令在容器内部就是1号进程，若该进程不存在了，那么容器就会处于退出的状态，比如，宿主机中执行 1. echo 1,执行完后，该命令立马就结束了 2. ping www.baidu.com,执行完后，命令的进程会持续运行 $ docker run -d --name test_echo nginx:alpine echo 1,容器会立马退出 $ docker run -d --name test_ping nginx:alpine ping www.baidu.com,容器不会退出，但是因为没有加-d参数，因此一直在前台运行，若ctrl+C终止，则容器退出，因为1号进程被终止了 ## 映射端口,把容器的端口映射到宿主机中,-p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; $ docker run --name nginx -d -p 8080:80 nginx:alpine ## 资源限制,-cpuset-cpus用于设置容器可以使用的 vCPU 核。-c,--cpu-shares用于设置多个容器竞争 CPU 时，各个容器相对能分配到的 CPU 时间比例。假设有三个正在运行的容器，这三个容器中的任务都是 CPU 密集型的。第一个容器的 cpu 共享权值是 1024，其它两个容器的 cpu 共享权值是 512。第一个容器将得到 50% 的 CPU 时间，而其它两个容器就只能各得到 25% 的 CPU 时间了。如果再添加第四个 cpu 共享值为 1024 的容器，每个容器得到的 CPU 时间将重新计算。第一个容器的CPU 时间变为 33%，其它容器分得的 CPU 时间分别为 16.5%、16.5%、33%。必须注意的是，这个比例只有在 CPU 密集型的任务执行时才有用。在四核的系统上，假设有四个单进程的容器，它们都能各自使用一个核的 100% CPU 时间，不管它们的 cpu 共享权值是多少。 $ docker run --cpuset-cpus=\u0026#34;0-3\u0026#34; --cpu-shares=512 --memory=500m nginx:alpine 容器数据持久化\n## 挂载主机目录 $ docker run --name nginx -d -v /opt:/opt -v /var/log:/var/log nginx:alpine $ docker run --name mysql -e MYSQL_ROOT_PASSWORD=123456 -d -v /opt/mysql/:/var/lib/mysql mysql:5.7 ## 使用volumes卷 $ docker volume ls $ docker volume create my-vol $ docker run --name nginx -d -v my-vol:/opt/my-vol nginx:alpine $ docker exec -ti nginx touch /opt/my-vol/a.txt ## 验证数据共享 $ docker run --name nginx2 -d -v my-vol:/opt/hh nginx:alpine $ docker exec -ti nginx2 ls /opt/hh/ a.txt 进入容器或者执行容器内的命令\n$ docker exec -ti \u0026lt;container_id_or_name\u0026gt; /bin/sh $ docker exec -ti \u0026lt;container_id_or_name\u0026gt; hostname 主机与容器之间拷贝数据\n## 主机拷贝到容器 $ echo \u0026#39;123\u0026#39;\u0026gt;/tmp/test.txt $ docker cp /tmp/test.txt nginx:/tmp $ docker exec -ti nginx cat /tmp/test.txt 123 ## 容器拷贝到主机 $ docker cp nginx:/tmp/test.txt ./ 查看容器日志\n## 查看全部日志 $ docker logs nginx ## 实时查看最新日志 $ docker logs -f nginx ## 从最新的100条开始查看 $ docker logs --tail=100 -f nginx 停止或者删除容器\n## 停止运行中的容器 $ docker stop nginx ## 启动退出容器 $ docker start nginx ## 删除退出容器 $ docker rm nginx ## 删除运行中的容器 $ docker rm -f nginx 查看容器或者镜像的明细\n## 查看容器详细信息，包括容器IP地址等 $ docker inspect nginx ## 查看镜像的明细信息 $ docker inspect nginx:alpine Django应用容器化实践 django项目介绍 项目地址：https://gitee.com/agagin/python-demo.git\npython3 + uwsgi + nginx + mysql\n内部服务端口8002\n构建命令 $ docker build . -t ImageName:ImageTag -f Dockerfile 如何理解构建镜像的过程？\nDockerfile是一堆指令，在docker build的时候，按照该指令进行操作，最终生成我们期望的镜像\nFROM 指定基础镜像，必须为第一个命令\n格式：\rFROM \u0026lt;image\u0026gt;\rFROM \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt;\r示例：\rFROM mysql:5.7\r注意：\rtag是可选的，如果不使用tag时，会使用latest版本的基础镜像 MAINTAINER 镜像维护者的信息\n格式：\rMAINTAINER \u0026lt;name\u0026gt;\r示例：\rMAINTAINER Yongxin Li\rMAINTAINER inspur_lyx@hotmail.com\rMAINTAINER Yongxin Li \u0026lt;inspur_lyx@hotmail.com\u0026gt; COPY|ADD 添加本地文件到镜像中\n格式：\rCOPY \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt;\r示例：\rADD hom* /mydir/ # 添加所有以\u0026#34;hom\u0026#34;开头的文件\rADD test relativeDir/ # 添加 \u0026#34;test\u0026#34; 到 `WORKDIR`/relativeDir/\rADD test /absoluteDir/ # 添加 \u0026#34;test\u0026#34; 到 /absoluteDir/ WORKDIR 工作目录\n格式：\rWORKDIR /path/to/workdir\r示例：\rWORKDIR /a (这时工作目录为/a)\r注意：\r通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行 RUN 构建镜像过程中执行命令\n格式：\rRUN \u0026lt;command\u0026gt;\r示例：\rRUN yum install nginx\rRUN pip install django\rRUN mkdir test \u0026amp;\u0026amp; rm -rf /var/lib/unusedfiles\r注意：\rRUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache CMD 构建容器后调用，也就是在容器启动时才进行调用\n格式：\rCMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (执行可执行文件，优先)\rCMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数)\rCMD command param1 param2 (执行shell内部命令)\r示例：\rCMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]\rCMD ping www.baidu.com\r注意：\rCMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 ENTRYPOINT 设置容器初始化命令，使其可执行化\n格式：\rENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] (可执行文件, 优先)\rENTRYPOINT command param1 param2 (shell内部命令)\r示例：\rENTRYPOINT [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]\r注意：\rENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令 ENV\n格式：\rENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt;\rENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;\r示例：\rENV myName John\rENV myCat=fluffy EXPOSE\n格式：\rEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...]\r示例：\rEXPOSE 80 443\rEXPOSE 8080\rEXPOSE 11211/tcp 11211/udp\r注意：\rEXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口 Dockerfile dockerfiles/myblog/Dockerfile\n# This my first django Dockerfile # Version 1.0 # Base images 基础镜像 FROM centos:centos7.5.1804 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #ENV 设置环境变量 ENV LANG en_US.UTF-8 ENV LC_ALL en_US.UTF-8 #RUN 执行以下命令 RUN curl -so /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo RUN yum install -y python36 python3-devel gcc pcre-devel zlib-devel make net-tools #工作目录 WORKDIR /opt/myblog #拷贝文件至工作目录 COPY . . #安装nginx RUN tar -zxf nginx-1.13.7.tar.gz -C /opt \u0026amp;\u0026amp; cd /opt/nginx-1.13.7 \u0026amp;\u0026amp; ./configure --prefix=/usr/local/nginx \\ \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx RUN cp myblog.conf /usr/local/nginx/conf/myblog.conf #安装依赖的插件 RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt RUN chmod +x run.sh \u0026amp;\u0026amp; rm -rf ~/.cache/pip #EXPOSE 映射端口 EXPOSE 8002 #容器启动时执行命令 CMD [\u0026#34;./run.sh\u0026#34;] 执行构建：\n$ docker build . -t myblog:v1 -f Dockerfile 定制化基础镜像 dockerfiles/myblog/Dockerfile-base\n# Base images 基础镜像 FROM centos:centos7.5.1804 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #ENV 设置环境变量 ENV LANG en_US.UTF-8 ENV LC_ALL en_US.UTF-8 #RUN 执行以下命令 RUN curl -so /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo RUN yum install -y python36 python3-devel gcc pcre-devel zlib-devel make net-tools COPY nginx-1.13.7.tar.gz /opt #安装nginx RUN tar -zxf /opt/nginx-1.13.7.tar.gz -C /opt \u0026amp;\u0026amp; cd /opt/nginx-1.13.7 \u0026amp;\u0026amp; ./configure --prefix=/usr/local/nginx \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx ## 构建基础镜像 $ docker build . -t centos-python3-nginx:v1 -f Dockerfile-base $ docker tag centos-python3-nginx:v1 172.21.32.6:5000/base/centos-python3-nginx:v1 $ docker push 172.21.32.6:5000/base/centos-python3-nginx:v1 简化Dockerfile dockerfiles/myblog/Dockerfile-optimized\n# This my first django Dockerfile # Version 1.0 # Base images 基础镜像 FROM centos-python3-nginx:v1 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #工作目录 WORKDIR /opt/myblog #拷贝文件至工作目录 COPY . . RUN cp myblog.conf /usr/local/nginx/conf/myblog.conf #安装依赖的插件 RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt RUN chmod +x run.sh \u0026amp;\u0026amp; rm -rf ~/.cache/pip #EXPOSE 映射端口 EXPOSE 8002 #容器启动时执行命令 CMD [\u0026#34;./run.sh\u0026#34;] $ docker build . -t myblog -f Dockerfile-optimized 运行mysql $ docker run -d -p 3306:3306 --name mysql -v /opt/mysql/mysql-data/:/var/lib/mysql -e MYSQL_DATABASE=myblog -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 ## 查看数据库 $ docker exec -ti mysql bash #/ mysql -uroot -p123456 #/ show databases; ## navicator连接 启动Django应用 ## 启动容器 $ docker run -d -p 8002:8002 --name myblog -e MYSQL_HOST=172.21.32.6 -e MYSQL_USER=root -e MYSQL_PASSWD=123456 myblog ## migrate $ docker exec -ti myblog bash #/ python3 manage.py makemigrations #/ python3 manage.py migrate #/ python3 manage.py createsuperuser ## 创建超级用户 $ docker exec -ti myblog python3 manage.py createsuperuser ## 收集静态文件 ## $ docker exec -ti myblog python3 manage.py collectstatic 访问62.234.214.206:8002/admin\n构建镜像，替换默认编码：\ndockerfiles/mysql/my.cnf\n$ cat my.cnf [mysqld] user=root character-set-server=utf8 lower_case_table_names=1 [client] default-character-set=utf8 [mysql] default-character-set=utf8 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ dockerfiles/mysql/Dockerfile\nFROM mysql:5.7 COPY my.cnf /etc/mysql/my.cnf ## CMD或者ENTRYPOINT默认继承 $ docker build . -t mysql:5.7-utf8 $ docker tag mysql:5.7-utf8 172.21.16.3:5000/mysql:5.7-utf8 $ docker push 172.21.16.3:5000/mysql:5.7-utf8 ## 删除旧的mysql容器，使用新镜像启动,不用再次初始化 $ docker rm -f mysql $ rm -rf /opt/mysql/mysql-data/* $ docker run -d -p 3306:3306 --name mysql -v /opt/mysql/mysql-data/:/var/lib/mysql -e MYSQL_DATABASE=myblog -e MYSQL_ROOT_PASSWORD=123456 172.21.32.6:5000/mysql:5.7-utf8 ## 重新migrate $ docker exec -ti myblog bash #/ python3 manage.py makemigrations #/ python3 manage.py migrate #/ python3 manage.py createsuperuser 实现原理 录屏！！！ 虚拟化核心需要解决的问题：资源隔离与资源限制\n虚拟机硬件虚拟化技术， 通过一个 hypervisor 层实现对资源的彻底隔离。 容器则是操作系统级别的虚拟化，利用的是内核的 Cgroup 和 Namespace 特性，此功能完全通过软件实现。 Namespace 资源隔离 命名空间是全局资源的一种抽象，将资源放到不同的命名空间中，各个命名空间中的资源是相互隔离的。 通俗来讲，就是docker在启动一个容器的时候，会调用Linux Kernel Namespace的接口，来创建一块虚拟空间，创建的时候，可以支持设置下面这几种（可以随意选择）,docker默认都设置。\npid：用于进程隔离（PID：进程ID） net：管理网络接口（NET：网络） ipc：管理对 IPC 资源的访问（IPC：进程间通信（信号量、消息队列和共享内存）） mnt：管理文件系统挂载点（MNT：挂载） uts：隔离主机名和域名 user：隔离用户和用户组（3.8以后的内核才支持） func setNamespaces(daemon *Daemon, s *specs.Spec, c *container.Container) error { // user // network // ipc // uts // pid if c.HostConfig.PidMode.IsContainer() { ns := specs.LinuxNamespace{Type: \u0026#34;pid\u0026#34;} pc, err := daemon.getPidContainer(c) if err != nil { return err } ns.Path = fmt.Sprintf(\u0026#34;/proc/%d/ns/pid\u0026#34;, pc.State.GetPID()) setNamespace(s, ns) } else if c.HostConfig.PidMode.IsHost() { oci.RemoveNamespace(s, specs.LinuxNamespaceType(\u0026#34;pid\u0026#34;)) } else { ns := specs.LinuxNamespace{Type: \u0026#34;pid\u0026#34;} setNamespace(s, ns) } return nil } CGroup 资源限制 通过namespace可以保证容器之间的隔离，但是无法控制每个容器可以占用多少资源， 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题。\nControl Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。每一个 CGroup 都是一组被相同的标准和参数限制的进程。而我们需要做的，其实就是把容器这个进程加入到指定的Cgroup中。深入理解CGroup，请点此。\nUnionFS 联合文件系统 Linux namespace和cgroup分别解决了容器的资源隔离与资源限制，那么容器是很轻量的，通常每台机器中可以运行几十上百个容器， 这些个容器是共用一个image，还是各自将这个image复制了一份，然后各自独立运行呢？ 如果每个容器之间都是全量的文件系统拷贝，那么会导致至少如下问题：\n运行容器的速度会变慢 容器和镜像对宿主机的磁盘空间的压力 怎么解决这个问题\u0026mdash;\u0026mdash;Docker的存储驱动\n镜像分层存储 UnionFS Docker 镜像是由一系列的层组成的，每层代表 Dockerfile 中的一条指令，比如下面的 Dockerfile 文件：\nFROM ubuntu:15.04 COPY . /app RUN make /app CMD python /app/app.py 这里的 Dockerfile 包含4条命令，其中每一行就创建了一层，下面显示了上述Dockerfile构建出来的镜像运行的容器层的结构：\n镜像就是由这些层一层一层堆叠起来的，镜像中的这些层都是只读的，当我们运行容器的时候，就可以在这些基础层至上添加新的可写层，也就是我们通常说的容器层，对于运行中的容器所做的所有更改（比如写入新文件、修改现有文件、删除文件）都将写入这个容器层。\n对容器层的操作，主要利用了写时复制（CoW）技术。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。 CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。\n镜像中每一层的文件都是分散在不同的目录中的，如何把这些不同目录的文件整合到一起呢？\nUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统联合到同一个挂载点的文件系统服务。 它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，整个联合的过程被称为联合挂载（Union Mount）。\n上图是AUFS的实现，AUFS是作为Docker存储驱动的一种实现，Docker 还支持了不同的存储驱动，包括 aufs、devicemapper、overlay2、zfs 和 Btrfs 等等，在最新的 Docker 中，overlay2 取代了 aufs 成为了推荐的存储驱动，但是在没有 overlay2 驱动的机器上仍然会使用 aufs 作为 Docker 的默认驱动。\nDocker网络 录屏！！！ docker容器是一块具有隔离性的虚拟系统，容器内可以有自己独立的网络空间，\n多个容器之间是如何实现通信的呢？ 容器和宿主机之间又是如何实现的通信呢？ 使用-p参数是怎么实现的端口映射? 带着我们就这些问题，我们来学习一下docker的网络模型，最后我会通过抓包的方式，给大家演示一下数据包在容器和宿主机之间的转换过程。\n网络模式 我们在使用docker run创建Docker容器时，可以用\u0026ndash;net选项指定容器的网络模式，Docker有以下4种网络模式：\nbridge模式，使用\u0026ndash;net=bridge指定，默认设置\nhost模式，使用\u0026ndash;net=host指定，容器内部网络空间共享宿主机的空间，效果类似直接在宿主机上启动一个进程，端口信息和宿主机共用。\ncontainer模式，使用\u0026ndash;net=container:NAME_or_ID指定\n指定容器与特定容器共享网络命名空间\nnone模式，使用\u0026ndash;net=none指定\n网络模式为空，即仅保留网络命名空间，但是不做任何网络相关的配置(网卡、IP、路由等)\nbridge模式 那我们之前在演示创建docker容器的时候其实是没有指定的网络模式的，如果不指定的话默认就会使用bridge模式，bridge本意是桥的意思，其实就是网桥模式，那我们怎么理解网桥，如果需要做类比的话，我们可以把网桥看成一个二层的交换机设备，我们来看下这张图：\n交换机通信简图\n网桥模式示意图\n网桥在哪，查看网桥\n$ yum install -y bridge-utils $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242b5fbe57b no veth3a496ed 有了网桥之后，那我们看下docker在启动一个容器的时候做了哪些事情才能实现容器间的互联互通\nDocker 创建一个容器的时候，会执行如下操作：\n创建一对虚拟接口/网卡，也就是veth pair； 本地主机一端桥接 到默认的 docker0 或指定网桥上，并具有一个唯一的名字，如 veth9953b75； 容器一端放到新启动的容器内部，并修改名字作为 eth0，这个网卡/接口只在容器的命名空间可见； 从网桥可用地址段中（也就是与该bridge对应的network）获取一个空闲地址分配给容器的 eth0 配置默认路由到网桥 那整个过程其实是docker自动帮我们完成的，清理掉所有容器，来验证。\n## 清掉所有容器 $ docker rm -f `docker ps -aq` $ docker ps $ brctl show # 查看网桥中的接口，目前没有 ## 创建测试容器test1 $ docker run -d --name test1 nginx:alpine $ brctl show # 查看网桥中的接口，已经把test1的veth端接入到网桥中 $ ip a |grep veth # 已在宿主机中可以查看到 $ docker exec -ti test1 sh / # ifconfig # 查看容器的eth0网卡及分配的容器ip / # route -n # 观察默认网关都指向了网桥的地址，即所有流量都转向网桥，等于是在veth pair接通了网线 Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 # 再来启动一个测试容器，测试容器间的通信 $ docker run -d --name test2 nginx:alpine $ docker exec -ti test sh / # sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories / # apk add curl / # curl 172.17.0.8:80 ## 为啥可以通信，因为两个容器是接在同一个网桥中的，通信其实是通过mac地址和端口的的记录来做转发的。test1访问test2，通过test1的eth0发送ARP广播，网桥会维护一份mac映射表，我们可以大概通过命令来看一下， $ brctl showmacs docker0 ## 这些mac地址是主机端的veth网卡对应的mac，可以查看一下 $ ip a 我们如何知道网桥上的这些虚拟网卡与容器端是如何对应？\n通过ifindex，网卡索引号\n## 查看test1容器的网卡索引 $ docker exec -ti test1 cat /sys/class/net/eth0/ifindex ## 主机中找到虚拟网卡后面这个@ifxx的值，如果是同一个值，说明这个虚拟网卡和这个容器的eth0网卡是配对的。 $ ip a |grep @if 整理脚本，快速查看对应：\nfor container in $(docker ps -q); do iflink=`docker exec -it $container sh -c \u0026#39;cat /sys/class/net/eth0/iflink\u0026#39;` iflink=`echo $iflink|tr -d \u0026#39;\\r\u0026#39;` veth=`grep -l $iflink /sys/class/net/veth*/ifindex` veth=`echo $veth|sed -e \u0026#39;s;^.*net/\\(.*\\)/ifindex$;\\1;\u0026#39;` echo $container:$veth done 上面我们讲解了容器之间的通信，那么容器与宿主机的通信是如何做的？\n添加端口映射：\n## 启动容器的时候通过-p参数添加宿主机端口与容器内部服务端口的映射 $ docker run --name test -d -p 8088:80 nginx:alpine $ curl localhost:8088 端口映射如何实现的？先来回顾iptables链表图\n访问本机的8088端口，数据包会从流入方向进入本机，因此涉及到PREROUTING和INPUT链，我们是通过做宿主机与容器之间加的端口映射，所以肯定会涉及到端口转换，那哪个表是负责存储端口转换信息的呢，就是nat表，负责维护网络地址转换信息的。因此我们来查看一下PREROUTING链的nat表：\n$ iptables -t nat -nvL PREROUTING Chain PREROUTING (policy ACCEPT 159 packets, 20790 bytes) pkts bytes target prot opt in out source destination 3 156 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL 规则利用了iptables的addrtype拓展，匹配网络类型为本地的包，如何确定哪些是匹配本地，\n$ ip route show table local type local local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 local 192.168.136.133 dev ens33 proto kernel scope host src 192.168.136.133 也就是说目标地址类型匹配到这些的，会转发到我们的TARGET中，TARGET是动作，意味着对符合要求的数据包执行什么样的操作，最常见的为ACCEPT或者DROP，此处的TARGET为DOCKER，很明显DOCKER不是标准的动作，那DOCKER是什么呢？我们通常会定义自定义的链，这样把某类对应的规则放在自定义链中，然后把自定义的链绑定到标准的链路中，因此此处DOCKER 是自定义的链。那我们现在就来看一下DOCKER这个自定义链上的规则。\n$ iptables -t nat -nvL DOCKER Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8088 to:172.17.0.2:80 此条规则就是对主机收到的目的端口为8088的tcp流量进行DNAT转换，将流量发往172.17.0.2:80，172.17.0.2地址是不是就是我们上面创建的Docker容器的ip地址，流量走到网桥上了，后面就走网桥的转发就ok了。 所以，外界只需访问192.168.136.133:8088就可以访问到容器中的服务了。\n数据包在出口方向走POSTROUTING链，我们查看一下规则：\n$ iptables -t nat -nvL POSTROUTING Chain POSTROUTING (policy ACCEPT 1099 packets, 67268 bytes) pkts bytes target prot opt in out source destination 86 5438 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 172.17.0.4 172.17.0.4 tcp dpt:80 大家注意MASQUERADE这个动作是什么意思，其实是一种更灵活的SNAT，把源地址转换成主机的出口ip地址，那解释一下这条规则的意思:\n这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。大概的过程就是ACK的包在容器里面发出来，会路由到网桥docker0，网桥根据宿主机的路由规则会转给宿主机网卡eth0，这时候包就从docker0网卡转到eth0网卡了，并从eth0网卡发出去，这时候这条规则就会生效了，把源地址换成了eth0的ip地址。\n注意一下，刚才这个过程涉及到了网卡间包的传递，那一定要打开主机的ip_forward转发服务，要不然包转不了，服务肯定访问不到。\n抓包演示 我们先想一下，我们要抓哪个网卡的包\n首先访问宿主机的8088端口，我们抓一下宿主机的eth0\n$ tcpdump -i eth0 port 8088 -w host.cap 然后最终包会流入容器内，那我们抓一下容器内的eth0网卡\n# 容器内安装一下tcpdump $ sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories $ apk add tcpdump $ tcpdump -i eth0 port 80 -w container.cap 到另一台机器访问一下，\n$ curl 172.21.32.6:8088/ 停止抓包，拷贝容器内的包到宿主机\n$ docker cp test:/root/container.cap /root/ 把抓到的内容拷贝到本地，使用wireshark进行分析。\n$ scp root@172.21.32.6:/root/*.cap /d/packages （wireshark合并包进行分析）\n进到容器内的包做DNAT，出去的包做SNAT，这样对外面来讲，根本就不知道机器内部是谁提供服务，其实这就和一个内网多个机器公用一个外网IP地址上网的效果是一样的，对吧，那这也属于NAT功能的一个常见的应用场景。\nHost模式 容器内部不会创建网络空间，共享宿主机的网络空间\n$ docker run --net host -d --name mysql mysql:5.7 Conatiner模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。\n## 启动测试容器，共享mysql的网络空间 $ docker run -ti --rm --net=container:mysql busybox sh / # ip a / # netstat -tlp|grep 3306 / # telnet localhost 3306 实用技巧 清理主机上所有退出的容器\n$ docker rm $(docker ps -aq) 调试或者排查容器启动错误\n## 若有时遇到容器启动失败的情况，可以先使用相同的镜像启动一个临时容器，先进入容器 $ docker exec -ti --rm \u0026lt;image_id\u0026gt; bash ## 进入容器后，手动执行该容器对应的ENTRYPOINT或者CMD命令，这样即使出错，容器也不会退出，因为bash作为1号进程，我们只要不退出容器，该容器就不会自动退出 本章小结 为了解决软件交付过程中的环境依赖，同时提供一种更加轻量的虚拟化技术，Docker出现了\n2013年诞生，15年开始迅速发展，从17.03月开始，使用时间日期管理版本，稳定版以每季度为准\nDocker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理\n使用yum部署docker，启动后通过操作docker这个命令行，自动调用docker daemon完成容器相关操作\n常用操作\nsystemctl start|stop|restart docker docker build | pull -\u0026gt; docker tag -\u0026gt; docker push docker run \u0026ndash;name my-demo -d -p 8080:80 -v /opt/data:/data demo:v20200327 docker cp /path/a.txt mycontainer:/opt docker exec -ti mycontainer /bin/sh docker logs -f mycontainer 通过dockerfile构建业务镜像，先使用基础镜像，然后通过一系列的指令把我们的业务应用所需要的运行环境和依赖都打包到镜像中，然后通过CMD或者ENTRYPOINT指令把镜像启动时的入口制定好，完成封装即可。有点类似于，先找来一个空的集装箱(基础镜像)，然后把项目依赖的服务都扔到集装箱中，然后设置好服务的启动入口，关闭箱门，即完成了业务镜像的制作。\n容器的实现依赖于内核模块提供的namespace和control-group的功能，通过namespace创建一块虚拟空间，空间内实现了各类资源(进程、网络、文件系统)的隔离，提供control-group实现了对隔离的空间的资源使用的限制。\ndocker镜像使用分层的方式进行存储，根据主机的存储驱动的不同，实现方式会不同，kernel在3.10.0-514以上自动支持overlay2 存储驱动，也是目前Docker推荐的方式。\n得益于分层存储的模式，多个容器可以通过copy-on-write的策略，在镜像的最上层加一个可写层，实现一个镜像快速启动多个容器的场景\ndocker的网络模式分为4种，最常用的为bridge和host模式。bridge模式通过docker0网桥，启动容器的时候通过创建一对虚拟网卡，将容器连接在桥上，同时维护了虚拟网卡与网桥端口的关系，实现容器间的通信。容器与宿主机之间的通信通过iptables端口映射的方式，docker利用iptables的PREROUTING和POSTROUTING的nat功能，实现了SNAT与DNAT，使得容器内部的服务被完美的保护起来。\n第二章 Kubernetes实践之旅 录屏！！！ 本章学习kubernetes的架构及工作流程，重点介绍如何使用Deployment管理Pod生命周期，实现服务不中断的滚动更新，通过服务发现来实现集群内部的服务间访问，并通过ingress-nginx实现外部使用域名访问集群内部的服务。同时介绍基于EFK如何搭建Kubernetes集群的日志收集系统。\n学完本章，我们的Django demo项目已经可以运行在k8s集群中，同时我们可以使用域名进行服务的访问。\n架构及核心组件介绍 使用kubeadm快速搭建集群 运行第一个Pod应用 Pod进阶 Pod控制器的使用 实现服务与Node绑定的几种方式 负载均衡与服务发现 使用Ingress实现集群服务的7层代理 Django项目k8s落地实践 基于EFK实现kubernetes集群的日志平台（扩展） 集群认证与授权 纯容器模式的问题 业务容器数量庞大，哪些容器部署在哪些节点，使用了哪些端口，如何记录、管理，需要登录到每台机器去管理？ 跨主机通信，多个机器中的容器之间相互调用如何做，iptables规则手动维护？ 跨主机容器间互相调用，配置如何写？写死固定IP+端口？ 如何实现业务高可用？多个容器对外提供服务如何实现负载均衡？ 容器的业务中断了，如何可以感知到，感知到以后，如何自动启动新的容器? 如何实现滚动升级保证业务的连续性？ \u0026hellip;\u0026hellip; 容器调度管理平台 Docker Swarm Mesos Google Kubernetes\n2017年开始Kubernetes凭借强大的容器集群管理功能, 逐步占据市场,目前在容器编排领域一枝独秀\nhttps://kubernetes.io/\n架构图 区分组件与资源\n核心组件 ETCD：分布式高性能键值数据库,存储整个集群的所有元数据\nApiServer: API服务器,集群资源访问控制入口,提供restAPI及安全访问控制\nScheduler：调度器,负责把业务容器调度到最合适的Node节点\nController Manager：控制器管理,确保集群资源按照期望的方式运行\nReplication Controller Node controller ResourceQuota Controller Namespace Controller ServiceAccount Controller Tocken Controller Service Controller Endpoints Controller kubelet：运行在每运行在每个节点上的主要的“节点代理”个节点上的主要的“节点代理”\npod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理. 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要 kubectl: 命令行接口，用于对 Kubernetes 集群运行命令 https://kubernetes.io/zh/docs/reference/kubectl/\nCNI实现: 通用网络接口, 我们使用flannel来作为k8s集群的网络插件, 实现跨节点通信\n工作流程 用户准备一个资源文件（记录了业务应用的名称、镜像地址等信息），通过调用APIServer执行创建Pod APIServer收到用户的Pod创建请求，将Pod信息写入到etcd中 调度器通过list-watch的方式，发现有新的pod数据，但是这个pod还没有绑定到某一个节点中 调度器通过调度算法，计算出最适合该pod运行的节点，并调用APIServer，把信息更新到etcd中 kubelet同样通过list-watch方式，发现有新的pod调度到本机的节点了，因此调用容器运行时，去根据pod的描述信息，拉取镜像，启动容器，同时生成事件信息 同时，把容器的信息、事件及状态也通过APIServer写入到etcd中 实践\u0026ndash;集群安装 录屏！！！ kubeadm https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/\n《Kubernetes安装手册（非高可用版）》\n核心组件 静态Pod的方式：\n## etcd、apiserver、controller-manager、kube-scheduler $ kubectl -n kube-system get po systemd服务方式：\n$ systemctl status kubelet kubectl：二进制命令行工具\n理解集群资源 组件是为了支撑k8s平台的运行，安装好的软件。\n资源是如何去使用k8s的能力的定义。比如，k8s可以使用Pod来管理业务应用，那么Pod就是k8s集群中的一类资源，集群中的所有资源可以提供如下方式查看：\n$ kubectl api-resources 如何理解namespace：\n命名空间，集群内一个虚拟的概念，类似于资源池的概念，一个池子里可以有各种资源类型，绝大多数的资源都必须属于某一个namespace。集群初始化安装好之后，会默认有如下几个namespace：\n$ kubectl get namespaces NAME STATUS AGE default Active 84m kube-node-lease Active 84m kube-public Active 84m kube-system Active 84m kubernetes-dashboard Active 71m 所有NAMESPACED的资源，在创建的时候都需要指定namespace，若不指定，默认会在default命名空间下 相同namespace下的同类资源不可以重名，不同类型的资源可以重名 不同namespace下的同类资源可以重名 通常在项目使用的时候，我们会创建带有业务含义的namespace来做逻辑上的整合 kubectl的使用 类似于docker，kubectl是命令行工具，用于与APIServer交互，内置了丰富的子命令，功能极其强大。 https://kubernetes.io/docs/reference/kubectl/overview/\n$ kubectl -h $ kubectl get -h $ kubectl create -h $ kubectl create namespace -h kubectl如何管理集群资源\n$ kubectl get po -v=7 实践\u0026ndash;使用k8s管理业务应用 最小调度单元 Pod 录屏！！！ docker调度的是容器，在k8s集群中，最小的调度单元是Pod（豆荚）\n为什么引入Pod 与容器引擎解耦\nDocker、Rkt。平台设计与引擎的具体的实现解耦\n多容器共享网络|存储|进程 空间, 支持的业务场景更加灵活\n使用yaml格式定义Pod myblog/one-pod/pod.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\t\u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;component\u0026#34;: \u0026#34;myblog\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;172.21.32.6:5000/myblog\u0026#34;, \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;MYSQL_HOST\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;127.0.0.1\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;MYSQL_PASSWD\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;123456\u0026#34; } ], \u0026#34;ports\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: 8002 } ] }, { \u0026#34;name\u0026#34;: \u0026#34;mysql\u0026#34;, ... } ] } } apiVersion 含义 alpha 进入K8s功能的早期候选版本，可能包含Bug，最终不一定进入K8s beta 已经过测试的版本，最终会进入K8s，但功能、对象定义可能会发生变更。 stable 可安全使用的稳定版本 v1 stable 版本之后的首个版本，包含了更多的核心对象 apps/v1 使用最广泛的版本，像Deployment、ReplicaSets都已进入该版本 资源类型与apiVersion对照表\nKind apiVersion ClusterRoleBinding rbac.authorization.k8s.io/v1 ClusterRole rbac.authorization.k8s.io/v1 ConfigMap v1 CronJob batch/v1beta1 DaemonSet extensions/v1beta1 Node v1 Namespace v1 Secret v1 PersistentVolume v1 PersistentVolumeClaim v1 Pod v1 Deployment v1、apps/v1、apps/v1beta1、apps/v1beta2 Service v1 Ingress extensions/v1beta1 ReplicaSet apps/v1、apps/v1beta2 Job batch/v1 StatefulSet apps/v1、apps/v1beta1、apps/v1beta2 快速获得资源和版本\n$ kubectl explain pod $ kubectl explain Pod.apiVersion 创建和访问Pod ## 创建namespace, namespace是逻辑上的资源池 $ kubectl create namespace demo ## 使用指定文件创建Pod $ kubectl create -f demo-pod.yaml ## 查看pod，可以简写po ## 所有的操作都需要指定namespace，如果是在default命名空间下，则可以省略 $ kubectl -n demo get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m 10.244.1.146 k8s-slave1 ## 使用Pod Ip访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/ ## 进入容器,执行初始化, 不必到对应的主机执行docker exec $ kubectl -n demo exec -ti myblog -c myblog bash / # env / # python3 manage.py migrate $ kubectl -n demo exec -ti myblog -c mysql bash / # mysql -p123456 ## 再次访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/ Infra容器 登录k8s-slave1节点\n$ docker ps -a |grep myblog ## 发现有三个容器 ## 其中包含mysql和myblog程序以及Infra容器 ## 为了实现Pod内部的容器可以通过localhost通信，每个Pod都会启动Infra容器，然后Pod内部的其他容器的网络空间会共享该Infra容器的网络空间(Docker网络的container模式)，Infra容器只需要hang住网络空间，不需要额外的功能，因此资源消耗极低。 ## 登录master节点，查看pod内部的容器ip均相同，为pod ip $ kubectl -n demo exec -ti myblog -c myblog bash / # ifconfig $ kubectl -n demo exec -ti myblog -c mysql bash / # ifconfig pod容器命名: k8s_\u0026lt;container_name\u0026gt;_\u0026lt;pod_name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;random_string\u0026gt;\n查看pod详细信息 ## 查看pod调度节点及pod_ip $ kubectl -n demo get pods -o wide ## 查看完整的yaml $ kubectl -n demo get po myblog -o yaml ## 查看pod的明细信息及事件 $ kubectl -n demo describe pod myblog Troubleshooting and Debugging #进入Pod内的容器 $ kubectl -n \u0026lt;namespace\u0026gt; exec \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; -ti /bin/sh #查看Pod内容器日志,显示标准或者错误输出日志 $ kubectl -n \u0026lt;namespace\u0026gt; logs -f \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; 更新服务版本 $ kubectl apply -f demo-pod.yaml 删除Pod服务 #根据文件删除 $ kubectl delete -f demo-pod.yaml #根据pod_name删除 $ kubectl -n \u0026lt;namespace\u0026gt; delete pod \u0026lt;pod_name\u0026gt; Pod数据持久化 若删除了Pod，由于mysql的数据都在容器内部，会造成数据丢失，因此需要数据进行持久化。\n定点使用hostpath挂载，nodeSelector定点\nmyblog/one-pod/pod-with-volume.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; volumeMounts: - name: mysql-data mountPath: /var/lib/mysql 保存文件为pod-with-volume.yaml，执行创建\n## 若存在旧的同名服务，先删除掉，后创建 $ kubectl -n demo delete pod myblog ## 创建 $ kubectl create -f pod-with-volume.yaml ## 此时pod状态Pending $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE myblog 0/2 Pending 0 32s ## 查看原因，提示调度失败，因为节点不满足node selector $ kubectl -n demo describe po myblog Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 12s (x2 over 12s) default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector. ## 为节点打标签 $ kubectl label node k8s-slave1 component=mysql ## 再次查看，已经运行成功 $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m54s 10.244.1.150 k8s-slave1 ## 到k8s-slave1节点，查看/opt/mysql/data $ ll /opt/mysql/data/ total 188484 -rw-r----- 1 polkitd input 56 Mar 29 09:20 auto.cnf -rw------- 1 polkitd input 1676 Mar 29 09:20 ca-key.pem -rw-r--r-- 1 polkitd input 1112 Mar 29 09:20 ca.pem drwxr-x--- 2 polkitd input 8192 Mar 29 09:20 sys ... ## 执行migrate，创建数据库表，然后删掉pod，再次创建后验证数据是否存在 $ kubectl -n demo exec -ti myblog python3 manage.py migrate ## 访问服务，正常 $ curl 10.244.1.150:8002/blog/index/ ## 删除pod $ kubectl delete -f pod-with-volume.yaml ## 再次创建Pod $ kubectl create -f pod-with-volume.yaml ## 查看pod ip并访问服务 $ kubectl -n demo get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 7s 10.244.1.151 k8s-slave1 ## 未做migrate，服务正常 $ curl 10.244.1.151:8002/blog/index/ 使用PV+PVC连接分布式存储解决方案\nceph glusterfs nfs 服务健康检查 检测容器服务是否健康的手段，若不健康，会根据设置的重启策略（restartPolicy）进行操作，两种检测机制可以分别单独设置，若不设置，默认认为Pod是健康的。\n两种机制：\nLivenessProbe探针 用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。 ReadinessProbe探针 用于判断容器是否正常提供服务，即容器的Ready是否为True，是否可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。（剔除此pod不参与接收请求不会将流量转发给此Pod）。 三种类型：\nexec：通过执行命令来检查服务是否正常，回值为0则表示容器健康 httpGet方式：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康 tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康 示例：\n完整文件路径 myblog/one-pod/pod-with-healthcheck.yaml\ncontainers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。 periodSeconds：执行探测的频率。默认是10秒，最小1秒。 timeoutSeconds：探测超时时间。默认1秒，最小1秒。 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1，最小值是1。 failureThreshold：探测成功后，最少连续探测失败多少次 才被认定为失败。默认是3，最小值是1。 重启策略：\nPod的重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。 Pod的重启策略包括Always、OnFailure和Never，默认值为Always。\nAlways：当容器失败时，由kubelet自动重启该容器； OnFailure：当容器终止运行且退出码不为0时，有kubelet自动重启该容器； Never：不论容器运行状态如何，kubelet都不会重启该容器。 镜像拉取策略 spec: containers: - name: myblog image: 172.21.32.6:5000/demo/myblog imagePullPolicy: IfNotPresent 设置镜像的拉取策略，默认为IfNotPresent\nAlways，总是拉取镜像，即使本地有镜像也从仓库拉取 IfNotPresent ，本地有则使用本地镜像，本地没有则去仓库拉取 Never，只使用本地镜像，本地没有则报错 Pod资源限制 为了保证充分利用集群资源，且确保重要容器在运行周期内能够分配到足够的资源稳定运行，因此平台需要具备\nPod的资源限制的能力。 对于一个pod来说，资源最基础的2个的指标就是：CPU和内存。\nKubernetes提供了个采用requests和limits 两种类型参数对资源进行预分配和使用限制。\n完整文件路径：myblog/one-pod/pod-with-resourcelimits.yaml\n... containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m ... requests：\n容器使用的最小资源需求,作用于schedule阶段，作为容器调度时资源分配的判断依赖 只有当前节点上可分配的资源量 \u0026gt;= request 时才允许将容器调度到该节点 request参数不限制容器的最大可使用资源 requests.cpu被转成docker的\u0026ndash;cpu-shares参数，与cgroup cpu.shares功能相同 (无论宿主机有多少个cpu或者内核，\u0026ndash;cpu-shares选项都会按照比例分配cpu资源） requests.memory没有对应的docker参数，仅作为k8s调度依据 limits：\n容器能使用资源的最大值 设置为0表示对使用的资源不做限制, 可无限的使用 当pod 内存超过limit时，会被oom 当cpu超过limit时，不会被kill，但是会限制不超过limit值 limits.cpu会被转换成docker的–cpu-quota参数。与cgroup cpu.cfs_quota_us功能相同 limits.memory会被转换成docker的–memory参数。用来限制容器使用的最大内存 对于 CPU，我们知道计算机里 CPU 的资源是按“时间片”的方式来进行分配的，系统里的每一个操作都需要 CPU 的处理，所以，哪个任务要是申请的 CPU 时间片越多，那么它得到的 CPU 资源就越多。\n然后还需要了解下 CGroup 里面对于 CPU 资源的单位换算：\n1 CPU = 1000 millicpu（1 Core = 1000m） 这里的 m 就是毫、毫核的意思，Kubernetes 集群中的每一个节点可以通过操作系统的命令来确认本节点的 CPU 内核数量，然后将这个数量乘以1000，得到的就是节点总 CPU 总毫数。比如一个节点有四核，那么该节点的 CPU 总毫量为 4000m。\ndocker run命令和 CPU 限制相关的所有选项如下：\n选项 描述 --cpuset-cpus=\u0026quot;\u0026quot; 允许使用的 CPU 集，值可以为 0-3,0,1 -c,--cpu-shares=0 CPU 共享权值（相对权重） cpu-period=0 限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000] --cpu-quota=0 限制 CPU CFS 配额，必须不小于1ms，即 \u0026gt;= 1000，绝对限制 docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:16.04 /bin/bash 将 CFS 调度的周期设为 50000，将容器在每个周期内的 CPU 配额设置为 25000，表示该容器每 50ms 可以得到 50% 的 CPU 运行时间。\n注意：若内存使用超出限制，会引发系统的OOM机制，因CPU是可压缩资源，不会引发Pod退出或重建\nyaml优化 目前完善后的yaml，myblog/one-pod/pod-completed.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql 为什么要优化\n考虑真实的使用场景，像数据库这类中间件，是作为公共资源，为多个项目提供服务，不适合和业务容器绑定在同一个Pod中，因为业务容器是经常变更的，而数据库不需要频繁迭代 yaml的环境变量中存在敏感信息（账号、密码），存在安全隐患 解决问题一，需要拆分yaml\nmyblog/two-pod/mysql.yaml\napiVersion: v1 kind: Pod metadata: name: mysql namespace: demo labels: component: mysql spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql myblog.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: containers: - name: myblog image: 172.21.32.6:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;172.21.32.6\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 创建测试\n## 先删除旧pod $ kubectl -n demo delete po myblog ## 分别创建mysql和myblog $ kubectl create -f mysql.yaml $ kubectl create -f myblog.yaml ## 查看pod，注意mysqlIP为宿主机IP，因为网络模式为host $ kubectl -n demo get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 1/1 Running 0 41s 10.244.1.152 k8s-slave1 mysql 1/1 Running 0 52s 192.168.136.131 k8s-slave1 ## 访问myblog服务正常 $ curl 10.244.1.152:8002/blog/index/ 解决问题二，环境变量中敏感信息带来的安全隐患\n为什么要统一管理环境变量\n环境变量中有很多敏感的信息，比如账号密码，直接暴漏在yaml文件中存在安全性问题 团队内部一般存在多个项目，这些项目直接存在配置相同环境变量的情况，因此可以统一维护管理 对于开发、测试、生产环境，由于配置均不同，每套环境部署的时候都要修改yaml，带来额外的开销 k8s提供两类资源，configMap和Secret，可以用来实现业务配置的统一管理， 允许将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性 。\nconfigMap，通常用来管理应用的配置文件或者环境变量，myblog/two-pod/configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: myblog namespace: demo data: MYSQL_HOST: \u0026#34;172.21.32.6\u0026#34; MYSQL_PORT: \u0026#34;3306\u0026#34; Secret，管理敏感类的信息，默认会base64编码存储，有三种类型\nService Account ：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；创建ServiceAccount后，Pod中指定serviceAccount后，自动创建该ServiceAccount对应的secret； Opaque ： base64编码格式的Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有docker registry的认证信息。 myblog/two-pod/secret.yaml\napiVersion: v1 kind: Secret metadata: name: myblog namespace: demo type: Opaque data: MYSQL_USER: cm9vdA==\t#注意加-n参数， echo -n root|base64 MYSQL_PASSWD: MTIzNDU2 创建并查看：\n$ kubectl create -f secret.yaml $ kubectl -n demo get secret 如果不习惯这种方式，可以通过如下方式：\n$ cat secret.txt MYSQL_USER=root MYSQL_PASSWD=123456 $ kubectl -n demo create secret generic myblog --from-env-file=secret.txt 修改后的mysql的yaml，资源路径：myblog/two-pod/mysql-with-config.yaml\n... spec: containers: - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 env: - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; ... 修改后的myblog的yaml，资源路径：myblog/two-pod/myblog-with-config.yaml\nspec: containers: - name: myblog image: 172.21.32.6:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST valueFrom: configMapKeyRef: name: myblog key: MYSQL_HOST - name: MYSQL_PORT valueFrom: configMapKeyRef: name: myblog key: MYSQL_PORT - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD 在部署不同的环境时，pod的yaml无须再变化，只需要在每套环境中维护一套ConfigMap和Secret即可。但是注意configmap和secret不能跨namespace使用，且更新后，pod内的env不会自动更新，重建后方可更新。\n如何编写资源yaml 拿来主义，从机器中已有的资源中拿\n$ kubectl -n kube-system get po,deployment,ds 学会在官网查找， https://kubernetes.io/docs/home/\n从kubernetes-api文档中查找， https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#pod-v1-core\nkubectl explain 查看具体字段含义\npod状态与生命周期 Pod的状态如下表所示：\n状态值 描述 Pending API Server已经创建该Pod，等待调度器调度 ContainerCreating 镜像正在创建 Running Pod内容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态 Succeeded Pod内所有容器均已成功执行退出，且不再重启 Failed Pod内所有容器均已退出，但至少有一个容器退出为失败状态 CrashLoopBackOff Pod内有容器启动失败，比如配置文件丢失导致主进程启动失败 Unknown 由于某种原因无法获取该Pod的状态，可能由于网络通信不畅导致 生命周期示意图：\n启动和关闭示意：\napiVersion: v1 kind: Pod metadata: name: demo-start-stop namespace: demo labels: component: demo-start-stop spec: initContainers: - name: init image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): INIT \u0026gt;\u0026gt; /loap/timing\u0026#39;] volumeMounts: - mountPath: /loap name: timing containers: - name: main image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): START \u0026gt;\u0026gt; /loap/timing; sleep 10; echo $(date +%s): END \u0026gt;\u0026gt; /loap/timing;\u0026#39;] volumeMounts: - mountPath: /loap name: timing livenessProbe: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): LIVENESS \u0026gt;\u0026gt; /loap/timing\u0026#39;] readinessProbe: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): READINESS \u0026gt;\u0026gt; /loap/timing\u0026#39;] lifecycle: postStart: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): POST-START \u0026gt;\u0026gt; /loap/timing\u0026#39;] preStop: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): PRE-STOP \u0026gt;\u0026gt; /loap/timing\u0026#39;] volumes: - name: timing hostPath: path: /tmp/loap 创建pod测试：\n$ kubectl create -f demo-pod-start.yaml ## 查看demo状态 $ kubectl -n demo get po -o wide -w ## 查看调度节点的/tmp/loap/timing $ cat /tmp/loap/timing 1585424708: INIT 1585424746: START 1585424746: POST-START 1585424754: READINESS 1585424756: LIVENESS 1585424756: END 须主动杀掉 Pod 才会触发 pre-stop hook，如果是 Pod 自己 Down 掉，则不会执行 pre-stop hook\n小结 实现k8s平台与特定的容器运行时解耦，提供更加灵活的业务部署方式，引入了Pod概念 k8s使用yaml格式定义资源文件，yaml中Map与List的语法，与json做类比 通过kubectl create | get | exec | logs | delete 等操作k8s资源，必须指定namespace 每启动一个Pod，为了实现网络空间共享，会先创建Infra容器，并把其他容器网络加入该容器 通过livenessProbe和readinessProbe实现Pod的存活性和就绪健康检查 通过requests和limit分别限定容器初始资源申请与最高上限资源申请 通过Pod IP访问具体的Pod服务，实现是 Pod控制器 录屏！！！ 只使用Pod, 将会面临如下需求:\n业务应用启动多个副本 Pod重建后IP会变化，外部如何访问Pod服务 运行业务Pod的某个节点挂了，可以自动帮我把Pod转移到集群中的可用节点启动起来 我的业务应用功能是收集节点监控数据,需要把Pod运行在k8集群的各个节点上 Workload (工作负载) 控制器又称工作负载是用于实现管理pod的中间层，确保pod资源符合预期的状态，pod的资源出现故障时，会尝试 进行重启，当根据重启策略无效，则会重新新建pod的资源。\nReplicaSet: 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能 Deployment：工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置 DaemonSet：用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务 Job：只要完成就立即退出，不需要重启或重建 Cronjob：周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 Deployment myblog/deployment/deploy-mysql.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace: demo spec: replicas: 1\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: mysql template: metadata: labels:\t#给Pod打label app: mysql spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.15:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql deploy-myblog.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: myblog namespace: demo spec: replicas: 1\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: myblog template: metadata: labels:\t#给Pod打label app: myblog spec: containers: - name: myblog image: 172.21.32.15:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST valueFrom: configMapKeyRef: name: myblog key: MYSQL_HOST - name: MYSQL_PORT valueFrom: configMapKeyRef: name: myblog key: MYSQL_PORT - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 创建Deployment $ kubectl create -f deploy.yaml 查看Deployment # kubectl api-resources $ kubectl -n demo get deploy NAME READY UP-TO-DATE AVAILABLE AGE myblog 1/1 1 1 2m22s mysql 1/1 1 1 2d11h * `NAME` 列出了集群中 Deployments 的名称。 * `READY`显示当前正在运行的副本数/期望的副本数。 * `UP-TO-DATE`显示已更新以实现期望状态的副本数。 * `AVAILABLE`显示应用程序可供用户使用的副本数。 * `AGE` 显示应用程序运行的时间量。 # 查看pod $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 109s mysql-85f4f65f99-w6jkj 1/1 Running 0 2m28s 副本保障机制 controller实时检测pod状态，并保障副本数一直处于期望的值。\n## 删除pod，观察pod状态变化 $ kubectl -n demo delete pod myblog-7c96c9f76b-qbbg7 # 观察pod $ kubectl get pods -o wide ## 设置两个副本, 或者通过kubectl -n demo edit deploy myblog的方式，最好通过修改文件，然后apply的方式，这样yaml文件可以保持同步 $ kubectl -n demo scale deploy myblog --replicas=2 deployment.extensions/myblog scaled # 观察pod $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 11m myblog-7c96c9f76b-s6brm 1/1 Running 0 55s mysql-85f4f65f99-w6jkj 1/1 Running 0 11m Pod驱逐策略 K8S 有个特色功能叫 pod eviction，它在某些场景下如节点 NotReady，或者资源不足时，把 pod 驱逐至其它节点，这也是出于业务保护的角度去考虑的。\nKube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。停掉kubelet\npod-eviction-timeout：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod\nmemory.available：节点可用内存 nodefs.available：节点根盘可用存储空间 nodefs.inodesFree：节点inodes可用数量 imagefs.available：镜像存储盘的可用空间 imagefs.inodesFree：镜像存储盘的inodes可用数量 服务更新 修改dockerfile，重新打tag模拟服务更新。\n更新方式：\n修改yaml文件，使用kubectl -n demo apply -f deploy-myblog.yaml来应用更新 kubectl -n demo edit deploy myblog在线更新 kubectl set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record 修改文件测试：\n$ vi mybolg/blog/template/index.html $ docker build . -t 172.21.32.6:5000/myblog:v2 -f Dockerfile_optimized $ docker push 172.21.32.6:5000/myblog:v2 更新策略 ... spec: replicas: 2\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: myblog strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate\t#指定更新方式为滚动更新，默认策略，通过get deploy yaml查看 ... 策略控制：\nmaxSurge：最大激增数, 指更新过程中, 最多可以比replicas预先设定值多出的pod数量, 可以为固定值或百分比,默认为desired Pods数的25%。计算时向上取整(比如3.4，取4)，更新过程中最多会有replicas + maxSurge个pod maxUnavailable： 指更新过程中, 最多有几个pod处于无法服务状态 , 可以为固定值或百分比，默认为desired Pods数的25%。计算时向下取整(比如3.6，取3) 在Deployment rollout时，需要保证Available(Ready) Pods数不低于 desired pods number - maxUnavailable; 保证所有的非异常状态Pods数不多于 desired pods number + maxSurge。\n以myblog为例，使用默认的策略，更新过程:\nmaxSurge 25%，2个实例，向上取整，则maxSurge为1，意味着最多可以有2+1=3个Pod，那么此时会新创建1个ReplicaSet，RS-new，把副本数置为1，此时呢，副本控制器就去创建这个新的Pod 同时，maxUnavailable是25%，副本数2*25%，向下取整，则为0，意味着，滚动更新的过程中，不能有少于2个可用的Pod，因此，旧的Replica（RS-old）会先保持不动，等RS-new管理的Pod状态Ready后，此时已经有3个Ready状态的Pod了，那么由于只要保证有2个可用的Pod即可，因此，RS-old的副本数会有2个变成1个，此时，会删掉一个旧的Pod 删掉旧的Pod的时候，由于总的Pod数量又变成2个了，因此，距离最大的3个还有1个Pod可以创建，所以，RS-new把管理的副本数由1改成2，此时又会创建1个新的Pod，等RS-new管理了2个Pod都ready后，那么就可以把RS-old的副本数由1置为0了，这样就完成了滚动更新 #查看滚动更新事件 $ kubectl -n demo describe deploy myblog ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 1 Normal ScalingReplicaSet 11s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 1 Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 2 Normal ScalingReplicaSet 6s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 0 $ kubectl get rs NAME DESIRED CURRENT READY AGE myblog-6cf56fc848 2 2 2 16h myblog-6fdcf98f9 0 0 0 16h 服务回滚 通过滚动升级的策略可以平滑的升级Deployment，若升级出现问题，需要最快且最好的方式回退到上一次能够提供正常工作的版本。为此K8S提供了回滚机制。\nrevision：更新应用时，K8S都会记录当前的版本号，即为revision，当升级出现问题时，可通过回滚到某个特定的revision，默认配置下，K8S只会保留最近的几个revision，可以通过Deployment配置文件中的spec.revisionHistoryLimit属性增加revision数量，默认是10。\n查看当前：\n$ kubectl -n demo rollout history deploy myblog ##CHANGE-CAUSE为空 $ kubectl delete -f deploy-myblog.yaml ## 方便演示到具体效果，删掉已有deployment 记录回滚：\n$ kubectl create -f deploy-myblog.yaml --record $ kubectl -n demo set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record=true 查看deployment更新历史：\n$ kubectl -n demo rollout history deploy myblog deployment.extensions/myblog REVISION CHANGE-CAUSE 1 kubectl create --filename=deploy-myblog.yaml --record=true 2 kubectl set image deploy myblog myblog=172.21.32.6:5000/demo/myblog:v1 --record=true 回滚到具体的REVISION:\n$ kubectl -n demo rollout undo deploy myblog --to-revision=1 deployment.extensions/myblog rolled back # 访问应用测试 Kubernetes调度 录屏！！！ 为何要控制Pod应该如何调度 集群中有些机器的配置高（SSD，更好的内存等），我们希望核心的服务（比如说数据库）运行在上面 某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上 \u0026hellip;\u0026hellip; NodeSelector label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，POD 的调度可以根据节点的 label 进行特定的部署。\n查看节点的label：\n$ kubectl get nodes --show-labels 为节点打label：\n$ kubectl label node k8s-master disktype=ssd 当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。\n... spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.6:5000/demo/mysql:5.7 ... nodeAffinity 节点亲和性 ， 比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合，不只是简单的相等匹配 。分为两种，软策略和硬策略。\npreferredDuringSchedulingIgnoredDuringExecution：软策略，如果你没有满足调度要求的节点的话，Pod就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有满足就忽略掉的策略。\nrequiredDuringSchedulingIgnoredDuringExecution ： 硬策略，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不会调度Pod。\n#要求 Pod 不能运行在128和132两个节点上，如果有个节点满足disktype=ssd的话就优先调度到这个节点上 ... spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog ports: - containerPort: 8002 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - 192.168.136.128 - 192.168.136.132 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disktype operator: In values: - ssd - sas ... 这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：\nIn：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod\n污点（Taints）与容忍（tolerations） 对于nodeAffinity无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度Pod。\nTaints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的。于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。\n比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：\n设置污点：\n$ kubectl taint node [node_name] key=value:[effect] 其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ] NoSchedule：一定不能被调度。 PreferNoSchedule：尽量不要调度。 NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。 示例：kubectl taint node k8s-master smoke=true:NoSchedule 去除污点：\n去除指定key及其effect： kubectl taint nodes [node_name] key:[effect]- #这里的key不用指定value 去除指定key所有的effect: kubectl taint nodes node_name key- 示例： kubectl taint node k8s-master smoke=true:NoSchedule kubectl taint node k8s-master smoke:NoExecute- kubectl taint node k8s-master smoke- 污点演示：\n## 给k8s-slave1打上污点，smoke=true:NoSchedule $ kubectl taint node k8s-slave1 smoke=true:NoSchedule $ kubectl taint node k8s-slave2 drunk=true:NoSchedule ## 扩容myblog的Pod，观察新Pod的调度情况 $ kuebctl -n demo scale deploy myblog --replicas=3 $ kubectl -n demo get po -w ## pending Pod容忍污点示例：myblog/deployment/deploy-myblog-taint.yaml\n... spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog tolerations: #设置容忍性 - key: \u0026#34;smoke\u0026#34; operator: \u0026#34;Equal\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;drunk\u0026#34; operator: \u0026#34;Equal\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; #意思是这个Pod要容忍的有污点的Node的key是smoke Equal true,效果是NoSchedule， #tolerations属性下各值必须使用引号，容忍的值都是设置Node的taints时给的值。 $ kubectl apply -f deploy-myblog-taint.yaml spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog tolerations: - operator: \u0026#34;Exists\u0026#34; Kubernetes服务访问之Service 录屏！！！ 通过以前的学习，我们已经能够通过Deployment来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两个问题：\nPod IP仅仅是集群内可见的虚拟IP，外部无法访问。 Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。 Service 负载均衡之Cluster IP service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP 。使用Service对象，通过selector进行标签选择，找到对应的Pod:\nmyblog/deployment/svc-myblog.yaml\napiVersion: v1 kind: Service metadata: name: myblog namespace: demo spec: ports: - port: 80 protocol: TCP targetPort: 8002 selector: app: myblog type: ClusterIP 操作演示：\n## 别名 $ alias kd=\u0026#39;kubectl -n demo\u0026#39; ## 创建服务 $ kd create -f svc-myblog.yaml $ kd get po --show-labels NAME READY STATUS RESTARTS AGE LABELS myblog-5c97d79cdb-jn7km 1/1 Running 0 6m5s app=myblog mysql-85f4f65f99-w6jkj 1/1 Running 0 176m app=mysql $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 7m50s $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002 Session Affinity: None Events: \u0026lt;none\u0026gt; ## 扩容myblog服务 $ kd scale deploy myblog --replicas=2 deployment.extensions/myblog scaled ## 再次查看 $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002,10.244.1.158:8002 Session Affinity: None Events: \u0026lt;none\u0026gt; Service与Pod如何关联:\nservice对象创建的同时，会创建同名的endpoints对象，若服务设置了readinessProbe, 当readinessProbe检测失败时，endpoints列表中会剔除掉对应的pod_ip，这样流量就不会分发到健康检测失败的Pod中\n$ kd get endpoints myblog NAME ENDPOINTS AGE myblog 10.244.0.68:8002,10.244.1.158:8002 7m Service Cluster-IP如何访问:\n$ kd get svc myblog NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 13m $ curl 10.99.174.93/blog/index/ 为mysql服务创建service：\napiVersion: v1 kind: Service metadata: name: mysql namespace: demo spec: ports: - port: 3306 protocol: TCP targetPort: 3306 selector: app: mysql type: ClusterIP 访问mysql：\n$ kd get svc mysql mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 3s $ curl 10.108.214.84:3306 目前使用hostNetwork部署，通过宿主机ip+port访问，弊端：\n服务使用hostNetwork，使得宿主机的端口大量暴漏，存在安全隐患 容易引发端口冲突 服务均属于k8s集群，尽可能使用k8s的网络访问，因此可以对目前myblog访问mysql的方式做改造：\n为mysql创建一个固定clusterIp的Service，把clusterIp配置在myblog的环境变量中 利用集群服务发现的能力，组件之间通过service name来访问 服务发现 在k8s集群中，组件之间可以通过定义的Service名称实现通信。\n演示服务发现：\n## 演示思路：在myblog的容器中直接通过service名称访问服务，观察是否可以访问通 # 先查看服务 $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 59m mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 35m # 进入myblog容器 $ kd exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# curl mysql:3306 5.7.29 )→ (mysql_native_password ot packets out of order [root@myblog-5c97d79cdb-j485f myblog]# curl myblog/blog/index/ 我的博客列表 虽然podip和clusterip都不固定，但是service name是固定的，而且具有完全的跨集群可移植性，因此组件之间调用的同时，完全可以通过service name去通信，这样避免了大量的ip维护成本，使得服务的yaml模板更加简单。因此可以对mysql和myblog的部署进行优化改造：\nmysql可以去掉hostNetwork部署，使得服务只暴漏在k8s集群内部网络 configMap中数据库地址可以换成Service名称，这样跨环境的时候，配置内容基本上可以保持不用变化 修改deploy-mysql.yaml\nspec: hostNetwork: true\t# 去掉此行 volumes: - name: mysql-data hostPath: path: /opt/mysql/data 修改configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: myblog namespace: demo data: MYSQL_HOST: \u0026#34;mysql\u0026#34;\t# 此处替换为mysql MYSQL_PORT: \u0026#34;3306\u0026#34; 应用修改：\n$ kubectl apply -f configmap.yaml $ kubectl apply -f deploy-mysql.yaml ## 重建pod $ kubectl -n demo delete po mysql-7f747644b8-6npzn #去掉taint $ kubectl taint node k8s-slave1 smoke- $ kubectl taint node k8s-slave2 drunk- ## myblog不用动，会自动因健康检测不过而重启 服务发现实现：\nCoreDNS是一个Go语言实现的链式插件DNS服务端，是CNCF成员，是一个高性能、易扩展的DNS服务端。\n$ kubectl -n kube-system get po -o wide|grep dns coredns-d4475785-2w4hk 1/1 Running 0 4d22h 10.244.0.64 coredns-d4475785-s49hq 1/1 Running 0 4d22h 10.244.0.65 # 查看myblog的pod解析配置 $ kubectl -n demo exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# cat /etc/resolv.conf nameserver 10.96.0.10 search demo.svc.cluster.local svc.cluster.local cluster.local options ndots:5 ## 10.96.0.10 从哪来 $ kubectl -n kube-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 51d ## 启动pod的时候，会把kube-dns服务的cluster-ip地址注入到pod的resolve解析配置中，同时添加对应的namespace的search域。 因此跨namespace通过service name访问的话，需要添加对应的namespace名称， service_name.namespace_name $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 26h Service负载均衡之NodePort cluster-ip为虚拟地址，只能在k8s集群内部进行访问，集群外部如果访问内部服务，实现方式之一为使用NodePort方式。NodePort会默认在 30000-32767 ，不指定的会随机使用其中一个。\nmyblog/deployment/svc-myblog-nodeport.yaml\napiVersion: v1 kind: Service metadata: name: myblog-np namespace: demo spec: ports: - port: 80 protocol: TCP targetPort: 8002 selector: app: myblog type: NodePort 查看并访问服务：\n$ kd create -f svc-myblog-nodeport.yaml service/myblog-np created $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 102m myblog-np NodePort 10.105.228.101 \u0026lt;none\u0026gt; 80:30647/TCP 4s mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 77m #集群内每个节点的NodePort端口都会进行监听 $ curl 192.168.136.128:30647/blog/index/ 我的博客列表 $ curl 192.168.136.131:30647/blog/index/ 我的博客列表 ## 浏览器访问 思考：\nNodePort的端口监听如何转发到对应的Pod服务？\nCLUSTER-IP为虚拟IP，集群内如何通过虚拟IP访问到具体的Pod服务？\nkube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过创建流量路由规则来实现网络的转发。参照\n有三种模式：\nUser space, 让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发 ， 所有报文都走一遍用户态，性能不高，k8s v1.2版本后废弃。 Iptables， 当前默认模式，完全由 IPtables 来实现， 通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。 IPVS， 与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 k8s 1.8版本开始引入，1.11版本开始稳定，需要开启宿主机的ipvs模块。 IPtables模式示意图：\n$ iptables-save |grep -v myblog-np|grep \u0026#34;demo/myblog\u0026#34; -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-WQNGJ7YFZKCTKPZK $ iptables-save |grep KUBE-SVC-WQNGJ7YFZKCTKPZK -A KUBE-SVC-WQNGJ7YFZKCTKPZK -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SVC-WQNGJ7YFZKCTKPZK -j KUBE-SEP-7GWC3FN2JI5KLE47 $ iptables-save |grep KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SEP-GB5GNOM5CZH7ICXZ -p tcp -m tcp -j DNAT --to-destination 10.244.1.158:8002 $ iptables-save |grep KUBE-SEP-7GWC3FN2JI5KLE47 -A KUBE-SEP-7GWC3FN2JI5KLE47 -p tcp -m tcp -j DNAT --to-destination 10.244.1.159:8002 Kubernetes服务访问之Ingress 对于Kubernetes的Service，无论是Cluster-Ip和NodePort均是四层的负载，集群内的服务如何实现七层的负载均衡，这就需要借助于Ingress，Ingress控制器的实现方式有很多，比如nginx, Contour, Haproxy, trafik, Istio，我们以nginx的实现为例做演示。\nIngress-nginx是7层的负载均衡器 ，负责统一管理外部对k8s cluster中service的请求。主要包含：\ningress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）； ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件） 示意图： 实现逻辑 1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化 2）然后读取ingress规则(规则就是写明了哪个域名对应哪个service)，按照自定义的规则，生成一段nginx配置 3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器把生成的nginx配置写入/etc/nginx.conf文件中 4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。\n安装 官方文档\n$ wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml ## 或者使用myblog/deployment/ingress/mandatory.yaml ## 修改部署节点 $ grep -n5 nodeSelector mandatory.yaml 212- spec: 213- hostNetwork: true #添加为host模式 214- # wait up to five minutes for the drain of connections 215- terminationGracePeriodSeconds: 300 216- serviceAccountName: nginx-ingress-serviceaccount 217: nodeSelector: 218- ingress: \u0026#34;true\u0026#34;\t#替换此处，来决定将ingress部署在哪些机器 219- containers: 220- - name: nginx-ingress-controller 221- image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 222- args: 使用示例：myblog/deployment/ingress.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: myblog namespace: demo spec: rules: - host: myblog.devops.cn http: paths: - path: / backend: serviceName: myblog servicePort: 80 ingress-nginx动态生成upstream配置：\n... server_name myblog.devops.cn ; listen 80 ; listen [::]:80 ; listen 443 ssl http2 ; listen [::]:443 ssl http2 ; set $proxy_upstream_name \u0026#34;-\u0026#34;; ssl_certificate_by_lua_block { certificate.call() } location / { set $namespace \u0026#34;demo\u0026#34;; set $ingress_name \u0026#34;myblog\u0026#34;; ... 访问 域名解析服务，将 myblog.devops.cn解析到ingress的地址上。ingress是支持多副本的，高可用的情况下，生产的配置是使用lb服务（内网F5设备，公网elb、slb、clb，解析到各ingress的机器，如何域名指向lb地址）\n本机，添加如下hosts记录来演示效果。\n192.168.136.128 myblog.devops.cn 然后，访问 http://myblog.devops.cn/blog/index/\nHTTPS访问：\n#自签名证书 $ openssl req -x509 -nodes -days 2920 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=*.devops.cn/O=ingress-nginx\u0026#34; # 证书信息保存到secret对象中，ingress-nginx会读取secret对象解析出证书加载到nginx配置中 $ kubectl -n demo create secret tls https-secret --key tls.key --cert tls.crt secret/https-secret created 修改yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: myblog-tls namespace: demo spec: rules: - host: myblog.devops.cn http: paths: - path: / backend: serviceName: myblog servicePort: 80 tls: - hosts: - myblog.devops.cn secretName: https-secret 然后，访问 https://myblog.devops.cn/blog/index/\nKubernetes认证与授权 录屏！！！ APIService安全控制 Authentication：身份认证\n这个环节它面对的输入是整个http request，负责对来自client的请求进行身份校验，支持的方法包括: client证书验证（https双向验证）\nbasic auth\n普通token\njwt token(用于serviceaccount)\nAPIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证， 只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功；\n使用kubeadm引导启动的k8s集群的apiserver初始配置中，默认支持client证书验证和serviceaccount两种身份验证方式。 证书认证通过设置--client-ca-file根证书以及--tls-cert-file和--tls-private-key-file来开启。\n在这个环节，apiserver会通过client证书或 http header中的字段(比如serviceaccount的jwt token)来识别出请求的用户身份，包括”user”、”group”等，这些信息将在后面的authorization环节用到。\nAuthorization：鉴权，你可以访问哪些资源\n这个环节面对的输入是http request context中的各种属性，包括：user、group、request path（比如：/api/v1、/healthz、/version等）、 request verb(比如：get、list、create等)。\nAPIServer会将这些属性值与事先配置好的访问策略(access policy）相比较。APIServer支持多种authorization mode，包括Node、RBAC、Webhook等。\nAPIServer启动时，可以指定一种authorization mode，也可以指定多种authorization mode，如果是后者，只要Request通过了其中一种mode的授权， 那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，authorization-mode的默认配置是”Node,RBAC”。\nAdmission Control：准入控制，一个控制链(层层关卡)，偏集群安全控制、管理方面。为什么说是安全相关的机制？\n以NamespaceLifecycle为例， 该插件确保处于Termination状态的Namespace不再接收新的对象创建请求，并拒绝请求不存在的Namespace。该插件还可以防止删除系统保留的Namespace:default，kube-system，kube-public。 NodeRestriction， 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制 。 为什么我们执行命令kubectl命令，可以直接管理k8s集群资源？\nkubectl的认证授权 kubectl的日志调试级别：\n信息 描述 v=0 通常，这对操作者来说总是可见的。 v=1 当您不想要很详细的输出时，这个是一个合理的默认日志级别。 v=2 有关服务和重要日志消息的有用稳定状态信息，这些信息可能与系统中的重大更改相关。这是大多数系统推荐的默认日志级别。 v=3 关于更改的扩展信息。 v=4 调试级别信息。 v=6 显示请求资源。 v=7 显示 HTTP 请求头。 v=8 显示 HTTP 请求内容。 v=9 显示 HTTP 请求内容，并且不截断内容。 $ kubectl get nodes -v=7 I0329 20:20:08.633065 3979 loader.go:359] Config loaded from file /root/.kube/config I0329 20:20:08.633797 3979 round_trippers.go:416] GET https://192.168.136.128:6443/api/v1/nodes?limit=500 kubeadm init启动完master节点后，会默认输出类似下面的提示内容：\n... ... Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ... ... 这些信息是在告知我们如何配置kubeconfig文件。按照上述命令配置后，master节点上的kubectl就可以直接使用$HOME/.kube/config的信息访问k8s cluster了。 并且，通过这种配置方式，kubectl也拥有了整个集群的管理员(root)权限。\n很多K8s初学者在这里都会有疑问：\n当kubectl使用这种kubeconfig方式访问集群时，Kubernetes的kube-apiserver是如何对来自kubectl的访问进行身份验证(authentication)和授权(authorization)的呢？ 为什么来自kubectl的请求拥有最高的管理员权限呢？ 查看/root/.kube/config文件：\n前面提到过apiserver的authentication支持通过tls client certificate、basic auth、token等方式对客户端发起的请求进行身份校验， 从kubeconfig信息来看，kubectl显然在请求中使用了tls client certificate的方式，即客户端的证书。\n证书base64解码：\n$ echo xxxxxxxxxxxxxx |base64 -d \u0026gt; kubectl.crt 说明在认证阶段，apiserver会首先使用--client-ca-file配置的CA证书去验证kubectl提供的证书的有效性,基本的方式 ：\n$ openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt kubectl.crt: OK 除了认证身份，还会取出必要的信息供授权阶段使用，文本形式查看证书内容：\n$ openssl x509 -in kubectl.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 4736260165981664452 (0x41ba9386f52b74c4) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Feb 10 07:33:39 2020 GMT Not After : Feb 9 07:33:40 2021 GMT Subject: O=system:masters, CN=kubernetes-admin ... 认证通过后，提取出签发证书时指定的CN(Common Name),kubernetes-admin，作为请求的用户名 (User Name), 从证书中提取O(Organization)字段作为请求用户所属的组 (Group)，group = system:masters，然后传递给后面的授权模块。\nkubeadm在init初始引导集群启动过程中，创建了许多default的role、clusterrole、rolebinding和clusterrolebinding， 在k8s有关RBAC的官方文档中，我们看到下面一些default clusterrole列表:\n其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。 沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。\n我们查看一下这一binding：\n$ kubectl describe clusterrolebinding cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true Role: Kind: ClusterRole Name: cluster-admin Subjects: Kind Name Namespace ---- ---- --------- Group system:masters 我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起， 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。\n我们再来查看一下cluster-admin这个role的具体权限信息：\n$ kubectl describe clusterrole cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.* [] [] [*] [*] [] [*] 非资源类，如查看集群健康状态。\nRBAC Role-Based Access Control，基于角色的访问控制， apiserver启动参数添加\u0026ndash;authorization-mode=RBAC 来启用RBAC认证模式，kubeadm安装的集群默认已开启。官方介绍\n查看开启：\n# master节点查看apiserver进程 $ ps aux |grep apiserver RBAC模式引入了4个资源：\nRole，角色\n一个Role只能授权访问单个namespace\n## 示例定义一个名为pod-reader的角色，该角色具有读取default这个命名空间下的pods的权限 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] ## apiGroups: \u0026#34;\u0026#34;,\u0026#34;apps\u0026#34;, \u0026#34;autoscaling\u0026#34;, \u0026#34;batch\u0026#34;, kubectl api-versions ## resources: \u0026#34;services\u0026#34;, \u0026#34;pods\u0026#34;,\u0026#34;deployments\u0026#34;... kubectl api-resources ## verbs: \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;exec\u0026#34; ClusterRole\n一个ClusterRole能够授予和Role一样的权限，但是它是集群范围内的。\n## 定义一个集群角色，名为secret-reader，该角色可以读取所有的namespace中的secret资源 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # \u0026#34;namespace\u0026#34; omitted since ClusterRoles are not namespaced name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] Rolebinding\n将role中定义的权限分配给用户和用户组。RoleBinding包含主题（users,groups,或service accounts）和授予角色的引用。对于namespace内的授权使用RoleBinding，集群范围内使用ClusterRoleBinding。\n## 定义一个角色绑定，将pod-reader这个role的权限授予给jane这个User，使得jane可以在读取default这个命名空间下的所有的pod数据 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User #这里可以是User,Group,ServiceAccount name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role #这里可以是Role或者ClusterRole,若是ClusterRole，则权限也仅限于rolebinding的内部 name: pod-reader # match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io 注意：rolebinding既可以绑定role，也可以绑定clusterrole，当绑定clusterrole的时候，subject的权限也会被限定于rolebinding定义的namespace内部，若想跨namespace，需要使用clusterrolebinding\n## 定义一个角色绑定，将dave这个用户和secret-reader这个集群角色绑定，虽然secret-reader是集群角色，但是因为是使用rolebinding绑定的，因此dave的权限也会被限制在development这个命名空间内 apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \u0026#34;dave\u0026#34; to read secrets in the \u0026#34;development\u0026#34; namespace. # You need to already have a ClusterRole named \u0026#34;secret-reader\u0026#34;. kind: RoleBinding metadata: name: read-secrets # # The namespace of the RoleBinding determines where the permissions are granted. # This only grants permissions within the \u0026#34;development\u0026#34; namespace. namespace: development subjects: - kind: User name: dave # Name is case sensitive apiGroup: rbac.authorization.k8s.io - kind: ServiceAccount name: dave # Name is case sensitive namespace: demo roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 考虑一个场景： 如果集群中有多个namespace分配给不同的管理员，每个namespace的权限是一样的，就可以只定义一个clusterrole，然后通过rolebinding将不同的namespace绑定到管理员身上，否则就需要每个namespace定义一个Role，然后做一次rolebinding。\nClusterRolebingding\n允许跨namespace进行授权\napiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows anyone in the \u0026#34;manager\u0026#34; group to read secrets in any namespace. kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: Group name: manager # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io kubelet的认证授权 查看kubelet进程\n$ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2020-04-01 02:34:13 CST; 1 day 14h ago Docs: https://kubernetes.io/docs/ Main PID: 851 (kubelet) Tasks: 21 Memory: 127.1M CGroup: /system.slice/kubelet.service └─851 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf 查看/etc/kubernetes/kubelet.conf，解析证书：\n$ echo xxxxx |base64 -d \u0026gt;kubelet.crt $ openssl x509 -in kubelet.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 9059794385454520113 (0x7dbadafe23185731) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Feb 10 07:33:39 2020 GMT Not After : Feb 9 07:33:40 2021 GMT Subject: O=system:nodes, CN=system:node:master-1 得到我们期望的内容：\nSubject: O=system:nodes, CN=system:node:k8s-master 我们知道，k8s会把O作为Group来进行请求，因此如果有权限绑定给这个组，肯定在clusterrolebinding的定义中可以找得到。因此尝试去找一下绑定了system:nodes组的clusterrolebinding\n$ kubectl get clusterrolebinding|awk \u0026#39;NR\u0026gt;1{print $1}\u0026#39;|xargs kubectl get clusterrolebinding -oyaml|grep -n10 system:nodes 98- roleRef: 99- apiGroup: rbac.authorization.k8s.io 100- kind: ClusterRole 101- name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 102- subjects: 103- - apiGroup: rbac.authorization.k8s.io 104- kind: Group 105: name: system:nodes 106-- apiVersion: rbac.authorization.k8s.io/v1 107- kind: ClusterRoleBinding 108- metadata: 109- creationTimestamp: \u0026#34;2020-02-10T07:34:02Z\u0026#34; 110- name: kubeadm:node-proxier 111- resourceVersion: \u0026#34;213\u0026#34; 112- selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier $ kubectl describe clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeclient [] [] [create] 结局有点意外，除了system:certificates.k8s.io:certificatesigningrequests:selfnodeclient外，没有找到system相关的rolebindings，显然和我们的理解不一样。 尝试去找资料，发现了这么一段 :\nDefault ClusterRole Default ClusterRoleBinding Description system:kube-scheduler system:kube-scheduler user Allows access to the resources required by the schedulercomponent. system:volume-scheduler system:kube-scheduler user Allows access to the volume resources required by the kube-scheduler component. system:kube-controller-manager system:kube-controller-manager user Allows access to the resources required by the controller manager component. The permissions required by individual controllers are detailed in the controller roles. system:node None Allows access to resources required by the kubelet, including read access to all secrets, and write access to all pod status objects. You should use the Node authorizer and NodeRestriction admission plugin instead of the system:node role, and allow granting API access to kubelets based on the Pods scheduled to run on them. The system:node role only exists for compatibility with Kubernetes clusters upgraded from versions prior to v1.8. system:node-proxier system:kube-proxy user Allows access to the resources required by the kube-proxycomponent. 大致意思是说：之前会定义system:node这个角色，目的是为了kubelet可以访问到必要的资源，包括所有secret的读权限及更新pod状态的写权限。如果1.8版本后，是建议使用 Node authorizer and NodeRestriction admission plugin 来代替这个角色的。\n我们目前使用1.16，查看一下授权策略：\n$ ps axu|grep apiserver kube-apiserver --authorization-mode=Node,RBAC --enable-admission-plugins=NodeRestriction 查看一下官网对Node authorizer的介绍：\nNode authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.\nIn future releases, the node authorizer may add or remove permissions to ensure kubelets have the minimal set of permissions required to operate correctly.\nIn order to be authorized by the Node authorizer, kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;\nService Account 前面说，认证可以通过证书，也可以通过使用ServiceAccount（服务账户）的方式来做认证。大多数时候，我们在基于k8s做二次开发时都是选择通过serviceaccount的方式。我们之前访问dashboard的时候，是如何做的？\n## 新建一个名为admin的serviceaccount，并且把名为cluster-admin的这个集群角色的权限授予新建的serviceaccount apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kubernetes-dashboard --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kubernetes-dashboard 我们查看一下：\n$ kubectl -n kubernetes-dashboard get sa admin -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2020-04-01T11:59:21Z\u0026#34; name: admin namespace: kubernetes-dashboard resourceVersion: \u0026#34;1988878\u0026#34; selfLink: /api/v1/namespaces/kubernetes-dashboard/serviceaccounts/admin uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f secrets: - name: admin-token-lfsrf 注意到serviceaccount上默认绑定了一个名为admin-token-lfsrf的secret，我们查看一下secret\n$ kubectl -n kubernetes-dashboard describe secret admin-token-lfsrf Name: admin-token-lfsrf Namespace: kubernetes-dashboard Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin kubernetes.io/service-account.uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 4 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A 开发的时候如何去调用k8s的api:\ncurl演示 $ curl -k -H \u0026#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A\u0026#34; https://62.234.214.206:6443/api/v1/namespaces/demo/pods?limit=500 postman 查看etcd数据 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看所有key值：\n$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get / --prefix --keys-only 查看具体的key对应的数据：\n$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb 基于EFK实现kubernetes集群的日志平台（扩展） 录屏！！！ EFK介绍 EFK工作示意\nElasticsearch\n一个开源的分布式、Restful 风格的搜索和数据分析引擎，它的底层是开源库Apache Lucene。它可以被下面这样准确地形容：\n一个分布式的实时文档存储，每个字段可以被索引与搜索； 一个分布式实时分析搜索引擎； 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。 Fluentd\n一个针对日志的收集、处理、转发系统。通过丰富的插件系统，可以收集来自于各种系统或应用的日志，转化为用户指定的格式后，转发到用户所指定的日志存储系统之中。\nFluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储、kafka等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下\n首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务 Kibana\nKibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。可以通过Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。也可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。\n部署es服务 部署分析 es生产环境是部署es集群，通常会使用statefulset进行部署，此例由于演示环境资源问题，部署为单点 数据存储挂载主机路径 es默认使用elasticsearch用户启动进程，es的数据目录是通过宿主机的路径挂载，因此目录权限被主机的目录权限覆盖，因此可以利用init container容器在es进程启动之前把目录的权限修改掉，注意init container要用特权模式启动。 部署并验证 efk/elasticsearch.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: elasticsearch-logging version: v7.4.2 name: elasticsearch-logging namespace: logging spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: elasticsearch-logging version: v7.4.2 serviceName: elasticsearch-logging template: metadata: labels: k8s-app: elasticsearch-logging version: v7.4.2 spec: nodeSelector: log: \u0026#34;true\u0026#34;\t## 指定部署在哪个节点。需根据环境来修改 containers: - env: - name: NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: cluster.initial_master_nodes value: elasticsearch-logging-0 - name: ES_JAVA_OPTS value: \u0026#34;-Xms512m -Xmx512m\u0026#34; image: 172.21.32.6:5000/elasticsearch/elasticsearch:7.4.2 name: elasticsearch-logging ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-logging dnsConfig: options: - name: single-request-reopen initContainers: - command: - /sbin/sysctl - -w - vm.max_map_count=262144 image: alpine:3.6 imagePullPolicy: IfNotPresent name: elasticsearch-logging-init resources: {} securityContext: privileged: true - name: fix-permissions image: alpine:3.6 command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026#34;] securityContext: privileged: true volumeMounts: - name: elasticsearch-logging mountPath: /usr/share/elasticsearch/data volumes: - name: elasticsearch-logging hostPath: path: /esdata --- apiVersion: v1 kind: Service metadata: labels: k8s-app: elasticsearch-logging name: elasticsearch namespace: logging spec: ports: - port: 9200 protocol: TCP targetPort: db selector: k8s-app: elasticsearch-logging type: ClusterIP $ kubectl create namespace logging ## 给slave1节点打上label，将es服务调度到slave1节点 $ kubectl label node k8s-slave1 log=true ## 部署服务，可以先去部署es的节点把镜像下载到本地 $ kubectl create -f elasticsearch.yaml statefulset.apps/elasticsearch-logging created service/elasticsearch created ## 等待片刻，查看一下es的pod部署到了k8s-slave1节点，状态变为running $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE IP NODE elasticsearch-logging-0 1/1 Running 0 69m 10.244.1.104 k8s-slave1 # 然后通过curl命令访问一下服务，验证es是否部署成功 $ kubectl -n logging get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.109.174.58 \u0026lt;none\u0026gt; 9200/TCP 71m $ curl 10.109.174.58:9200 { \u0026#34;name\u0026#34; : \u0026#34;elasticsearch-logging-0\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;docker-cluster\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;uic8xOyNSlGwvoY9DIBT1g\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 部署kibana 部署分析 kibana需要暴漏web页面给前端使用，因此使用ingress配置域名来实现对kibana的访问 kibana为无状态应用，直接使用Deployment来启动 kibana需要访问es，直接利用k8s服务发现访问此地址即可，http://elasticsearch:9200 部署并验证 资源文件 efk/kibana.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: 172.21.32.6:5000/kibana/kibana:7.4.2 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 --- apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 protocol: TCP targetPort: 5601 type: ClusterIP selector: app: kibana --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: logging spec: rules: - host: kibana.devops.cn http: paths: - path: / backend: serviceName: kibana servicePort: 5601 $ kubectl create -f kibana.yaml deployment.apps/kibana created service/kibana created ingress/kibana created # 然后查看pod，等待状态变成running $ kubectl -n logging get po NAME READY STATUS RESTARTS AGE elasticsearch-logging-0 1/1 Running 0 88m kibana-944c57766-ftlcw 1/1 Running 0 15m ## 配置域名解析 kibana.devops.cn，并访问服务进行验证，若可以访问，说明连接es成功 部署fluentd 部署分析 fluentd为日志采集服务，kubernetes集群的每个业务节点都有日志产生，因此需要使用daemonset的模式进行部署 为进一步控制资源，会为daemonset指定一个选择表情，fluentd=true来做进一步过滤，只有带有此标签的节点才会部署fluentd 日志采集，需要采集哪些目录下的日志，采集后发送到es端，因此需要配置的内容比较多，我们选择使用configmap的方式把配置文件整个挂载出来 部署服务 配置文件，efk/fluentd-es-main.yaml\napiVersion: v1 data: fluent.conf: |- # This is the root config file, which only includes components of the actual configuration # # Do not collect fluentd\u0026#39;s own logs to avoid infinite loops. \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; @include /fluentd/etc/config.d/*.conf kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: Reconcile name: fluentd-es-config-main namespace: logging 配置文件，fluentd-config.yaml，注意点：\n数据源source的配置，k8s会默认把容器的标准和错误输出日志重定向到宿主机中 默认集成了 kubernetes_metadata_filter 插件，来解析日志格式，得到k8s相关的元数据，raw.kubernetes match输出到es端的flush配置 kind: ConfigMap apiVersion: v1 metadata: name: fluentd-config namespace: logging labels: addonmanager.kubernetes.io/mode: Reconcile data: system.conf: |- \u0026lt;system\u0026gt; root_dir /tmp/fluentd-buffers/ \u0026lt;/system\u0026gt; containers.input.conf: |- \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos time_format %Y-%m-%dT%H:%M:%S.%NZ localtime tag raw.kubernetes.* format json read_from_head true \u0026lt;/source\u0026gt; # Detect exceptions in the log output and forward them as one log entry. \u0026lt;match raw.kubernetes.**\u0026gt; @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 \u0026lt;/match\u0026gt; forward.input.conf: |- # Takes the messages sent over TCP \u0026lt;source\u0026gt; @type forward \u0026lt;/source\u0026gt; output.conf: |- # Enriches records with Kubernetes metadata \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata \u0026lt;/filter\u0026gt; \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch port 9200 logstash_format true request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; daemonset定义文件，fluentd.yaml，注意点：\n需要配置rbac规则，因为需要访问k8s api去根据日志查询元数据 需要将/var/log/containers/目录挂载到容器中 需要将fluentd的configmap中的配置文件挂载到容器内 想要部署fluentd的节点，需要添加fluentd=true的标签 efk/fluentd.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \u0026#34;\u0026#34; resources: - \u0026#34;namespaces\u0026#34; - \u0026#34;pods\u0026#34; verbs: - \u0026#34;get\u0026#34; - \u0026#34;watch\u0026#34; - \u0026#34;list\u0026#34; --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: logging apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: fluentd-es apiGroup: \u0026#34;\u0026#34; --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: fluentd-es name: fluentd-es namespace: logging spec: selector: matchLabels: k8s-app: fluentd-es template: metadata: labels: k8s-app: fluentd-es spec: containers: - env: - name: FLUENTD_ARGS value: --no-supervisor -q image: 172.21.32.6:5000/fluentd-es-root:v1.6.2-1.0 imagePullPolicy: IfNotPresent name: fluentd-es resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - mountPath: /var/log name: varlog - mountPath: /var/lib/docker/containers name: varlibdockercontainers readOnly: true - mountPath: /home/docker/containers name: varlibdockercontainershome readOnly: true - mountPath: /fluentd/etc/config.d name: config-volume - mountPath: /fluentd/etc/fluent.conf name: config-volume-main subPath: fluent.conf nodeSelector: fluentd: \u0026#34;true\u0026#34; securityContext: {} serviceAccount: fluentd-es serviceAccountName: fluentd-es volumes: - hostPath: path: /var/log type: \u0026#34;\u0026#34; name: varlog - hostPath: path: /var/lib/docker/containers type: \u0026#34;\u0026#34; name: varlibdockercontainers - hostPath: path: /home/docker/containers type: \u0026#34;\u0026#34; name: varlibdockercontainershome - configMap: defaultMode: 420 name: fluentd-config name: config-volume - configMap: defaultMode: 420 items: - key: fluent.conf path: fluent.conf name: fluentd-es-config-main name: config-volume-main ## 给slave1和slave2打上标签，进行部署fluentd日志采集服务 $ kubectl label node k8s-slave1 fluentd=true node/k8s-slave1 labeled $ kubectl label node k8s-slave2 fluentd=true node/k8s-slave2 labeled # 创建服务 $ kubectl create -f fluentd-es-config-main.yaml configmap/fluentd-es-config-main created $ kubectl create -f fluentd-configmap.yaml configmap/fluentd-config created $ kubectl create -f fluentd.yaml serviceaccount/fluentd-es created clusterrole.rbac.authorization.k8s.io/fluentd-es created clusterrolebinding.rbac.authorization.k8s.io/fluentd-es created daemonset.extensions/fluentd-es created ## 然后查看一下pod是否已经在k8s-slave1和k8s-slave2节点启动成功 $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE elasticsearch-logging-0 1/1 Running 0 123m fluentd-es-246pl 1/1 Running 0 2m2s fluentd-es-4e21w 1/1 Running 0 2m10s kibana-944c57766-ftlcw 1/1 Running 0 50m EFK功能验证 验证思路 k8s-slave1和slave2中启动服务，同时往标准输出中打印测试日志，到kibana中查看是否可以收集\n创建测试容器 apiVersion: v1 kind: Pod metadata: name: counter spec: nodeSelector: fluentd: \u0026#34;true\u0026#34; containers: - name: count image: alpine:3.6 args: [/bin/sh, -c, \u0026#39;i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34;; i=$((i+1)); sleep 1; done\u0026#39;] $ kubectl get po NAME READY STATUS RESTARTS AGE counter 1/1 Running 0 6s 配置kibana 登录kibana界面，按照截图的顺序操作：\n也可以通过其他元数据来过滤日志数据，比如可以单击任何日志条目以查看其他元数据，如容器名称，Kubernetes 节点，命名空间等，比如kubernetes.pod_name : counter\n到这里，我们就在 Kubernetes 集群上成功部署了 EFK ，要了解如何使用 Kibana 进行日志数据分析，可以参考 Kibana 用户指南文档：https://www.elastic.co/guide/en/kibana/current/index.html\n","permalink":"https://wandong1.github.io/post/%E5%9F%BA%E4%BA%8Edocker%E5%92%8Ckubernetes%E7%9A%84%E4%BC%81%E4%B8%9A%E7%BA%A7devops%E5%AE%9E%E8%B7%B5%E8%AE%AD%E7%BB%83%E8%90%A5/","summary":"基于Docker和Kubernetes的企业级DevOps实践训练营 课程准备 离线镜像包\n百度：https://pan.baidu.com/s/1N1AYGCYftYGn6L0QPMWIMw 提取码：ev2h\n天翼云：https://cloud.189.cn/t/ENjUbmRR7FNz\nCentOS7.4版本以上 虚拟机3台（4C+8G+50G），内网互通，可连外网\n课件文档\n《训练营课件》 《安装手册》 git仓库\nhttps://gitee.com/agagin/python-demo.git python demo项目\nhttps://gitee.com/agagin/demo-resources.git demo项目演示需要的资源文件\n关于本人 李永信\n2012-2017，云平台开发工程师，先后对接过Vmware、OpenStack、Docker平台\n2017-2019， 运维开发工程师，Docker+Kubernetes的Paas平台运维开发\n2019至今，DevOps工程师\n8年多的时间，积攒了一定的开发和运维经验，跟大家分享。\n课程安排 2020.4.11 Docker + kubernetes\n2020.4.18 DevOps平台实践\n2天的时间，节奏会相对快一些\n小调研：\nA : 只听过docker，几乎没有docker的使用经验 B：有一定的docker实践经验，不熟悉或者几乎没用过k8s C：对于docker和k8s都有一定的实践经验，想更多了解如何基于docker+k8s构建devops平台 D：其他 课程介绍 最近的三年多时间，关注容器圈的话应该会知道这么几个事情：\n容器技术持续火爆\nKubernetes(k8s)成为容器编排管理的标准\n国内外厂商均已开始了全面拥抱Kubernetes的转型， 无数中小型企业已经落地 Kubernetes，或正走落地的道路上 。基于目前的发展趋势可以预见，未来几年以kubernetes平台为核心的容器运维管理、DevOps等将迎来全面的发展。\n本着实践为核心的思想，本课程使用企业常见的基于Django + uwsgi + Nginx架构的Python Demo项目，分别讲述三个事情：\n项目的容器化\n教大家如何把公司的项目做成容器，并且运行在docker环境中\n使用Kubernetes集群来管理容器化的项目\n带大家一步一步部署k8s集群，并把容器化后的demo项目使用k8s来管理起来\n使用Jenkins和Kubernetes集成，实现demo项目的持续集成/持续交付(CI/CD)\n会使用k8s管理应用生命周期后，还差最后的环节，就是如何把开发、测试、部署的流程使用自动化工具整合起来，最后一部分呢，课程会教会大家如何优雅的使用gitlab+Jenkins+k8s构建企业级的DevOps平台\n流程示意 你将学到哪些 Docker相关\n如何使用Dockerfile快速构建镜像 Docker镜像、容器、仓库的常用操作 Docker容器的网络（Bridge下的SNAT、DNAT） Kubernetes相关\n集群的快速搭建 kubernetes的架构及工作流程 使用Pod控制器管理业务应用的生命周期 使用CoreDNS、Service和Ingress实现服务发现、负载均衡及四层、七层网络的访问 Kubernetes的认证授权体系 使用EFK构建集群业务应用的日志收集系统","title":""},{"content":"一、Docker是什么 使用最广泛的开源容器引擎 一种操作系统级的虚拟化技术 依赖于Linux内核特性：Namespace（资源隔离）和Cgroups（资源限制） 一个简单的应用程序打包工具 docker的安装docker的安装 安装依赖包 mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils 添加Docker软件包源 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装Docker CE yum install -y docker-ce 启动Docker服务并设置开机启动 systemctl start docker systemctl enable docker https://docs.docker.com/engine/install/centos/ 官方文档：https://docs.docker.com 阿里云源：http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n二、镜像是什么？ 一个分层存储的文件，不是一个单一的文件 一个软件的环境 一个镜像可以创建N个容器 一种标准化的交付 一个不包含Linux内核而又精简的Linux操作系统 镜像从哪里来 Docker Hub是由Docker公司负责维护的公共镜像仓库，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像。 地址：https://hub.docker.com 配置镜像加速器：\nvi /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://b9pmyelo.mirror.aliyuncs.com\u0026#34;] } 镜像相关命令 镜像加速器获取地址： 阿里云\u0026gt;控制台\u0026gt;容器镜像服务\u0026gt;镜像工具\u0026gt;镜像加速器 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors docker image docker export 9a495b209d08 -o tomcat.tar\n镜像存储核心技术：联合文件系统（UnionFS） 镜像怎么高效存储？ 难道像虚拟机那样一个容器对应一个独立的镜像文件？这样对于密集型容器，磁盘占用率太大！ 答：引入联合文件系统，将镜像多层文件联合挂载到容器文件系统 镜像存储核心技术：写时复制（COW） 了解联合文件系统后，我们知道，镜像是只读的，类似共享形式让多个容器使用，如果要在容器里修改文件，即镜像里的文件，那该怎么办呢？ 答：引入写时复制（copy-on-write），需要修改文件操作时，会先从镜像里把要写的文件复制到自己的文件系统中进行修改。\ndocker官网关于overlay的工作原理图： 详细解释参考：https://www.cnblogs.com/wdliu/p/10483252.html\n优化建议： •使用SSD固态硬盘 •使用卷作为频繁读写文件的工作目录，绕过存储驱动，减少抽象的开销\n三、Docker容器管理 创建容器常用命令 容器资源限制命令 示例： 内存限额： 允许容器最多使用500M内存和100M的Swap，并禁用OOM Killer：\ndocker run -d --name nginx03 --memory=\u0026#34;500m\u0026#34; --memory-swap=“600m\u0026#34; --oom-kill-disable nginx CPU限额： 允许容器最多使用一个半的CPU：\ndocker run -d --name nginx04 --cpus=\u0026#34;1.5\u0026#34; nginx 允许容器最多使用宿主机50%的CPU：\ndocker run -d --name nginx05 --cpus=\u0026#34;.5\u0026#34; nginx 查看docker性能状态\ndocker stats dockerid 容器管理常用命令 示例： 查看容器内进程\ndocker top tomcat 容器实现核心技术：Namespace Linux的Namespace机制提供了6种不同命名空间：\nIPC：隔离进程间通信 MOUNT：隔离文件系统挂载点 NET：隔离网络协议栈 PID：隔离进程号，进程命名空间是一个父子结构，子空间对父空间可见 USER：隔离用户 UTS：隔离主机名和域名 容器实现核心技术：CGroups Docker利用namespace实现了容器之间资源隔离，但是namespace不能对容器资源限制，比如CPU、内存。 如果某一个容器属于CPU密集型任务，那么会影响其他容器使用CPU，导致多个容器相互影响并且抢占资源。 如何对多个容器的资源使用进行限制就成了容器化的主要问题。 答：引入Control Groups（简称CGroups），限制容器资源\nCGroups：所有的任务就是运行在系统中的一个进程，而CGroups以某种标准将一组进程为目标进行资源分配和控制。例如CPU、内存、带宽等，并且可以动态配置。\nCGroups主要功能： •限制进程组使用的资源数量（Resourcelimitation）：可以为进程组设定资源使用上限，例如内存 •进程组优先级控制（Prioritization）：可以为进程组分配特定CPU、磁盘IO吞吐量 •记录进程组使用的资源数量（Accounting）：例如使用记录某个进程组使用的CPU时间 •进程组控制（Control）：可以将进程组挂起和恢复 DevOps\nls/sys/fs/cgroup/-l查看cgroups可控制的资源： •blkio ：对快设备的IO进行限制。 •cpu：限制CPU时间片的分配，与cpuacct挂载同一目录。 •cpuacct ：生成cgroup中的任务占用CPU资源的报告，与cpu挂载同一目录。 •cpuset ：给cgroup中的任务分配独立的CPU（多核处理器）和内存节点。 •devices ：允许或者拒绝cgroup 中的任务访问设备。 •freezer ：暂停/恢复cgroup 中的任务。 •hugetlb ：限制使用的内存页数量。 •memory ：对cgroup 中任务的可用内存进行限制，并自动生成资源占用报告。 •net_cls ：使用等级识别符（classid）标记网络数据包，这让Linux 流量控制程序（tc）可以识别来自特定从cgroup 任务的数据包，并进行网络限制。 •net_prio：允许基于cgroup设置网络流量的优先级。 •perf_event：允许使用perf工具来监控cgroup。 •pids：限制任务的数量。\n容器实际资源限制位置： /sys/fs/cgroup/\u0026lt;资源名\u0026gt;/docker/\u0026lt;容器ID\u0026gt;\nDocker核心组件之间关系 DockerDaemon：Docker守护进程，负责与DockerClinet交互，并管理镜像、容器。 Containerd：是一个简单的守护进程，向上给DockerDaemon提供接口，向下通过containerd-shim结合runC管理容器。 runC：一个命令行工具，它根据OCI标准来创建和运行容器。 总结: Namespace 命名空间，Linux内核提供的一种对进程资源隔离的机制，例如进程、网络、挂载点等资源。\nCGroups 控制组，Linux内核提供的一种对进程组限制资源的机制；例如CPU、内存等资源。\nUnionFS 联合文件系统，支持将不同位置的目录挂载到同一虚拟文件系统，形成一种分层的模型。\n四、容器数据持久化 将数据从宿主机挂载到容器中的三种方式 Docker提供三种方式将数据从宿主机挂载到容器中：\nvolumes：Docker管理宿主机文件系统的一部分（/var/lib/docker/volumes）。保存数据的最佳方式。 bind mounts：将宿主机上的任意位置的文件或者目录挂载到容器中。 tmpfs：挂载存储在主机系统的内存中，而不会写入主机的文件系统。如果不希望将数据持久存储在任何位置，可以使用tmpfs，同时避免写入容器可写层提高性能。 Volume 管理卷：\ndocker volume create nginx-vol docker volume ls docker volume inspect nginx-vol 用卷创建一个容器(两种方式，推荐用--mount)：\ndocker run -d --name=nginx-test --mount src=nginx-vol,dst=/usr/share/nginx/html nginx docker run -d --name=nginx-test -v nginx-vol:/usr/share/nginx/html nginx 清理：\ndocker stop nginx-test docker rm nginx-test docker volume rm nginx-vol 注意： 1.如果没有指定卷，会自动创建。\nBind Mounts 用卷创建一个容器：\ndocker run -d --name=nginx-test --mount type=bind,src=/app/wwwroot,dst=/usr/share/nginx/html nginx docker run -d --name=nginx-test -v /app/wwwroot:/usr/share/nginx/html nginx 验证绑定：\ndocker inspect nginx-test 清理：\ndocker stop nginx-test docker rm nginx-test 注意： 1.如果容器挂载目录中存在数据，使用bind mounts将宿主机上指定目录挂载到容器指定目录后，宿主机目录的文件会覆盖容器目录中的源文件，即只能看到宿主机目录中的文件，原有数据文件将看不到。\n五、Docker容器网络 四种网络模式 bridge –net=bridge 默认网络，Docker启动后创建一个docker0网桥，默认创建的容器也是添加到这个网桥中。 也可以自定义网络，相比默认的具备内部DNS发现，可以通过容器名容器之间网络通信。 host –net=host 容器不会获得一个独立的network namespace，而是与宿主机共用一个。这就意味着容器不会有自己的网卡信息，而是使用宿主机的。容器除了网络，其他都是隔离的。 none –net=none 获取独立的network namespace，但不为容器进行任何网络配置，需要我们手动配置。 container –net=container:Name/ID 与指定的容器使用同一个network namespace，具有同样的网络配置信息，两个容器除了网络，其他都还是隔离的。 Docker网络模型 veth pair：成对出现的一种虚拟网络设备，数据从一端进,从另一端出。用于解决网络命名空间之间隔离。 docker0：网桥是一个二层网络设备，通过网桥可以将Linux支持的不同的端口连接起来，并实现类似交换机那样的多对多的通信。 容器网络访问原理 容器网络实现核心技术：Iptables INPUT链：接收的数据包是本机(入站)时，应用此链中的规则。 OUTPUT链：本机向外发送数据包(出站)时，应用此链中的规则。 FORWARD链：需要通过防火墙中转发送给其他地址的数据包(转发)时，应用测链中的规则。 PREROUTING链：在对数据包做路由选择之前，应用此链中的规则。DNAT POSTROUTING链：在对数据包做路由选择之后，应用此链中的规则。SNAT docker实现跨主机网络通信 Flannel是CoreOS维护的一个网络组件，在每个主机上运行守护 进程负责维护本地路由转发，Flannel使用ETCD来存储容器网络 与主机之前的关系。 其他主流容器跨主机网络方案： • Weave • Calico • OpenvSwitch\n1、etcd安装并配置 yum install etcd -y vi /etc/etcd/etcd.conf ETCD_DATA_DIR=\u0026#34;/var/lib/etcd/default.etcd\u0026#34; ETCD_LISTEN_CLIENT_URLS=\u0026#34;http://192.168.31.73:2379\u0026#34; ETCD_NAME=\u0026#34;default\u0026#34; ETCD_ADVERTISE_CLIENT_URLS=\u0026#34;http://192.168.31.73:2379\u0026#34; 2、flanneld安装并配置 yum install flannel -y vi /etc/sysconfig/flanneld FLANNEL_ETCD_ENDPOINTS=\u0026#34;http://192.168.31.73:2379\u0026#34; FLANNEL_ETCD_PREFIX=\u0026#34;/atomic.io/network\u0026#34; 3、向etcd写入子网 etcdctl --endpoints=\u0026#34;http://192.168.31.73:2379\u0026#34; set /atomic.io/network/config \u0026#39;{ \u0026#34;Network\u0026#34;: \u0026#34;172.17.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: {\u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;}} \u0026#39; 4、配置Docker使用flannel生成的网络信息 vi /usr/lib/systemd/system/docker.service EnvironmentFile=/run/flannel/docker ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS 注：先启动falnned再启动docker.\n5、启动所有服务并设置开机启动： systemctl daemon-reload systemctl start xxx systemctl enable xxx 6、在两台主机创建容器相互ping验证 注：iptables FORWARD链规则设置允许\niptables -P FORWARD ACCEPT ","permalink":"https://wandong1.github.io/post/docker%E5%85%A5%E9%97%A8%E8%87%B3%E8%BF%9B%E9%98%B6/","summary":"一、Docker是什么 使用最广泛的开源容器引擎 一种操作系统级的虚拟化技术 依赖于Linux内核特性：Namespace（资源隔离）和Cgroups（资源限制） 一个简单的应用程序打包工具 docker的安装docker的安装 安装依赖包 mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils 添加Docker软件包源 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装Docker CE yum install -y docker-ce 启动Docker服务并设置开机启动 systemctl start docker systemctl enable docker https://docs.docker.com/engine/install/centos/ 官方文档：https://docs.docker.com 阿里云源：http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n二、镜像是什么？ 一个分层存储的文件，不是一个单一的文件 一个软件的环境 一个镜像可以创建N个容器 一种标准化的交付 一个不包含Linux内核而又精简的Linux操作系统 镜像从哪里来 Docker Hub是由Docker公司负责维护的公共镜像仓库，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像。 地址：https://hub.docker.com 配置镜像加速器：\nvi /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://b9pmyelo.mirror.aliyuncs.com\u0026#34;] } 镜像相关命令 镜像加速器获取地址： 阿里云\u0026gt;控制台\u0026gt;容器镜像服务\u0026gt;镜像工具\u0026gt;镜像加速器 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors docker image docker export 9a495b209d08 -o tomcat.tar","title":"docker入门至进阶"},{"content":"创建服务 docker volume create jenkins-data docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home jenkins/jenkins 查看首次登录密钥 docker logs jenkins # 或者 cat /var/jenkins_home/secrets/initialAdminPassword 修改国内插件源 # 进入jenkins_home/updates目录 sed -i \u0026#39;s/http:\\/\\/updates.jenkins.ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json 管理Jenkins-\u0026gt;系统配置\u0026ndash;\u0026gt;管理插件\u0026ndash;\u0026gt;分别搜索Git Parameter/Git/Pipeline/kubernetes/Config File Provider， 选中点击安装。\n• Git：拉取代码\n• Git Parameter：Git参数化构建\n• Pipeline：流水线\n• kubernetes：连接Kubernetes动态创建Slave代理\n• Config File Provider：存储配置文件\n• Extended Choice Parameter：扩展选择框参数，支持多选\npipeline { agent any stages { stage(\u0026#39;pull code\u0026#39;) { steps { git credentialsId: \u0026#39;fd53dd28-a24f-48ce-b6a3-edaefef0c61a\u0026#39;, url: \u0026#39;https://github.com/wandong1/wandong1.github.io.git\u0026#39; } } stage(\u0026#39;print hello\u0026#39;) { steps { sh \u0026#39;echo \u0026#34;hello world\u0026#34;\u0026#39; } } } } ","permalink":"https://wandong1.github.io/post/docker%E9%83%A8%E7%BD%B2jenkins/","summary":"创建服务 docker volume create jenkins-data docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home jenkins/jenkins 查看首次登录密钥 docker logs jenkins # 或者 cat /var/jenkins_home/secrets/initialAdminPassword 修改国内插件源 # 进入jenkins_home/updates目录 sed -i \u0026#39;s/http:\\/\\/updates.jenkins.ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json 管理Jenkins-\u0026gt;系统配置\u0026ndash;\u0026gt;管理插件\u0026ndash;\u0026gt;分别搜索Git Parameter/Git/Pipeline/kubernetes/Config File Provider， 选中点击安装。\n• Git：拉取代码\n• Git Parameter：Git参数化构建\n• Pipeline：流水线\n• kubernetes：连接Kubernetes动态创建Slave代理\n• Config File Provider：存储配置文件\n• Extended Choice Parameter：扩展选择框参数，支持多选\npipeline { agent any stages { stage(\u0026#39;pull code\u0026#39;) { steps { git credentialsId: \u0026#39;fd53dd28-a24f-48ce-b6a3-edaefef0c61a\u0026#39;, url: \u0026#39;https://github.","title":"docker部署jenkins"},{"content":"六、Dockerfile定制容器镜像 Dockerfile常用指令 docker build构建镜像 Usage: docker build [OPTIONS] PATH | URL | -[flags]Options: -t, \u0026ndash;tag list # 镜像名称 -f, \u0026ndash;file string # 指定Dockerfile文件位置\ndocker build -t shykes/myapp . docker build -t shykes/myapp -f /path/Dockerfile /path docker build -t shykes/myapp http://www.example.com/Dockerfile CMD与ENTRYPOINT区别 CMD用法： •CMD [“executable”,“param1”,“param2”] ：exec形式（首选） •CMD [“param1”,“param2”] ：作为ENTRYPOINT的默认参数 •CMD command param1 param2 ：Shell形式 ENTRYPOINT用法： •ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] •ENTRYPOINT command param1 param2\n小结： 1.CMD和ENTRYPOINT指令都可以用来定义运行容器时所使用的默认命令 2.Dockerfile至少指定一个CMD或ENTRYPOINT 3.CMD可以用作ENTRYPOINT默认参数，或者用作容器的默认命令 4.docker run指定时，将会覆盖CMD 5.如果是可执行文件，希望运行时传参，应该使用ENTRYPOINT\n构建镜像例子 构建tomcat镜像，需要apache-tomcat-8.5.43.tar.gz包 FROM centos:7 MAINTAINER www.ctnrs.com ENV VERSION=8.5.43 RUN yum install java-1.8.0-openjdk wget curl unzip iproute net-tools -y \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum/* ADD apache-tomcat-${VERSION}.tar.gz /usr/local/ RUN mv /usr/local/apache-tomcat-${VERSION} /usr/local/tomcat \u0026amp;\u0026amp; \\ sed -i \u0026#39;1a JAVA_OPTS=\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;\u0026#39; /usr/local/tomcat/bin/catalina.sh \u0026amp;\u0026amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ENV PATH $PATH:/usr/local/tomcat/bin WORKDIR /usr/local/tomcat EXPOSE 8080 CMD [\u0026#34;catalina.sh\u0026#34;, \u0026#34;run\u0026#34;] 构建nginx镜像，需要nginx-1.15.5.tar.gz 和nginx.conf FROM centos:7 LABEL maintainer www.ctnrs.com RUN yum install -y gcc gcc-c++ make \\ openssl-devel pcre-devel gd-devel \\ iproute net-tools telnet wget curl \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum/* ADD nginx-1.15.5.tar.gz / RUN cd nginx-1.15.5 \u0026amp;\u0026amp; \\ ./configure --prefix=/usr/local/nginx \\ --with-http_ssl_module \\ --with-http_stub_status_module \u0026amp;\u0026amp; \\ make -j 4 \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; \\ mkdir /usr/local/nginx/conf/vhost \u0026amp;\u0026amp; \\ cd / \u0026amp;\u0026amp; rm -rf nginx* \u0026amp;\u0026amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ENV PATH $PATH:/usr/local/nginx/sbin COPY nginx.conf /usr/local/nginx/conf/nginx.conf WORKDIR /usr/local/nginx EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] JAVA微服务镜像构建：Jar JAVA_OPTS jvm参数，jar包 FROM java:8-jdk-alpine LABEL maintainer www.ctnrs.com ENV JAVA_OPTS=\u0026#34;$JAVA_OPTS -Dfile.encoding=UTF8 -Duser.timezone=GMT+08\u0026#34; RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; \\ apk add -U tzdata \u0026amp;\u0026amp; \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime COPY hello.jar / EXPOSE 8888 CMD [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java -jar $JAVA_OPTS /hello.jar\u0026#34;] 案例：容器化搭建个人博客系统 1、自定义网络\ndocker network create lnmp 2、创建Mysql容器\ndocker run -d \\ --name lnmp_mysql \\ --net lnmp \\ --mount src=mysql-vol,dst=/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress \\ mysql:5.7 --character-set-server=utf8 3、创建PHP容器\ndocker run -d --name lnmp_php --net lnmp \\ --mount src=wwwroot,dst=/wwwroot php:v1 4、创建Nginx容器\ndocker run -d --name lnmp_nginx --net lnmp -p 88:80 --mount src=wwwroot,dst=/wwwroot \\ --mount type=bind,src=$PWD/php.conf,dst=/usr/local/nginx/conf/vhost/php.conf nginx:v1 5、以wordpress博客为例 https://cn.wordpress.org/wordpress-4.9.4-zh_CN.tar.gz 将wordpress解压到网站根目录，即wwwroot存储卷\ntar -xzvf wordpress-4.9.4-zh_CN.tar.gz -C /var/lib/docker/volumes/wwwroot/_data/ cd /var/lib/docker/volumes/wwwroot/_data/wordpress;mv * ../ cd ..;rm -rf wordpress 即可正常访问wordpress,进入配置，数据库连接写数据库主机名 lnmp_mysql 因为在同一个新建的网络下面，主机名可以直接解析。 数据库账号密码写创建数据库时指定的账号密码。\n","permalink":"https://wandong1.github.io/post/docker%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/","summary":"六、Dockerfile定制容器镜像 Dockerfile常用指令 docker build构建镜像 Usage: docker build [OPTIONS] PATH | URL | -[flags]Options: -t, \u0026ndash;tag list # 镜像名称 -f, \u0026ndash;file string # 指定Dockerfile文件位置\ndocker build -t shykes/myapp . docker build -t shykes/myapp -f /path/Dockerfile /path docker build -t shykes/myapp http://www.example.com/Dockerfile CMD与ENTRYPOINT区别 CMD用法： •CMD [“executable”,“param1”,“param2”] ：exec形式（首选） •CMD [“param1”,“param2”] ：作为ENTRYPOINT的默认参数 •CMD command param1 param2 ：Shell形式 ENTRYPOINT用法： •ENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] •ENTRYPOINT command param1 param2\n小结： 1.CMD和ENTRYPOINT指令都可以用来定义运行容器时所使用的默认命令 2.Dockerfile至少指定一个CMD或ENTRYPOINT 3.CMD可以用作ENTRYPOINT默认参数，或者用作容器的默认命令 4.docker run指定时，将会覆盖CMD 5.如果是可执行文件，希望运行时传参，应该使用ENTRYPOINT\n构建镜像例子 构建tomcat镜像，需要apache-tomcat-8.","title":"docker镜像制作"},{"content":"expect 安装 yum install -y expect 免交互切root用户 #!/usr/bin/expect set passwd \u0026#34;123456\u0026#34; set timeout 10 spawn su - expect { \u0026#34;Password:\u0026#34; { send \u0026#34;$passwd\\r\u0026#34; } } interact 免交互ssh登录节点 #!/usr/bin/expect set timeout 10 set passwd \u0026#34;123456\u0026#34; spawn ssh wanye@172.17.0.17 -p 22 expect { \u0026#34;yes/no\u0026#34; { send \u0026#34;yes\\r\u0026#34;;exp_continue } \u0026#34;password:\u0026#34; { send \u0026#34;$passwd\\r\u0026#34; } } interact ","permalink":"https://wandong1.github.io/post/expect%E5%AE%9E%E7%8E%B0%E5%85%8D%E4%BA%A4%E4%BA%92%E7%99%BB%E5%BD%95/","summary":"expect 安装 yum install -y expect 免交互切root用户 #!/usr/bin/expect set passwd \u0026#34;123456\u0026#34; set timeout 10 spawn su - expect { \u0026#34;Password:\u0026#34; { send \u0026#34;$passwd\\r\u0026#34; } } interact 免交互ssh登录节点 #!/usr/bin/expect set timeout 10 set passwd \u0026#34;123456\u0026#34; spawn ssh wanye@172.17.0.17 -p 22 expect { \u0026#34;yes/no\u0026#34; { send \u0026#34;yes\\r\u0026#34;;exp_continue } \u0026#34;password:\u0026#34; { send \u0026#34;$passwd\\r\u0026#34; } } interact ","title":"expect实现免交互登录"},{"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment\n","permalink":"https://wandong1.github.io/post/hello-world/","summary":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment","title":"Hello World"},{"content":"使用keepalived实现vip的动态飘逸 keepalived.conf 主\n! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script check_nginx { script \u0026#34;/etc/keepalived/check_nginx.sh\u0026#34; } vrrp_instance VI_1{ state MASTER interface ens34 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_tpye PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.59/24 } track_script { check_nginx } } keepalived.conf 备\n! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script check_nginx { script \u0026#34;/etc/keepalived/check_nginx.sh\u0026#34; } vrrp_instance VI_1{ state BACKUP interface ens34 virtual_router_id 51 priority 90 advert_int 1 authentication { auth_tpye PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.59/24 } track_script { check_nginx } } 检查脚本 /etc/keepalived/check_nginx.sh #!/bin/bash count=$(ps -ef | grep nginx | egrep -cv \u0026#34;grep|$$\u0026#34;) if [ $count -eq 0 ];then systemctl stop keepalived fi nginx负载均衡4层代理配置 nginx upstream 模块： # For more information on configuration, see: # * Official English Documentation: http://nginx.org/en/docs/ # * Official Russian Documentation: http://nginx.org/ru/docs/ user nginx; worker_processes auto; error_log /var/log/nginx/error.log; pid /run/nginx.pid; # Load dynamic modules. See /usr/share/doc/nginx/README.dynamic. include /usr/share/nginx/modules/*.conf; events { worker_connections 1024; } stream { log_format main \u0026#34;$remote_addr $upstream_addr $time_local $status\u0026#34;; access_log /var/log/nginx/k8s-access.log main; upstream k8s-apiserver { server 192.168.200.62:6443; server 192.168.200.63:6443; } server { listen 0.0.0.0:6443; proxy_pass k8s-apiserver; } } http { log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; # Settings for a TLS enabled server. # # server { # listen 443 ssl http2 default_server; # listen [::]:443 ssl http2 default_server; # server_name _; # root /usr/share/nginx/html; # # ssl_certificate \u0026#34;/etc/pki/nginx/server.crt\u0026#34;; # ssl_certificate_key \u0026#34;/etc/pki/nginx/private/server.key\u0026#34;; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 10m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # # # Load configuration files for the default server block. # include /etc/nginx/default.d/*.conf; # # location / { # } # # error_page 404 /404.html; # location = /40x.html { # } # # error_page 500 502 503 504 /50x.html; # location = /50x.html { # } # } } ","permalink":"https://wandong1.github.io/post/k8s%E9%9B%86%E7%BE%A4%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1keepalived+nginx%E5%AE%9E%E7%8E%B0/","summary":"使用keepalived实现vip的动态飘逸 keepalived.conf 主\n! Configuration File for keepalived global_defs { notification_email { acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script check_nginx { script \u0026#34;/etc/keepalived/check_nginx.sh\u0026#34; } vrrp_instance VI_1{ state MASTER interface ens34 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_tpye PASS auth_pass 1111 } virtual_ipaddress { 192.168.200.59/24 } track_script { check_nginx } } keepalived.conf 备\n! Configuration File for keepalived global_defs { notification_email { acassen@firewall.","title":"k8s集群负载均衡keepalived+nginx实现"},{"content":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n# 创建一个 Master 节点\r$ kubeadm init\r# 将一个 Node 节点加入到当前集群中\r$ kubeadm join \u0026lt;Master节点的IP和端口 \u0026gt; 1. 安装要求 在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区 2. 准备环境 角色 IP k8s-master 192.168.31.61 k8s-node1 192.168.31.62 k8s-node2 192.168.31.63 关闭防火墙：\r$ systemctl stop firewalld\r$ systemctl disable firewalld\r关闭selinux：\r$ sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久\r$ setenforce 0 # 临时\r关闭swap：\r$ swapoff -a # 临时\r$ vim /etc/fstab # 永久\r设置主机名：\r$ hostnamectl set-hostname \u0026lt;hostname\u0026gt;\r在master添加hosts：\r$ cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF\r192.168.31.61 k8s-master\r192.168.31.62 k8s-node1\r192.168.31.63 k8s-node2\rEOF\r将桥接的IPv4流量传递到iptables的链：\r$ cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF\rnet.bridge.bridge-nf-call-ip6tables = 1\rnet.bridge.bridge-nf-call-iptables = 1\rEOF\r$ sysctl --system # 生效\r时间同步：\r$ yum install ntpdate -y\r$ ntpdate time.windows.com 3. 安装Docker/kubeadm/kubelet【所有节点】 Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker。\n3.1 安装Docker $ wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\r$ yum -y install docker-ce\r$ systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 配置镜像下载加速器：\n$ cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF\r{\r\u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://b9pmyelo.mirror.aliyuncs.com\u0026#34;]\r}\rEOF\r$ systemctl restart docker\r$ docker info 3.2 添加阿里云YUM软件源 $ cat \u0026gt; /etc/yum.repos.d/kubernetes.repo \u0026lt;\u0026lt; EOF\r[kubernetes]\rname=Kubernetes\rbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\renabled=1\rgpgcheck=0\rrepo_gpgcheck=0\rgpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\rEOF 3.3 安装kubeadm，kubelet和kubectl 由于版本更新频繁，这里指定版本号部署：\n$ yum install -y kubelet-1.19.0 kubeadm-1.19.0 kubectl-1.19.0\r$ systemctl enable kubelet 4. 部署Kubernetes Master https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#initializing-your-control-plane-node\n在192.168.31.61（Master）执行。\n$ kubeadm init \\\r--apiserver-advertise-address=192.168.31.61 \\\r--image-repository registry.aliyuncs.com/google_containers \\\r--kubernetes-version v1.19.0 \\\r--service-cidr=10.96.0.0/12 \\\r--pod-network-cidr=10.244.0.0/16 \\\r--ignore-preflight-errors=all \u0026ndash;apiserver-advertise-address 集群通告地址 \u0026ndash;image-repository 由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址 \u0026ndash;kubernetes-version K8s版本，与上面安装的一致 \u0026ndash;service-cidr 集群内部虚拟网络，Pod统一访问入口 \u0026ndash;pod-network-cidr Pod网络，，与下面部署的CNI网络组件yaml中保持一致 或者使用配置文件引导：\n$ vi kubeadm.conf\rapiVersion: kubeadm.k8s.io/v1beta2\rkind: ClusterConfiguration\rkubernetesVersion: v1.18.0\rimageRepository: registry.aliyuncs.com/google_containers networking:\rpodSubnet: 10.244.0.0/16 serviceSubnet: 10.96.0.0/12 $ kubeadm init --config kubeadm.conf --ignore-preflight-errors=all kubeadm init初始化工作： 1、[preflight] 环境检查和拉取镜像 kubeadm config images pull 2、[certs] 生成k8s证书和etcd证书 /etc/kubernetes/pki 3、[kubeconfig] 生成kubeconfig文件 4、[kubelet-start] 生成kubelet配置文件 5、[control-plane] 部署管理节点组件，用镜像启动容器 kubectl get pods -n kube-system 6、[etcd] 部署etcd数据库，用镜像启动容器 7、[upload-config] [kubelet] [upload-certs] 上传配置文件到k8s中 8、[mark-control-plane] 给管理节点添加一个标签 node-role.kubernetes.io/master=\u0026rsquo;\u0026rsquo;，再添加一个污点[node-role.kubernetes.io/master:NoSchedule] 9、[bootstrap-token] 自动为kubelet颁发证书 10、[addons] 部署插件，CoreDNS、kube-proxy\n拷贝kubectl使用的连接k8s认证文件到默认路径：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config $ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 2m v1.18.0 5. 加入Kubernetes Node 在192.168.31.62/63（Node）执行。\n向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：\n$ kubeadm join 192.168.31.61:6443 --token esce21.q6hetwm8si29qxwn \\\r--discovery-token-ca-cert-hash sha256:00603a05805807501d7181c3d60b478788408cfe6cedefedb1f97569708be9c5 默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：\n$ kubeadm token create\r$ kubeadm token list\r$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39;\r63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924\r$ kubeadm join 192.168.31.61:6443 --token nuja6n.o3jrhsffiqs9swnu --discovery-token-ca-cert-hash sha256:63bca849e0e01691ae14eab449570284f0c3ddeea590f8da988c07fe2729e924 或者直接命令快捷生成：kubeadm token create \u0026ndash;print-join-command\nhttps://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/\n6. 部署容器网络（CNI） https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network\n注意：只需要部署下面其中一个，推荐Calico。\nCalico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。\nCalico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。\n此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。\nhttps://docs.projectcalico.org/getting-started/kubernetes/quickstart\n$ wget https://docs.projectcalico.org/manifests/calico.yaml 下载完后还需要修改里面定义Pod网络（CALICO_IPV4POOL_CIDR），与前面kubeadm init指定的一样\n修改完后应用清单：\n$ kubectl apply -f calico.yaml\r$ kubectl get pods -n kube-system 7. 测试kubernetes集群 验证Pod工作 验证Pod网络通信 验证DNS解析 在Kubernetes集群中创建一个pod，验证是否正常运行：\n$ kubectl create deployment nginx --image=nginx\r$ kubectl expose deployment nginx --port=80 --type=NodePort\r$ kubectl get pod,svc 访问地址：http://NodeIP:Port\n8. 部署 Dashboard $ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml 默认Dashboard只能集群内部访问，修改Service为NodePort类型，暴露到外部：\n$ vi recommended.yaml\r...\rkind: Service\rapiVersion: v1\rmetadata:\rlabels:\rk8s-app: kubernetes-dashboard\rname: kubernetes-dashboard\rnamespace: kubernetes-dashboard\rspec:\rports:\r- port: 443\rtargetPort: 8443\rnodePort: 30001\rselector:\rk8s-app: kubernetes-dashboard\rtype: NodePort\r...\r$ kubectl apply -f recommended.yaml\r$ kubectl get pods -n kubernetes-dashboard\rNAME READY STATUS RESTARTS AGE\rdashboard-metrics-scraper-6b4884c9d5-gl8nr 1/1 Running 0 13m\rkubernetes-dashboard-7f99b75bf4-89cds 1/1 Running 0 13m 访问地址：https://NodeIP:30001\n创建service account并绑定默认cluster-admin管理员集群角色：\n# 创建用户\r$ kubectl create serviceaccount dashboard-admin -n kube-system\r# 用户授权\r$ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin\r# 获取用户Token\r$ kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk \u0026#39;/dashboard-admin/{print $1}\u0026#39;) 使用输出的token登录Dashboard。\n解决办法 1.集群状态不正常 /etc/kubernetes/manifests/kube-controller-manager.yaml /etc/kubernetes/manifests/kube-scheduler.yaml 注释 - \u0026ndash;port=0 这一行 2.node节点使用kubectl命令，需要知道kubeconfig配置文件。kubeadm安装的 通常在/etc/kubernetes/admin.conf。拷贝一份即可。 两种方式 A.使用kubectl命令是指定配置文件。\nkubectl get node --kubeconfig=admin.conf B.将kubeconfig配置文件放到$HOME/.kube/目录并重命名为config。然后赋予足够权限。\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get node ","permalink":"https://wandong1.github.io/post/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","summary":"kubeadm是官方社区推出的一个用于快速部署kubernetes集群的工具。\n这个工具能通过两条指令完成一个kubernetes集群的部署：\n# 创建一个 Master 节点\r$ kubeadm init\r# 将一个 Node 节点加入到当前集群中\r$ kubeadm join \u0026lt;Master节点的IP和端口 \u0026gt; 1. 安装要求 在开始之前，部署Kubernetes集群机器需要满足以下几个条件：\n一台或多台机器，操作系统 CentOS7.x-86_x64 硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap分区 2. 准备环境 角色 IP k8s-master 192.168.31.61 k8s-node1 192.168.31.62 k8s-node2 192.168.31.63 关闭防火墙：\r$ systemctl stop firewalld\r$ systemctl disable firewalld\r关闭selinux：\r$ sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config # 永久\r$ setenforce 0 # 临时\r关闭swap：\r$ swapoff -a # 临时\r$ vim /etc/fstab # 永久\r设置主机名：\r$ hostnamectl set-hostname \u0026lt;hostname\u0026gt;\r在master添加hosts：\r$ cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF\r192.","title":"kubeadm安装K8S集群"},{"content":"一、kubectl命令补全 yum install -y bash-completion #临时生效\nsource /usr/share/bash-completion/bash_completion #永久生效\nsource \u0026lt;(kubectl completion bash) echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc ","permalink":"https://wandong1.github.io/post/kubectl%E5%91%BD%E4%BB%A4%E8%A1%A5%E5%85%A8/","summary":"一、kubectl命令补全 yum install -y bash-completion #临时生效\nsource /usr/share/bash-completion/bash_completion #永久生效\nsource \u0026lt;(kubectl completion bash) echo \u0026#34;source \u0026lt;(kubectl completion bash)\u0026#34; \u0026gt;\u0026gt; ~/.bashrc ","title":"kubectl命令补全"},{"content":"kubectl管理命令概要 配置命令补全： http://123.207.224.155:8080/post/60\n体验 使用Deployment控制器部署镜像：\nkubectl create deployment web --image=lizhenliang/java-demo kubectl get deploy,pods 使用Service将Pod暴露出去：\nkubectl expose deployment web --port=80 --target-port=8080 --type=NodePort kubectl get service 访问应用： http://NodeIP:Port # 端口随机生成，通过get svc获取\n查看资源并显示标签\nkubectl get pods --show-labels 根据查看带有指定标签名字的资源\nkubectl get pod -l app=web https://kubernetes.io/docs 官方资料查询\nYAML字段记忆技巧 用create命令生成 kubectl create deployment nginx --image=nginx:1.16 -o yaml --dry-run=client \u0026gt; my-deploy.yaml 用get命令导出 kubectl get deploymentnginx-o yaml \u0026gt; my-deploy.yaml Pod容器的字段拼写忘记了 kubectl explain pods.spec.containers kubectl explain deployment ","permalink":"https://wandong1.github.io/post/kubectl%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%E6%A6%82%E8%A6%81/","summary":"kubectl管理命令概要 配置命令补全： http://123.207.224.155:8080/post/60\n体验 使用Deployment控制器部署镜像：\nkubectl create deployment web --image=lizhenliang/java-demo kubectl get deploy,pods 使用Service将Pod暴露出去：\nkubectl expose deployment web --port=80 --target-port=8080 --type=NodePort kubectl get service 访问应用： http://NodeIP:Port # 端口随机生成，通过get svc获取\n查看资源并显示标签\nkubectl get pods --show-labels 根据查看带有指定标签名字的资源\nkubectl get pod -l app=web https://kubernetes.io/docs 官方资料查询\nYAML字段记忆技巧 用create命令生成 kubectl create deployment nginx --image=nginx:1.16 -o yaml --dry-run=client \u0026gt; my-deploy.yaml 用get命令导出 kubectl get deploymentnginx-o yaml \u0026gt; my-deploy.yaml Pod容器的字段拼写忘记了 kubectl explain pods.spec.containers kubectl explain deployment ","title":"kubectl管理命令概要"},{"content":"扩容硬盘后进行如下操作。\nfdisk /dev/vda1 确认只有一个vda1分区的情况下，按d删除分区，然后按n重新创建vda1分区，指定更大范围的sectors,且覆盖原有的sectors。 重启机器，后 resize2fs /dev/vda1\nhttps://help.aliyun.com/document_detail/113316.html\n","permalink":"https://wandong1.github.io/post/linux%E7%B3%BB%E7%BB%9F%E6%A0%B9%E5%88%86%E5%8C%BA%E6%89%A9%E5%AE%B9/","summary":"扩容硬盘后进行如下操作。\nfdisk /dev/vda1 确认只有一个vda1分区的情况下，按d删除分区，然后按n重新创建vda1分区，指定更大范围的sectors,且覆盖原有的sectors。 重启机器，后 resize2fs /dev/vda1\nhttps://help.aliyun.com/document_detail/113316.html","title":"linux系统根分区扩容"},{"content":"uptime 查看负载高 uptime uptime 命令用于查看服务器运行了多长时间以及有多少个用户登录，快速获知服务器的负荷情况。\nuptime 的输出包含一项内容是 load average，显示了最近 1，5，15 分钟的负荷情况。它的值代表等待CPU 处理的进程数，如果 CPU 没有时间处理这些进程，load average 值会升高；反之则会降低。load average的最佳值是 1，说明每个进程都可以马上处理并且没有CPU cycles 被丢失。对于单 CPU 的机器，1 或者2 是可以接受的值；对于多路 CPU 的机器，load average 值可能在 8 到 10 之间。也可以使用 uptime 命令来判断网络性能。例如，某个网络应用性能很低，通过运行uptime 查看服务器的负荷是否很高，如果不是，那么问题应该是网络方面造成的。\n以下是 uptime 的运行实例： 9:24am up 19:06, 1 user, load average: 0.00, 0.00, 0.00 也可以查看/proc/loadavg 和/proc/uptime 两个文件，注意不能编辑/proc 中的文件，要用 cat 等命令来查看，如：\ncat /proc/loadavg 0.0 0.00 0.00 1/55 5505 uptime 命令用法十分简单：直接输入\nuptime 例： 18:02:41 up 41 days, 23:42,\t1 user,\tload average: 0.00, 0.00, 0.00\n1 可以被认为是最优的负载值。负载是会随着系统不同改变得。单 CPU 系统 1-3 和 SMP 系统 6-10 都是可能接受的。 另外还有一个参数\t-V ，是用来查询版本的。 (注意是大写的字母 v) [linux @ localhost]$ uptime -V procps version 3.2.7 [linux @ localhost]$ uptime 显示结果为： 10:19:04 up 257 days, 18:56,\t12 users,\tload average: 2.10, 2.10,2.09 显示内容说明： 10:19:04\t//系统当前时间 up 257 days, 18:56\t//主机已运行时间,时间越大，说明你的机器越稳定。 12 user\t//用户连接数，是总连接数而不是用户数 load average\t// 系统平均负载，统计最近 1，5，15 分钟的系统平均负载 那么什么是系统平均负载呢？ 系统平均负载是指在特定时间间隔内运行队列中的平均进程数。如果每个 CPU 内核的当前活动进程数不大于 3 的话，那么系统的性能是良好的。如果每个 CPU 内核的任务数大于 5，那么这台机器的性能有严重问题。如果你的 linux 主机是 1 个双核 CPU 的话，当 Load Average为 6 的时候说明机器已经被充分使用了。 5、W w 命令主要是查看当前登录的用户，这个命令相对来说比较简单。我们来看一下截图。 在上面这个截图里面呢，第一列user，代表登录的用户，第二列，tty 代表用户登录的终端号，因为在linux 中并不是只有一个终端的，pts/2 代表是图形界面的第二个终端（这仅是个人意见，网上的对pts 的看法可能有些争议)。第三列 FROM 代表登录的地方，如果是远程登录的，会显示 ip 地址，:0 表示的是 display 0:0，意思就是主控制台的第一个虚拟终端。第四列 login@代表登录的时间，第五列的 IDLE 代表系统的闲置时间。最后一列what 是代表正在运行的进程，因为我正在运行 w 命令，所以咋root 显示w。\niostat 用途：报告中央处理器（CPU）统计信息和整个系统、适配器、tty 设备、磁盘和 CD-ROM 的输入／ 输出统计信息。 语法：iostat [ -c | -d ] [ -k ] [ -t | -m ] [ -V ] [ -x [ device ] ] [ interval [ count ] ] 描述：iostat 命令用来监视系统输入／输出设备负载，这通过观察与它们的平均传送速率相关的物理磁盘的活动时间来实现。iostat 命令生成的报告可以用来更改系统配置来更好地平衡物理磁盘和适配器之间的输入／输出负载。 参数：-c 为汇报 CPU 的使用情况；-d 为汇报磁盘的使用情况；-k 表示每秒按 kilobytes 字节显示数据； -m 表示每秒按 M 字节显示数据；-t 为打印汇报的时间；-v 表示打印出版本信息和用法；-x device 指定要统计的设备名称，默认为所有的设备；interval 指每次统计间隔的时间；count 指按照这个时间间隔统计的次数。 iostat 结果解析 rrqm/s: 每秒进行 merge 的读操作数目。即 delta(rmerge)/s wrqm/s: 每秒进行 merge 的写操作数目。即 delta(wmerge)/s r/s: 每秒完成的读 I/O 设备次数。即 delta(rio)/s w/s: 每秒完成的写 I/O 设备次数。即 delta(wio)/s rsec/s: 每秒读扇区数。即 delta(rsect)/s\nwsec/s: 每秒写扇区数。即 delta(wsect)/s rkB/s: 每秒读 K 字节数。是 rsect/s 的一半，因为每扇区大小为 512 字节。 wkB/s: 每秒写 K 字节数。是 wsect/s 的一半。 avgrq-sz: 平均每次设备 I/O 操作的数据大小 (扇区)。即 delta(rsect+wsect)/delta(rio+wio) avgqu-sz: 平均 I/O 队列长度。即 delta(aveq)/s/1000 (因为 aveq 的单位为毫秒)。 await: 平均每次设备 I/O 操作的等待时间 (毫秒)。即 delta(ruse+wuse)/delta(rio+wio) svctm: 平均每次设备 I/O 操作的服务时间 (毫秒)。即 delta(use)/delta(rio+wio) %util: 一秒中有百分之多少的时间用于 I/O 操作，或者说一秒中有多少时间 I/O 队列是非空的。即 delta(use)/s/1000 (因为 use 的单位为毫秒) 如果 %util 接近 100%，说明产生的 I/O 请求太多，I/O 系统已经满负荷，该磁盘可能存在瓶颈。比较重要的参数 %util:\t一秒中有百分之多少的时间用于 I/O 操作，或者说一秒中有多少时间 I/O 队列是非空的 svctm:\t平均每次设备 I/O 操作的服务时间await:\t平均每次设备 I/O 操作的等待时间avgqu-sz: 平均 I/O 队列长度 如果%util 接近 100%,表明 i/o 请求太多,i/o 系统已经满负荷,磁盘可能存在瓶颈,一般%util 大于 70%,i/o 压力就比较大,读取速度有较多的wait.同时可以结合 vmstat 查看查看b 参数(等待资源的进程数)和wa 参数(IO 等待所占用的 CPU 时间的百分比,高过 30%时 IO 压力高)。\n要理解这些性能指标我们先看下图 IO 的执行过程的各个参数 上图的左边是 iostat 显示的各个性能指标，每个性能指标都会显示在一条虚线之上，这表明这个性能指标是从虚线之上的那个读写阶段开始计量的，比如说图中的 w/s 从Linux IO scheduler 开始穿过硬盘控制器(CCIS/3ware)，这就表明w/s 统计的是每秒钟从Linux IO scheduler 通过硬盘控制器的写IO 的数量。结合上图对读 IO 操作的过程做一个说明，在从 OS Buffer Cache 传入到 OS Kernel(Linux IO scheduler) 的读 IO 操作的个数实际上是 rrqm/s+r/s，直到读 IO 请求到达 OS Kernel 层之后，有每秒钟有 rrqm/s 个读 IO 操作被合并，最终转送给磁盘控制器的每秒钟读 IO 的个数为 r/w；在进入到操作系统的设备层(/dev/sda)之后，计数器开始对 IO 操作进行计时，最终的计算结果表现是 await，这个值就是我们要的IO 响应时间了；svctm 是在 IO 操作进入到磁盘控制器之后直到磁盘控制器返回结果所花费的时间，这是一个实际 IO 操作所花的时间，当 await 与 svctm 相差很大的时候，我们就要注意磁盘的 IO 性能了； 而 avgrq-sz 是从 OS Kernel 往下传递请求时单个 IO 的大小，avgqu-sz 则是在 OS Kernel 中 IO 请求队列的平均大小。 现在我们可以将 iostat 输出结果和我们上面讨论的指标挂钩了设备 IO 操作:总 IO(io)/s = r/s(读) +w/s(写) =1.46 + 25.28=26.74 平均每次设备I/O 操作只需要0.36 毫秒完成,现在却需要10.57 毫秒完成，因为发出的请求太多(每秒26.74 个)，假如请求时同时发出的，可以这样计算平均等待时间: 平均等待时间=单个 I/O 服务器时间*(1+2+\u0026hellip;+请求总数-1)/请求总数 每秒发出的 I/0 请求很多,但是平均队列就 4,表示这些请求比较均匀,大部分处理还是比较及时 svctm 一般要小于 await (因为同时等待的请求的等待时间被重复计算了)，svctm 的大小一般和磁盘性能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致 svctm 的增加。await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await， 说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。 队列长度(avgqu-sz)也可作为衡量系统 I/O 负荷的指标，但由于 avgqu-sz 是按照单位时间的平均值，所以不能反映瞬间的 I/O 洪水。 I/O 系统 vs. 超市排队 举一个例子，我们在超市排队 checkout 时，怎么决定该去哪个交款台呢? 首当是看排的队人数，5 个人总比 20 人要快吧? 除了数人头，我们也常常看看前面人购买的东西多少，如果前面有个采购了一星期食品的大妈，那么可以考虑换个队排了。还有就是收银员的速度了，如果碰上了连钱都点不清楚的新手，那就有的 等了。另外，时机也很重要，可能 5 分钟前还人满为患的收款台，现在已是人去楼空，这时候交款可是很爽啊，当然，前提是那过去的 5 分钟里所做的事情比排队要有意义 (不过我还没发现什么事情比排队还无聊的)。 I/O 系统也和超市排队有很多类似之处: r/s+w/s 类似于交款人的总数 平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数平均服务时间(svctm)类似于收银员的收款速度 平均等待时间(await)类似于平均每人的等待时间 平均 I/O 数据(avgrq-sz)类似于平均每人所买的东西多少 I/O 操作率 (％util)类似于收款台前有人排队的时间比例。 我们可以根据这些数据分析出 I/O 请求的模式，以及 I/O 的速度和响应时间。一个例子\niostat -x 1 avg-cpu: ％user ％nice ％sys ％idle 16.24 0.00 4.31 79.44 Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s rkB/s wkB/s avgrq-sz avgqu-sz await svctm ％util /dev/cciss/c0d0 0.00 44.90 1.02 27.55 8.16 579.59 4.08 289.80 20.57 22.35 78.21 5.00 14.29 /dev/cciss/c0d0p1 0.00 44.90 1.02 27.55 8.16 579.59 4.08 289.80 20.57 22.35 78.21 5.00 14.29 /dev/cciss/c0d0p2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 上面的 iostat 输出表明秒有 28.57 次设备 I/O 操作: delta(io)/s = r/s +w/s = 1.02+27.55 = 28.57 (次/秒) 其中写操作占了主体 (w:r = 27:1)。 平均每次设备 I/O 操作只需要 5ms 就可以完成，但每个 I/O 请求却需要等上 78ms，为什么? 因为发出的 I/O 请求太多 (每秒钟约 29 个)，假设这些请求是同时发出的，那么平均等待时间可以这样计算: 平均等待时间 = 单个 I/O 服务时间 * ( 1 + 2 + \u0026hellip; + 请求总数-1) / 请求总数 应用到上面的例子: 平均等待时间 = 5ms * (1+2+\u0026hellip;+28)/29 = 70ms，和 iostat 给出的 78ms 的平均等待时间很接近。这反过来表明 I/O 是同时发起的。每秒发出的 I/O 请求很多 (约 29 个)，平均队列却不长 (只有 2 个 左右)，这表明这 29 个请求的到来并不均匀，大部分时间 I/O 是空闲的。一秒中有14.29％ 的时间 I/O 队列中是有请求的，也就是说，85.71％ 的时间里 I/O 系统无事可做，所有 29 个I/O 请求都在 142 毫秒之内处理掉了。 delta(ruse+wuse)/delta(io) = await = 78.21 =\u0026gt; delta(ruse+wuse)/s =78.21 * delta(io)/s = 78.21*28.57 = 2232.8， 表明每秒内的 I/O 请求总共需要等待 2232.8ms。所以平均队列长度应为 2232.8ms/1000ms = 2.23，而iostat 给出的平均队列长度 (avgqu-sz) 却为 22.35，为什么?! 因为 iostat 中有 bug，avgqu-sz 值应为 2.23， 而不是 22.35。\nIO问题导致的负载变高排查方法： https://www.cnblogs.com/ccku/p/13938489.html\niotop -oP 查看有io操作的进程 iotop -b -n 3 -d 5\n","permalink":"https://wandong1.github.io/post/linux%E7%B3%BB%E7%BB%9F%E8%B4%9F%E8%BD%BD%E9%AB%98%E6%8E%92%E6%9F%A5/","summary":"uptime 查看负载高 uptime uptime 命令用于查看服务器运行了多长时间以及有多少个用户登录，快速获知服务器的负荷情况。\nuptime 的输出包含一项内容是 load average，显示了最近 1，5，15 分钟的负荷情况。它的值代表等待CPU 处理的进程数，如果 CPU 没有时间处理这些进程，load average 值会升高；反之则会降低。load average的最佳值是 1，说明每个进程都可以马上处理并且没有CPU cycles 被丢失。对于单 CPU 的机器，1 或者2 是可以接受的值；对于多路 CPU 的机器，load average 值可能在 8 到 10 之间。也可以使用 uptime 命令来判断网络性能。例如，某个网络应用性能很低，通过运行uptime 查看服务器的负荷是否很高，如果不是，那么问题应该是网络方面造成的。\n以下是 uptime 的运行实例： 9:24am up 19:06, 1 user, load average: 0.00, 0.00, 0.00 也可以查看/proc/loadavg 和/proc/uptime 两个文件，注意不能编辑/proc 中的文件，要用 cat 等命令来查看，如：\ncat /proc/loadavg 0.0 0.00 0.00 1/55 5505 uptime 命令用法十分简单：直接输入\nuptime 例： 18:02:41 up 41 days, 23:42,\t1 user,\tload average: 0.","title":"linux系统负载高排查"},{"content":"LVM的工作原理 LVM（ Logical Volume Manager）逻辑卷管理，是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘\n它就是通过将底层的物理硬盘封装起来，然后以逻辑卷的方式呈现给上层应用。在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。\nLVM常用的术语 物理存储介质（The physical media）:LVM存储介质可以是磁盘分区，整个磁盘，RAID阵列或SAN磁盘，设备必须初始化为LVM物理卷，才能与LVM结合使用\n物理卷PV（physical volume） ：物理卷就是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数,创建物理卷它可以用硬盘分区，也可以用硬盘本身；\n卷组VG（Volume Group） ：一个LVM卷组由一个或多个物理卷组成 \n逻辑卷LV（logical volume） ：LV建立在VG之上，可以在LV之上建立文件系统\nPE（physical extents） ：PV物理卷中可以分配的最小存储单元，PE的大小是可以指定的，默认为4MB\nLE（logical extent） ： LV逻辑卷****中可以分配的最小存储单元，在同一个卷组中，LE的大小和PE是相同的，并且一一对应\n最小存储单位总结：\n名称 最小存储单位\n硬盘 扇区（512字节）\n文件系统 block（1K或4K ）# mkfs.ext4 -b 2048 /dev/sdb1 ，最大支持到4096\nraid chunk （512K） #mdadm -C -v /dev/md5 -l 5 -n 3 -c 512 -x 1 /dev/sde{1,2,3,5}\nLVM PE （4M） # vgcreate -s 4M vg1 /dev/sdb{1,2}\nLVM主要元素构成： 总结：多个磁盘/分区/raid-》多个物理卷PV-》合成卷组VG-》从VG划出逻辑卷LV-》格式化LV挂载使用\nLVM优点 使用卷组，使多个硬盘空间看起来像是一个大的硬盘\n使用逻辑卷，可以跨多个硬盘空间的分区 sdb1 sdb2 sdc1 sdd2 sdf\n在使用逻辑卷时，它可以在空间不足时动态调整它的大小\n在调整逻辑卷大小时，不需要考虑逻辑卷在硬盘上的位置，不用担心没有可用的连续空间\n可以在线对LV,VG 进行创建，删除，调整大小等操作。LVM上的文件系统也需要重新调整大小。\n允许创建快照，可以用来保存文件系统的备份。\nRAID+LVM一起用：LVM是软件的卷管理方式，而RAID是磁盘管理的方法。对于重要的数据，使用RAID来保护物理的磁盘不会因为故障而中断业务，再用LVM用来实现对卷的良性的管理，更好的利用磁盘资源。\n创建LVM的基本步骤 1) 物理磁盘被格式化为PV，(空间被划分为一个个的PE) #PV包含PE\n2) 不同的PV加入到同一个VG中，(不同PV的PE全部进入到了VG的PE池内) #VG包含PV\n3) 在VG中创建LV逻辑卷，基于PE创建，(组成LV的PE可能来自不同的物理磁盘) #LV基于PE创建\n4) LV直接可以格式化后挂载使用 #格式化挂载使用\n5) LV的扩充缩减实际上就是增加或减少组成该LV的PE数量，其过程不会丢失原始数据\nlvm常用的命令 下面的操作会用的一些查看命令：\n创建并使用LVM逻辑卷\n1、 创建PV 添加一个sdb磁盘\n[root@xuegod63 ~]# fdisk /dev/sdb #创建4个主分区，每个分区1G [root@xuegod63 ~]# ls /dev/sdb\\ /dev/sdb /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb4 #设定分区类型代码：fdisk /dev/sdb ===\u0026gt; t ===\u0026gt; 选择分区号 ====\u0026gt; 8e ====\u0026gt; w #注：现在系统已经很智能了， 直接使用默认的 83 Linux分区，也可以创建pv的。 [root@xuegod63 ~]# pvcreate /dev/sdb{1,2,3,4} #创建pv Physical volume \u0026#34;/dev/sdb1\u0026#34; successfully created. Physical volume \u0026#34;/dev/sdb2\u0026#34; successfully created. Physical volume \u0026#34;/dev/sdb3\u0026#34; successfully created. Physical volume \u0026#34;/dev/sdb4\u0026#34; successfully created. [root@xuegod63 ~]# pvdisplay /dev/sdb1 #查看物理卷信息 \u0026#34;/dev/sdb1\u0026#34; is a new physical volume of \u0026#34;1.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/sdb1 VG Name PV Size 1.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID SHKFwf-WsLr-kkox-wlee-dAXc-5eL0-hyhaTV 2、创建vg卷组： # 语法： vgcreate vg名字 pv的名字 可以跟多个pv [root@xuegod63 ~]# vgcreate vg01 /dev/sdb1 Volume group \u0026#34;vg01\u0026#34; successfully created [root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg01 1 0 0 wz--n- 1020.00m 1020.00m [root@xuegod63 ~]# vgdisplay vg01 --- Volume group --- VG Name vg01 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size 1020.00 MiB PE Size 4.00 MiB Total PE 255 Alloc PE / Size 0 / 0 3、创建LV lvcreate -n 指定新逻辑卷的名称 -L指定lv大小的SIZE(M,G) （-l：小l 指定LE的数量） vgname [root@xuegod63 ~]# lvcreate -n lv01 -L 16M vg01 Logical volume \u0026#34;lv01\u0026#34; created. [root@xuegod63 ~]# lvcreate -n lv02 -l 4 vg01 Logical volume \u0026#34;lv02\u0026#34; created. [root@xuegod63 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv01 vg01 -wi-a----- 16.00m lv02 vg01 -wi-a----- 16.00m [root@xuegod63 ~]# pvdisplay /dev/sdb1 --- Physical volume --- PV Name /dev/sdb1 VG Name vg01 PV Size 1.00 GiB / not usable 4.00 MiB Allocatable yes PE Size 4.00 MiB Total PE 255 Free PE 247 Allocated PE 8 # Allocated [\u0026#39;æləkeɪtɪd] 分配 ，已经使用了8个PE [root@xuegod63 ~]# vgdisplay vg01 Alloc PE / Size 8 / 32.00 MiB #已经使用8个PE，32MB Free PE / Size 247 / 988.00 MiB 4、文件系统格式与挂载 [root@xuegod63 ~]# mkdir /lv01 互动： lv01 逻辑卷的路径在哪？ [root@xuegod63 ~]# ls /dev/vg01/ #查看逻辑卷 lv01 lv02 [root@xuegod63 ~]# ll /dev/vg01/lv01 #其实lv01是dm-0的软链接 lrwxrwxrwx 1 root root 7 5月 18 19:02 /dev/vg01/lv01 -\u0026gt; ../dm-0 [root@xuegod63 ~]# mkfs.ext4 /dev/vg01/lv01 [root@xuegod63 ~]# mount /dev/vg01/lv01 /lv01 [root@xuegod63 ~]# df -Th /lv01 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 ext4 15M 268K 14M 2% /lv01 [root@xuegod63 ~]#echo \u0026#34;/dev/vg01/lv01 /lv01 ext4 defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab 5、指定PE大小用 指定PE大小用的参数： -s ,如果存储的数据都是大文件，那么PE尽量调大，读取速度快\n[root@xuegod63 ~]# vgcreate -s 16M vg02 /dev/sdb2 Volume group \u0026#34;vg02\u0026#34; successfully created # PE的大小只有为2的幂数，且最大为512M [root@xuegod63 ~]# vgdisplay vg02 --- Volume group --- VG Name vg02 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 1 Act PV 1 VG Size 1008.00 MiB PE Size 16.00 MiB #已经是16MB** 6、LV扩容 首先，确定一下是否有可用的扩容空间，因为空间是从VG里面创建的，并且LV不能跨VG扩容\n[root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg01 1 2 0 wz--n- 1020.00m 988.00m vg02 1 0 0 wz--n- 1008.00m 1008.00m** 用的命令如下：\n扩容逻辑卷\n[root@xuegod63 ~]# lvextend -L +30m /dev/vg01/lv01 说明：在指定大小的时候，扩容30m和扩容到30m是不一样的写法\n扩容30m ====\u0026gt; -L +30M\n扩容到30m =====\u0026gt; -L 30M\n[root@xuegod63 ~]# lvextend -L +30m /dev/vg01/lv01 Rounding size to boundary between physical extents: 32.00 MiB. Size of logical volume vg01/lv01 changed from 16.00 MiB (4 extents) to 48.00 MiB (12 extents). Logical volume vg01/lv01 successfully resized.** [root@xuegod63 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv01 vg01 -wi-ao---- 48.00m #LV已经扩容成功 lv02 vg01 -wi-a----- 16.00m [root@xuegod63 ~]# df -Th /lv01 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 ext4 15M 268K 14M 2% /lv01 注：可以看到LV虽然扩展了，但是文件系统大小还是原来的，下面开始扩容文件系统\next4文件系统扩容使用命令语法： resize2fs 逻辑卷名称\nxfs文件系统扩容使用命令语法： xfs_growfs 挂载点 xfs_growfs /var/\nresize2fs和xfs_growfs 两者的区别是传递的参数不一样的，xfs_growfs是采用的挂载点；resize2fs是逻辑卷名称，而且resize2fs命令不能对xfs类型文件系统使用\n[root@xuegod63 ~]# resize2fs /dev/vg01/lv01 resize2fs 1.42.9 (28-Dec-2013) Filesystem at /dev/vg01/lv01 is mounted on /lv01; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/vg01/lv01 is now 49152 blocks long. [root@xuegod63 ~]# df -Th /lv01 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 ext4 46M （扩容成功）522K 43M 2% /lv01 [root@xuegod63 ~]# lvextend -L 80M -r /dev/vg01/lv01 #直接扩容到80M空间，一步到位，不用再扩文件系统了 [root@xuegod63 ~]# df -T /lv01/ 文件系统 类型 1K-块 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 ext4 78303 776 73761 2% /lv01 [root@xuegod63 ~]# df -Th /lv01/ 文件系统 类型 容量 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 ext4 77M 776K 73M 2% /lv01** 7、VG扩容 [root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg01 1 2 0 wz--n- 1020.00m 924.00m vg02 1 0 0 wz--n- 1008.00m 1008.00m # vg扩容的场景：vg卷组中的空间不了够，需要添加新的硬盘进来 [root@xuegod63 ~]# pvcreate /dev/sdb3 # 创建pv [root@xuegod63 ~]# vgextend vg01 /dev/sdb3 #扩容成功 Volume group \u0026#34;vg01\u0026#34; successfully extended [root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg01 2 2 0 wz--n- 1.99g \u0026lt;1.90g vg02 1 0 0 wz--n- 1008.00m 1008.00m 8、LVM缩小 互动：LVM可以动态增加，可以动态缩小吗？\n答：LVM可以动态增加，也可以动态缩小，但是XFS不支持动态缩小，所以我们无法实现基于xfs的动态缩小。btrfs文件系统支持在线缩小。\n[root@xuegod63 ~]# lvreduce -L -20m /dev/vg01/lv01 WARNING: Reducing active and open logical volume to 60.00 MiB. THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce vg01/lv01? [y/n]: y Size of logical volume vg01/lv01 changed from 80.00 MiB (20 extents) to 60.00 MiB (15 extents). Logical volume vg01/lv01 successfully resized. #缩小成功** 但是文件系统没有缩小成功：\n[root@xuegod63 ~]# df -h /lv01/ 文件系统 容量 已用 可用 已用% 挂载点 /dev/mapper/vg01-lv01 77M 776K 73M 2% /lv01 #发现文件系统上空间没有变 [root@xuegod63 ~]# lvextend -L 10M -r /dev/vg01/lv01 #这两个命令也是不能执行成功的 [root@xuegod63 ~]# resize2fs /dev/vg01/lv01 #这两个命令也是不能执行成功的 VG的缩减，要保证你的物理卷是否被使用，是因为它无法缩减一个正在使用的PV\n[root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg01 2 2 0 wz--n- 1.99g \u0026lt;1.92g vg02 1 0 0 wz--n- 1008.00m 1008.00m [root@xuegod63 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdb1 vg01 lvm2 a-- 1020.00m 944.00m /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 vg01 lvm2 a-- 1020.00m 1020.00m /dev/sdb4 lvm2 --- 1.00g 1.00g [root@xuegod63 ~]# cp -r /boot/grub /lv01/ #复制一些测试数据 [root@xuegod63 ~]# vgreduce vg01 /dev/sdb1 #将sdb1移出失败，因sdb1正在被使用 Physical volume \u0026#34;/dev/sdb1\u0026#34; still in use** 互动：如果sdb1是一个磁盘阵列，而这个磁盘阵列使用年代太久，我们必须移出怎么办？\n移动数据：\n[root@xuegod63 ~]# pvmove /dev/sdb1 /dev/sdb3 #将sdb1上数据移到新增加sdb3 pv 上 /dev/sdb1: Moved: 23.53% /dev/sdb1: Moved: 76.47% /dev/sdb1: Moved: 100.00% [root@xuegod63 ~]# vgreduce vg01 /dev/sdb1 #移完数据再移出 Removed \u0026#34;/dev/sdb1\u0026#34; from volume group \u0026#34;vg01\u0026#34; [root@xuegod63 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdb1 lvm2 --- 1.00g 1.00g /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 vg01 lvm2 a-- 1020.00m 952.00m #vg01中只有sdb3了 9、 LVM删除 创建LVM流程:\npvcreate创建pv -\u0026gt; vgcreate创建卷组 -\u0026gt; lvcreate创建逻辑卷 -\u0026gt; mkfs.xfs lv 格式化-\u0026gt; mount挂载\n删除LVM流程：\numount卸载 -\u0026gt; lvremove lv移出卷组中所有逻辑卷-\u0026gt; vgremove vg移出卷组-\u0026gt; pvremove 移出pv\n[root@xuegod63 ~]# umount /lv01 [root@xuegod63 ~]# lvremove /dev/vg01/lv01 Do you really want to remove active logical volume vg01/lv01? [y/n]: y Logical volume \u0026#34;lv01\u0026#34; successfully removed [root@xuegod63 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv02 vg01 -wi-a----- 16.00m #已经看不到lv01 [root@xuegod63 ~]# vgremove vg01 #直接移出卷组 Do you really want to remove volume group \u0026#34;vg01\u0026#34; containing 1 logical volumes? [y/n]: y Do you really want to remove active logical volume vg01/lv02? [y/n]: y #如果卷组中还有lv，移出时，会提示，是否也移出，咱们这里直接移出 Logical volume \u0026#34;lv02\u0026#34; successfully removed Volume group \u0026#34;vg01\u0026#34; successfully removed [root@xuegod63 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg02 1 0 0 wz--n- 1008.00m 1008.00m #没有vg01 移出pv sdb1\n[root@xuegod63 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdb1 lvm2 --- 1.00g 1.00g /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 lvm2 --- 1.00g 1.00g /dev/sdb4 lvm2 --- 1.00g 1.00g [root@xuegod63 ~]# pvremove /dev/sdb1 #已经移出 Labels on physical volume \u0026#34;/dev/sdb1\u0026#34; successfully wiped. [root@xuegod63 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sdb2 vg02 lvm2 a-- 1008.00m 1008.00m /dev/sdb3 lvm2 --- 1.00g 1.00g /dev/sdb4 lvm2 --- 1.00g 1.00g 15.3 实战-使用SSM工具为公司的邮件服务器创建可动态扩容的存储池\n安装SSM ssm工具了一下\n[root@xuegod63 ~]# yum -y install system-storage-manager\nSSM：检查关于可用硬驱和LVM卷的信息。显示关于现有磁盘存储设备、存储池、LVM卷和存储快照的信息。\n15.3.1 查看磁盘信息\n列出设备信息\nroot@xuegod63 ~]# ssm list dev\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\nDevice Free Used Total Pool Mount point\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\n/dev/fd0 4.00 KB\n/dev/sda 20.00 GB PARTITIONED\n/dev/sda1 200.00 MB /boot\n/dev/sda2 1.00 GB SWAP\n/dev/sda3 10.00 GB /\n/dev/sdb 20.00 GB\n/dev/sdb1 1.00 GB\n/dev/sdb2 1008.00 MB 0.00 KB 1.00 GB vg02\n/dev/sdb3 1.00 GB\n/dev/sdb4 1.00 GB\n存储池信息\n[root@xuegod63 ~]# ssm list pool\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\nPool Type Devices Free Used Total\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\nvg02 lvm 1 1008.00 MB 0.00 KB 1008.00 MB\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n15.3.2 实战：为公司的邮件服务器创建基于LVM的邮件存储\n实战场景：公司要搭建一台邮件服务器，考虑到后期公司发展规模扩张，需要你创建一个名为mail 的LVM存储池，并在其上创建一个名为mail-lv，初始大小为1G的lvm卷，格式化为xfs文件系统，并将其挂载/mail-lv目录下。此存储池中的空间后期要可以动态扩容。\n将sdb上所有卷组信息删除：\n[root@xuegod63 ~]# vgremove vg02\n[root@xuegod63 ~]# pvremove /dev/sdb{1,2,3,4}\n创建目录\n[root@xuegod72 ~]# mkdir /mail-lv\n用的命令如下：\nssm create -s lv大小 -n lv名称 \u0026ndash;fstype lv文件系统类型 -p 卷组名 设备 挂载点\n自动把设备变成pv，创建vg , lv ,格式化文件系统, 自动挂载\n[root@xuegod63 ~]# ssm create -s 1G -n mail-lv \u0026ndash;fstype xfs -p mail /dev/sdb[1-4] /mail-lv\nPhysical volume \u0026ldquo;/dev/sdb1\u0026rdquo; successfully created.\nPhysical volume \u0026ldquo;/dev/sdb2\u0026rdquo; successfully created.\nPhysical volume \u0026ldquo;/dev/sdb3\u0026rdquo; successfully created.\nPhysical volume \u0026ldquo;/dev/sdb4\u0026rdquo; successfully created.\nVolume group \u0026ldquo;mail\u0026rdquo; successfully created\nWARNING: ext4 signature detected on /dev/mail/mail-lv at offset 1080. Wipe it? [y/n]: y\nWiping ext4 signature on /dev/mail/mail-lv.\nLogical volume \u0026ldquo;mail-lv\u0026rdquo; created.\nmeta-data=/dev/mail/mail-lv isize=512 agcount=4, agsize=65536 blks\n​ = sectsz=512 attr=2, projid32bit=1\n​ = crc=1 finobt=0, sparse=0\ndata = bsize=4096 blocks=262144, imaxpct=25\n​ = sunit=0 swidth=0 blks\nnaming =version 2 bsize=4096 ascii-ci=0 ftype=1\nlog =internal log bsize=4096 blocks=2560, version=2\n​ = sectsz=512 sunit=0 blks, lazy-count=1\nrealtime =none extsz=4096 blocks=0, rtextents=0\n[root@xuegod63 ~]# df -h /mail-lv/\n文件系统 容量 已用 可用 已用% 挂载点\n/dev/mapper/mail-mail\u0026ndash;lv 1014M 33M 982M 4% /mail-lv\n总结：\n15.1 LVM的工作原理\n15.2 创建LVM的基本步骤\n15.3 实战-使用SSM工具为公司的邮件服务器创建可动态扩容的存储池\n","permalink":"https://wandong1.github.io/post/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%AE%A1%E7%90%86/","summary":"LVM的工作原理 LVM（ Logical Volume Manager）逻辑卷管理，是在磁盘分区和文件系统之间添加的一个逻辑层，来为文件系统屏蔽下层磁盘分区布局，提供一个抽象的盘卷，在盘卷上建立文件系统。管理员利用LVM可以在磁盘不用重新分区的情况下动态调整文件系统的大小，并且利用LVM管理的文件系统可以跨越磁盘，当服务器添加了新的磁盘后，管理员不必将原有的文件移动到新的磁盘上，而是通过LVM可以直接扩展文件系统跨越磁盘\n它就是通过将底层的物理硬盘封装起来，然后以逻辑卷的方式呈现给上层应用。在LVM中，其通过对底层的硬盘进行封装，当我们对底层的物理硬盘进行操作时，其不再是针对于分区进行操作，而是通过一个叫做逻辑卷的东西来对其进行底层的磁盘管理操作。\nLVM常用的术语 物理存储介质（The physical media）:LVM存储介质可以是磁盘分区，整个磁盘，RAID阵列或SAN磁盘，设备必须初始化为LVM物理卷，才能与LVM结合使用\n物理卷PV（physical volume） ：物理卷就是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数,创建物理卷它可以用硬盘分区，也可以用硬盘本身；\n卷组VG（Volume Group） ：一个LVM卷组由一个或多个物理卷组成 \n逻辑卷LV（logical volume） ：LV建立在VG之上，可以在LV之上建立文件系统\nPE（physical extents） ：PV物理卷中可以分配的最小存储单元，PE的大小是可以指定的，默认为4MB\nLE（logical extent） ： LV逻辑卷****中可以分配的最小存储单元，在同一个卷组中，LE的大小和PE是相同的，并且一一对应\n最小存储单位总结：\n名称 最小存储单位\n硬盘 扇区（512字节）\n文件系统 block（1K或4K ）# mkfs.ext4 -b 2048 /dev/sdb1 ，最大支持到4096\nraid chunk （512K） #mdadm -C -v /dev/md5 -l 5 -n 3 -c 512 -x 1 /dev/sde{1,2,3,5}\nLVM PE （4M） # vgcreate -s 4M vg1 /dev/sdb{1,2}\nLVM主要元素构成： 总结：多个磁盘/分区/raid-》多个物理卷PV-》合成卷组VG-》从VG划出逻辑卷LV-》格式化LV挂载使用\nLVM优点 使用卷组，使多个硬盘空间看起来像是一个大的硬盘\n使用逻辑卷，可以跨多个硬盘空间的分区 sdb1 sdb2 sdc1 sdd2 sdf","title":"lvm逻辑卷管理"},{"content":"Nginx限流熔断 作为优秀的负载均衡模块，目前是我们工作中用到最多的。其实，该模块是提供了我们需要的后端限流功能的。通过官方文档介绍，\n令牌桶算法 算法思想是：\n令牌以固定速率产生，并缓存到令牌桶中； 令牌桶放满时，多余的令牌被丢弃； 请求要消耗等比例的令牌才能被处理； 令牌不够时，请求被缓存。 漏桶算法 算法思想是：\n水（请求）从上方倒入水桶，从水桶下方流出（被处理）； 来不及流出的水存在水桶中（缓冲），以固定速率流出； 水桶满后水溢出（丢弃）。 这个算法的核心是：缓存请求、匀速处理、多余的请求直接丢弃。 相比漏桶算法，令牌桶算法不同之处在于它不但有一只“桶”，还有个队列，这个桶是用来存放令牌的，队列才是用来存放请求的。 从作用上来说，漏桶和令牌桶算法最明显的区别就是是否允许突发流量(burst)的处理，漏桶算法能够强行限制数据的实时传输（处理）速率，对突发流量不做额外处理；而令牌桶算法能够在限制数据的平均传输速率的同时允许某种程度的突发传输。\nNginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阈值。\n案例 通过查看nginx官方文档，https://www.nginx.cn/doc/\nHttpLimit zone 本模块可以针对条件，进行会话的并发连接数控制。（例如：限制每个IP的并发连接数。）\n配置示例\nhttp {\r: limit_zone one $binary_remote_addr 10m;\r: ...\r: server {\r: ...\r: location /download/ {\r: limit_conn one 1;\r: } 指令 [#limit_zone limit_zone] [#limit_conn limit_conn] limit_zone 语法： limit_zone zone_name $variable the_size\n默认值： no\n作用域： http\n本指令定义了一个数据区，里面记录会话状态信息。 $variable 定义判断会话的变量；the_size 定义记录区的总容量。\n例子：\nlimit_zone one $binary_remote_addr 10m; 定义一个叫“one”的记录区，总容量为 10M，以变量 $binary_remote_addr 作为会话的判断基准（即一个地址一个会话）。\n您可以注意到了，在这里使用的是 $binary_remote_addr 而不是 $remote_addr。\n$remote_addr 的长度为 7 至 15 bytes，会话信息的长度为 32 或 64 bytes。 而 $binary_remote_addr 的长度为 4 bytes，会话信息的长度为 32 bytes。\n当区的大小为 1M 的时候，大约可以记录 32000 个会话信息（一个会话占用 32 bytes）。\nlimit_conn 语法： limit_conn zone_name the_size\n默认值： no\n作用域： http, server, location\n指定一个会话最大的并发连接数。 当超过指定的最发并发连接数时，服务器将返回 \u0026ldquo;Service unavailable\u0026rdquo; (503)。\n例子：\nlimit_zone one $binary_remote_addr 10m;\r: server {\r: location /download/ {\r: limit_conn one 1;\r: } 定义一个叫“one”的记录区，总容量为 10M，以变量 $binary_remote_addr 作为会话的判断基准（即一个地址一个会话）。 限制 /download/ 目录下，一个会话只能进行一个连接。 简单点，就是限制 /download/ 目录下，一个IP只能发起一个连接，多过一个，一律503。\nab 测试安装 #ab运行需要依赖apr-util包，安装命令为： yum install apr-util #安装依赖 yum-utils中的yumdownload 工具，如果没有找到 yumdownload 命令可以 yum install yum-utils cd /opt mkdir abtmp cd abtmp yum install yum-utils.noarch yumdownloader httpd-tools* rpm2cpio httpd-*.rpm | cpio -idmv #操作完成后 将会产生一个 usr 目录 ab文件就在这个usr 目录中 #简单使用说明 ./ab -c 100 -n 10000 http://127.0.0.1/index.html #-c 100 即：每次并发100个 #-n 10000 即： 共发送10000个请求 ","permalink":"https://wandong1.github.io/post/nginx%E9%99%90%E6%B5%81%E7%86%94%E6%96%AD/","summary":"Nginx限流熔断 作为优秀的负载均衡模块，目前是我们工作中用到最多的。其实，该模块是提供了我们需要的后端限流功能的。通过官方文档介绍，\n令牌桶算法 算法思想是：\n令牌以固定速率产生，并缓存到令牌桶中； 令牌桶放满时，多余的令牌被丢弃； 请求要消耗等比例的令牌才能被处理； 令牌不够时，请求被缓存。 漏桶算法 算法思想是：\n水（请求）从上方倒入水桶，从水桶下方流出（被处理）； 来不及流出的水存在水桶中（缓冲），以固定速率流出； 水桶满后水溢出（丢弃）。 这个算法的核心是：缓存请求、匀速处理、多余的请求直接丢弃。 相比漏桶算法，令牌桶算法不同之处在于它不但有一只“桶”，还有个队列，这个桶是用来存放令牌的，队列才是用来存放请求的。 从作用上来说，漏桶和令牌桶算法最明显的区别就是是否允许突发流量(burst)的处理，漏桶算法能够强行限制数据的实时传输（处理）速率，对突发流量不做额外处理；而令牌桶算法能够在限制数据的平均传输速率的同时允许某种程度的突发传输。\nNginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阈值。\n案例 通过查看nginx官方文档，https://www.nginx.cn/doc/\nHttpLimit zone 本模块可以针对条件，进行会话的并发连接数控制。（例如：限制每个IP的并发连接数。）\n配置示例\nhttp {\r: limit_zone one $binary_remote_addr 10m;\r: ...\r: server {\r: ...\r: location /download/ {\r: limit_conn one 1;\r: } 指令 [#limit_zone limit_zone] [#limit_conn limit_conn] limit_zone 语法： limit_zone zone_name $variable the_size\n默认值： no\n作用域： http\n本指令定义了一个数据区，里面记录会话状态信息。 $variable 定义判断会话的变量；the_size 定义记录区的总容量。\n例子：\nlimit_zone one $binary_remote_addr 10m; 定义一个叫“one”的记录区，总容量为 10M，以变量 $binary_remote_addr 作为会话的判断基准（即一个地址一个会话）。","title":"nginx限流熔断"},{"content":"import consul from random import randint import requests import json class ConsulClient(): \u0026#39;\u0026#39;\u0026#39;定义consul操作类\u0026#39;\u0026#39;\u0026#39; def __init__(self, host=None, port=None, token=None): \u0026#39;\u0026#39;\u0026#39;初始化，指定consul主机、端口和token\u0026#39;\u0026#39;\u0026#39; self.host = host # consul 主机 self.port = port # consul 端口 self.token = token self.consul = consul.Consul(host=host, port=port) def register(self, name, service_id, address, port, tags, interval, url): \u0026#39;\u0026#39;\u0026#39;注册服务\u0026#39;\u0026#39;\u0026#39; # 设置检测模式：http和tcp # tcp模式 # check=consul.Check().tcp(self.host, self.port, # \u0026#34;5s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;30s\u0026#34;) # http模式 check = consul.Check().http(url, interval, timeout=None, deregister=None, header=None) self.consul.agent.service.register(name, service_id=service_id, address=address, port=port, tags=tags, interval=interval, check=check) def getService(self, name): \u0026#39;\u0026#39;\u0026#39;通过负载均衡获取服务实例\u0026#39;\u0026#39;\u0026#39; # 获取相应服务下的DataCenter url = \u0026#39;http://\u0026#39; + self.host + \u0026#39;:\u0026#39; + str(self.port) + \u0026#39;/v1/catalog/service/\u0026#39; + name dataCenterResp = requests.get(url) if dataCenterResp.status_code != 200: raise Exception(\u0026#39;can not connect to consul \u0026#39;) listData = json.loads(dataCenterResp.text) # 初始化DataCenter dcset = set() for service in listData: dcset.add(service.get(\u0026#39;Datacenter\u0026#39;)) # 服务列表初始化 serviceList = [] for dc in dcset: if self.token: url = f\u0026#39;http://{self.host}:{self.port}/v1/health/service/{name}?dc={dc}\u0026amp;token={self.token}\u0026#39; else: url = f\u0026#39;http://{self.host}:{self.port}/v1/health/service/{name}?dc={dc}\u0026amp;token=\u0026#39; resp = requests.get(url) if resp.status_code != 200: raise Exception(\u0026#39;can not connect to consul \u0026#39;) text = resp.text serviceListData = json.loads(text) for serv in serviceListData: status = serv.get(\u0026#39;Checks\u0026#39;)[1].get(\u0026#39;Status\u0026#39;) # 选取成功的节点 if status == \u0026#39;passing\u0026#39;: address = serv.get(\u0026#39;Service\u0026#39;).get(\u0026#39;Address\u0026#39;) port = serv.get(\u0026#39;Service\u0026#39;).get(\u0026#39;Port\u0026#39;) serviceList.append({\u0026#39;port\u0026#39;: port, \u0026#39;address\u0026#39;: address}) if len(serviceList) == 0: raise Exception(\u0026#39;no serveice can be used\u0026#39;) else: # 随机获取一个可用的服务实例 print(\u0026#39;当前服务列表：\u0026#39;, serviceList) service = serviceList[randint(0, len(serviceList) - 1)] return service[\u0026#39;address\u0026#39;], int(service[\u0026#39;port\u0026#39;]) ","permalink":"https://wandong1.github.io/post/python-consul%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BB%A3%E7%A0%81%E8%8C%83%E4%BE%8B/","summary":"import consul from random import randint import requests import json class ConsulClient(): \u0026#39;\u0026#39;\u0026#39;定义consul操作类\u0026#39;\u0026#39;\u0026#39; def __init__(self, host=None, port=None, token=None): \u0026#39;\u0026#39;\u0026#39;初始化，指定consul主机、端口和token\u0026#39;\u0026#39;\u0026#39; self.host = host # consul 主机 self.port = port # consul 端口 self.token = token self.consul = consul.Consul(host=host, port=port) def register(self, name, service_id, address, port, tags, interval, url): \u0026#39;\u0026#39;\u0026#39;注册服务\u0026#39;\u0026#39;\u0026#39; # 设置检测模式：http和tcp # tcp模式 # check=consul.Check().tcp(self.host, self.port, # \u0026#34;5s\u0026#34;, \u0026#34;30s\u0026#34;, \u0026#34;30s\u0026#34;) # http模式 check = consul.Check().http(url, interval, timeout=None, deregister=None, header=None) self.consul.agent.service.register(name, service_id=service_id, address=address, port=port, tags=tags, interval=interval, check=check) def getService(self, name): \u0026#39;\u0026#39;\u0026#39;通过负载均衡获取服务实例\u0026#39;\u0026#39;\u0026#39; # 获取相应服务下的DataCenter url = \u0026#39;http://\u0026#39; + self.","title":"python consul注册中心客户端代码范例"},{"content":"安装工具 指定pip源安装\npip install rdbtools -i https://mirrors.aliyun.com/pypi/simple/ pip install python-lzf -i https://mirrors.aliyun.com/pypi/simple/ 生成内存报告 rdb -c memory dump.rdb \u0026gt; redis_memory_report.csv\n详细参考：https://www.cnblogs.com/xingxia/p/redis_rdb_tools.html\n","permalink":"https://wandong1.github.io/post/redis%E5%B7%A5%E5%85%B7%E4%B9%8Bredis_rdb_tools/","summary":"安装工具 指定pip源安装\npip install rdbtools -i https://mirrors.aliyun.com/pypi/simple/ pip install python-lzf -i https://mirrors.aliyun.com/pypi/simple/ 生成内存报告 rdb -c memory dump.rdb \u0026gt; redis_memory_report.csv\n详细参考：https://www.cnblogs.com/xingxia/p/redis_rdb_tools.html","title":"redis工具之redis_rdb_tools"},{"content":"tcpdump -i bond0 tcp and port 3029 and host 10.42.23.141 -n -nn -vvv -w redis2.cap data and tcp.dstport == 3034\nimport dpkt import socket from openpyxl import Workbook def analysis_of_redis_cap(cap_file,redis_port): wb = Workbook() ws = wb.active table_hed = [\u0026#39;源地址\u0026#39;,\u0026#39;目的地址\u0026#39;,\u0026#39;redis请求命令\u0026#39;] ws.append(table_hed) with open(cap_file,\u0026#39;rb\u0026#39;) as f: string_data = dpkt.pcap.Reader(f) for ts, buf in string_data: eth = dpkt.ethernet.Ethernet(buf) ip = eth.data tcp = ip.data # print(tcp.dport) if tcp.dport == int(redis_port): try: data_pre = tcp.data.decode(\u0026#39;utf-8\u0026#39;) data_pre = data_pre.split(\u0026#39;\\r\\n\u0026#39;) except Exception as res: data_pre = \u0026#39;\u0026#39; request_info = \u0026#39;\u0026#39; for i in data_pre: if i.startswith(\u0026#39;$\u0026#39;): pass else: request_info += i + \u0026#39; \u0026#39; if len(request_info) \u0026gt; 1: # print(socket.inet_ntoa(ip.src),tcp.sport) src_ip = \u0026#39;{}:{}\u0026#39;.format(socket.inet_ntoa(ip.src),tcp.sport) dest_ip = \u0026#39;{}:{}\u0026#39;.format(socket.inet_ntoa(ip.dst),tcp.dport) lst = [src_ip, dest_ip, request_info] ws.append(lst) # print(request_info) wb.save(\u0026#39;redis_{}查询请求.xlsx\u0026#39;.format(redis_port)) if __name__ == \u0026#39;__main__\u0026#39;: cap_file = r\u0026#39;redis3028.cap\u0026#39; analysis_of_redis_cap(cap_file,3028) ","permalink":"https://wandong1.github.io/post/redis%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/","summary":"tcpdump -i bond0 tcp and port 3029 and host 10.42.23.141 -n -nn -vvv -w redis2.cap data and tcp.dstport == 3034\nimport dpkt import socket from openpyxl import Workbook def analysis_of_redis_cap(cap_file,redis_port): wb = Workbook() ws = wb.active table_hed = [\u0026#39;源地址\u0026#39;,\u0026#39;目的地址\u0026#39;,\u0026#39;redis请求命令\u0026#39;] ws.append(table_hed) with open(cap_file,\u0026#39;rb\u0026#39;) as f: string_data = dpkt.pcap.Reader(f) for ts, buf in string_data: eth = dpkt.ethernet.Ethernet(buf) ip = eth.data tcp = ip.data # print(tcp.dport) if tcp.dport == int(redis_port): try: data_pre = tcp.","title":"redis抓包分析"},{"content":"TCP 协议简述 TCP 提供面向有连接的通信传输，面向有连接是指在传送数据之前必须先建立连接，数据传送完成后要释放连接。\n无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。 同时由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是全双工模式，所以需要四次挥手关闭连接。\nTCP包首部 网络中传输的数据包由两部分组成：一部分是协议所要用到的首部，另一部分是上一层传过来的数据。首部的结构由协议的具体规范详细定义。在数据包的首部，明确标明了协议应该如何读取数据。反过来说，看到首部，也就能够了解该协议必要的信息以及所要处理的数据。包首部就像协议的脸。\n所以我们在学习TCP协议之前，首先要知道TCP在网络传输中处于哪个位置，以及它的协议的规范，下面我们就看看TCP首部的网络传输起到的作用：\n下面的图是TCP头部的规范定义，它定义了TCP协议如何读取和解析数据： TCP首部承载这TCP协议需要的各项信息，下面我们来分析一下：\nTCP端口号 TCP的连接是需要四个要素确定唯一一个连接： （源IP，源端口号）+ （目地IP，目的端口号） 所以TCP首部预留了两个16位作为端口号的存储，而IP地址由上一层IP协议负责传递 源端口号和目地端口各占16位两个字节，也就是端口的范围是2^16=65535 另外1024以下是系统保留的，从1024-65535是用户使用的端口范围\nTCP的序号和确认号： 32位序号 seq：Sequence number 缩写seq ，TCP通信过程中某一个传输方向上的字节流的每个字节的序号，通过这个来确认发送的数据有序，比如现在序列号为1000，发送了1000，下一个序列号就是2000。 32位确认号 ack：Acknowledge number 缩写ack，TCP对上一次seq序号做出的确认号，用来响应TCP报文段，给收到的TCP报文段的序号seq加1。\nTCP的标志位 每个TCP段都有一个目的，这是借助于TCP标志位选项来确定的，允许发送方或接收方指定哪些标志应该被使用，以便段被另一端正确处理。 用的最广泛的标志是 SYN，ACK 和 FIN，用于建立连接，确认成功的段传输，最后终止连接。\n**SYN：**简写为S，同步标志位，用于建立会话连接，同步序列号； ACK： 简写为.，确认标志位，对已接收的数据包进行确认； FIN： 简写为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接； **PSH：**简写为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队； **RST：**简写为R，重置标志位，用于连接复位、拒绝错误和非法的数据包； **URG：**简写为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽快处理；\nTCP 三次握手建立连接 所谓三次握手(Three-way Handshake)，是指建立一个 TCP 连接时，需要客户端和服务器总共发送3个报文。\n三次握手的目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，客户端执行 connect() 时。将触发三次握手。\n三次握手过程的示意图如下： 第一次握手： 客户端将TCP报文标志位SYN置为1，随机产生一个序号值seq=J，保存在TCP首部的序列号(Sequence Number)字段里，指明客户端打算连接的服务器的端口，并将该数据包发送给服务器端，发送完毕后，客户端进入SYN_SENT状态，等待服务器端确认。\n第二次握手： 服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务器端将TCP报文标志位SYN和ACK都置为1，ack=J+1，随机产生一个序号值seq=K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。\n第三次握手： 客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。\n**注意:**我们上面写的ack和ACK，不是同一个概念：\n小写的ack代表的是头部的确认号Acknowledge number， 缩写ack，是对上一个包的序号进行确认的号，ack=seq+1。 大写的ACK，则是我们上面说的TCP首部的标志位，用于标志的TCP包是否对上一个包进行了确认操作，如果确认了，则把ACK标志位设置成1。 下面我自己做实验，开一个HTTP服务，监听80端口，然后使用Tcpdump命令抓包，看一下TCP三次握手的过程：\ntcpdump命令解析： -i : 指定抓包的网卡是enp0s3 -n: 把域名转成IP显示 -t: 不显示时间 -S: 序列号使用绝对数值，不指定-S的话，序列号会使用相对的数值 port: 指定监听端口是80 host:指定监听的主机名\nsudo tcpdump -n -t -S -i enp0s3 port 80 第一次握手，标志位Flags=S IP 10.0.2.2.51323 \u0026gt; 10.0.2.15.80: Flags [S], seq 84689409, win 65535, options [mss 1460], length 0 第二次握手，标志位Flags=[S.] IP 10.0.2.15.80 \u0026gt; 10.0.2.2.51323: Flags [S.], seq 1893430205, ack 84689410, win 64240, options [mss 1460], length 0 第三次握手，标志位Flags=[.] IP 10.0.2.2.51323 \u0026gt; 10.0.2.15.80: Flags [.], ack 1893430206, win 65535, length 0 建立连接后，客户端发送http请求 IP 10.0.2.2.51321 \u0026gt; 10.0.2.15.80: Flags [P.], seq 1:753, ack 1, win 65535, length 752: HTTP: GET / HTTP/1.1 我们看下实战中TCP的三次握手过程： 第一次握手，客户端51323端口号向服务器端80号端口发起连接，此时标志位flags=S，即SYN=1标志，表示向服务端发起连接的请求，同时生成序列号seq=84689409 第二次握手，服务端标志位flags=[S.]，即SYN+ACK标志位设置为1，表示对上一个请求连接的报文进行确认，同时设置ack=seq+1=184689410，生成序列号seq=1893430205 第三次握手，客户端对服务端的响应进行确认，所以此时标志位是[.]即ACK=1，同时返回对上一个报文的seq的确认号，ack=1893430206 至此，三次握手完成，一个TCP连接建立完成，接下来就是双端传输数据了 为什么需要三次握手？ 我们假设client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。\n本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。\n假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。\n所以，采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。\nTCP 四次挥手关闭连接 四次挥手即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发。 由于TCP连接是全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。\n四次挥手过程的示意图如下： 挥手请求可以是Client端，也可以是Server端发起的，我们假设是Client端发起：\n第一次挥手： Client端发起挥手请求，向Server端发送标志位是FIN报文段，设置序列号seq，此时，Client端进入FIN_WAIT_1状态，这表示Client端没有数据要发送给Server端了。 **第二次挥手：**Server端收到了Client端发送的FIN报文段，向Client端返回一个标志位是ACK的报文段，ack设为seq加1，Client端进入FIN_WAIT_2状态，Server端告诉Client端，我确认并同意你的关闭请求。 第三次挥手： Server端向Client端发送标志位是FIN的报文段，请求关闭连接，同时Client端进入LAST_ACK状态。 第四次挥手 ： Client端收到Server端发送的FIN报文段，向Server端发送标志位是ACK的报文段，然后Client端进入TIME_WAIT状态。Server端收到Client端的ACK报文段以后，就关闭连接。此时，Client端等待2MSL的时间后依然没有收到回复，则证明Server端已正常关闭，那好，Client端也可以关闭连接了。\n为什么连接的时候是三次握手，关闭的时候却是四次握手？ 建立连接时因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。所以建立连接只需要三次握手。\n由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是全双工模式。 这就意味着，关闭连接时，当Client端发出FIN报文段时，只是表示Client端告诉Server端数据已经发送完毕了。当Server端收到FIN报文并返回ACK报文段，表示它已经知道Client端没有数据发送了，但是Server端还是可以发送数据到Client端的，所以Server很可能并不会立即关闭SOCKET，直到Server端把数据也发送完毕。 当Server端也发送了FIN报文段时，这个时候就表示Server端也没有数据要发送了，就会告诉Client端，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。\n为什么要等待2MSL？ **MSL：**报文段最大生存时间，它是任何报文段被丢弃前在网络内的最长时间。\n有以下两个原因：\n**第一点：**保证TCP协议的全双工连接能够可靠关闭： 由于IP协议的不可靠性或者是其它网络原因，导致了Server端没有收到Client端的ACK报文，那么Server端就会在超时之后重新发送FIN，如果此时Client端的连接已经关闭处于CLOESD状态，那么重发的FIN就找不到对应的连接了，从而导致连接错乱，所以，Client端发送完最后的ACK不能直接进入CLOSED状态，而要保持TIME_WAIT，当再次收到FIN的收，能够保证对方收到ACK，最后正确关闭连接。 **第二点：**保证这次连接的重复数据段从网络中消失 如果Client端发送最后的ACK直接进入CLOSED状态，然后又再向Server端发起一个新连接，这时不能保证新连接的与刚关闭的连接的端口号是不同的，也就是新连接和老连接的端口号可能一样了，那么就可能出现问题：如果前一次的连接某些数据滞留在网络中，这些延迟数据在建立新连接后到达Client端，由于新老连接的端口号和IP都一样，TCP协议就认为延迟数据是属于新连接的，新连接就会接收到脏数据，这样就会导致数据包混乱。所以TCP连接需要在TIME_WAIT状态等待2倍MSL，才能保证本次连接的所有数据在网络中消失。\n","permalink":"https://wandong1.github.io/post/tcp%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B/","summary":"TCP 协议简述 TCP 提供面向有连接的通信传输，面向有连接是指在传送数据之前必须先建立连接，数据传送完成后要释放连接。\n无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。 同时由于TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议，TCP是全双工模式，所以需要四次挥手关闭连接。\nTCP包首部 网络中传输的数据包由两部分组成：一部分是协议所要用到的首部，另一部分是上一层传过来的数据。首部的结构由协议的具体规范详细定义。在数据包的首部，明确标明了协议应该如何读取数据。反过来说，看到首部，也就能够了解该协议必要的信息以及所要处理的数据。包首部就像协议的脸。\n所以我们在学习TCP协议之前，首先要知道TCP在网络传输中处于哪个位置，以及它的协议的规范，下面我们就看看TCP首部的网络传输起到的作用：\n下面的图是TCP头部的规范定义，它定义了TCP协议如何读取和解析数据： TCP首部承载这TCP协议需要的各项信息，下面我们来分析一下：\nTCP端口号 TCP的连接是需要四个要素确定唯一一个连接： （源IP，源端口号）+ （目地IP，目的端口号） 所以TCP首部预留了两个16位作为端口号的存储，而IP地址由上一层IP协议负责传递 源端口号和目地端口各占16位两个字节，也就是端口的范围是2^16=65535 另外1024以下是系统保留的，从1024-65535是用户使用的端口范围\nTCP的序号和确认号： 32位序号 seq：Sequence number 缩写seq ，TCP通信过程中某一个传输方向上的字节流的每个字节的序号，通过这个来确认发送的数据有序，比如现在序列号为1000，发送了1000，下一个序列号就是2000。 32位确认号 ack：Acknowledge number 缩写ack，TCP对上一次seq序号做出的确认号，用来响应TCP报文段，给收到的TCP报文段的序号seq加1。\nTCP的标志位 每个TCP段都有一个目的，这是借助于TCP标志位选项来确定的，允许发送方或接收方指定哪些标志应该被使用，以便段被另一端正确处理。 用的最广泛的标志是 SYN，ACK 和 FIN，用于建立连接，确认成功的段传输，最后终止连接。\n**SYN：**简写为S，同步标志位，用于建立会话连接，同步序列号； ACK： 简写为.，确认标志位，对已接收的数据包进行确认； FIN： 简写为F，完成标志位，表示我已经没有数据要发送了，即将关闭连接； **PSH：**简写为P，推送标志位，表示该数据包被对方接收后应立即交给上层应用，而不在缓冲区排队； **RST：**简写为R，重置标志位，用于连接复位、拒绝错误和非法的数据包； **URG：**简写为U，紧急标志位，表示数据包的紧急指针域有效，用来保证连接不被阻断，并督促中间设备尽快处理；\nTCP 三次握手建立连接 所谓三次握手(Three-way Handshake)，是指建立一个 TCP 连接时，需要客户端和服务器总共发送3个报文。\n三次握手的目的是连接服务器指定端口，建立 TCP 连接，并同步连接双方的序列号和确认号，交换 TCP 窗口大小信息。在 socket 编程中，客户端执行 connect() 时。将触发三次握手。\n三次握手过程的示意图如下： 第一次握手： 客户端将TCP报文标志位SYN置为1，随机产生一个序号值seq=J，保存在TCP首部的序列号(Sequence Number)字段里，指明客户端打算连接的服务器的端口，并将该数据包发送给服务器端，发送完毕后，客户端进入SYN_SENT状态，等待服务器端确认。\n第二次握手： 服务器端收到数据包后由标志位SYN=1知道客户端请求建立连接，服务器端将TCP报文标志位SYN和ACK都置为1，ack=J+1，随机产生一个序号值seq=K，并将该数据包发送给客户端以确认连接请求，服务器端进入SYN_RCVD状态。\n第三次握手： 客户端收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给服务器端，服务器端检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，客户端和服务器端进入ESTABLISHED状态，完成三次握手，随后客户端与服务器端之间可以开始传输数据了。\n**注意:**我们上面写的ack和ACK，不是同一个概念：\n小写的ack代表的是头部的确认号Acknowledge number， 缩写ack，是对上一个包的序号进行确认的号，ack=seq+1。 大写的ACK，则是我们上面说的TCP首部的标志位，用于标志的TCP包是否对上一个包进行了确认操作，如果确认了，则把ACK标志位设置成1。 下面我自己做实验，开一个HTTP服务，监听80端口，然后使用Tcpdump命令抓包，看一下TCP三次握手的过程：","title":"tcp三次握手和四次挥手"},{"content":"从github上检出代码从github上检出代码 $ git clone git://github.com/alibaba/tsar.git $ cd tsar $ make $ make install 从github上下载源码 $ wget -O tsar.zip https://github.com/alibaba/tsar/archive/master.zip --no-check-certificate $ unzip tsar.zip $ cd tsar $ make $ make install 安装后生成的文件 Tsar配置文件路径：/etc/tsar/tsar.conf，tsar的采集模块和输出的具体配置； 定时任务配置:/etc/cron.d/tsar，负责每分钟调用tsar执行采集任务； 日志文件轮转配置:/etc/logrotate.d/tsar，每个月会把tsar的本地存储进行轮转； 模块路径：/usr/local/tsar/modules，各个模块的动态库so文件；\n详细参考：https://www.cnblogs.com/zafu/p/11782988.html\n","permalink":"https://wandong1.github.io/post/tsar%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","summary":"从github上检出代码从github上检出代码 $ git clone git://github.com/alibaba/tsar.git $ cd tsar $ make $ make install 从github上下载源码 $ wget -O tsar.zip https://github.com/alibaba/tsar/archive/master.zip --no-check-certificate $ unzip tsar.zip $ cd tsar $ make $ make install 安装后生成的文件 Tsar配置文件路径：/etc/tsar/tsar.conf，tsar的采集模块和输出的具体配置； 定时任务配置:/etc/cron.d/tsar，负责每分钟调用tsar执行采集任务； 日志文件轮转配置:/etc/logrotate.d/tsar，每个月会把tsar的本地存储进行轮转； 模块路径：/usr/local/tsar/modules，各个模块的动态库so文件；\n详细参考：https://www.cnblogs.com/zafu/p/11782988.html","title":"tsar安装和使用"},{"content":"概述 Harbor是由VMWare公司开源的容器镜像仓库。事实上，Harbor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制，AD/LDAP集成以及审计日志等，足以满足基本企业需求。 官方：https://goharbor.io/ Github：https://github.com/goharbor/harbor\n部署先决条件 服务器硬件配置： •最低要求：CPU2核/内存4G/硬盘40GB •推荐：CPU4核/内存8G/硬盘160GB 软件： •Docker CE 17.06版本+ •Docker Compose1.18版本+ Harbor安装有2种方式： •在线安装：从Docker Hub下载Harbor相关镜像，因此安装软件包非常小 •离线安装：安装包包含部署的相关镜像，因此安装包比较大\nHarbor部署HTTP 1、先安装Docker和Docker Compose https://github.com/docker/compose/releases\nmv docker-compose-Linux-x86_64 /usr/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/bin/docker-compose 2、部署Harbor HTTP https://github.com/goharbor/harbor/releases\ntar zxvf harbor-offline-installer-v2.0.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml vi harbor.yml hostname: reg.myharbor.com https: # 先注释https相关配置 harbor_admin_password: Harbor12345 ./prepare ./install.sh # 查看已安装的依赖容器 docker-compose ps # 重启docker-compose docker-compose down docker-compose up -d web界面登录：IP：80端口访问， 用户名 admin/Harbor12345\nhttp部署方式基本使用 1、配置http镜像仓库可信任\nvi /etc/docker/daemon.json {\u0026#34;insecure-registries\u0026#34;:[\u0026#34;reg.ctnrs.com\u0026#34;]} systemctl restart docker 2、打标签\ndocker tag centos:7 reg.myharbor.com/library/centos:7 3、上传\ndocker push reg.myharbor.com/library/centos:7 4、下载\ndocker pull reg.myharbor.com/library/centos:7 打标格式：仓库地址/项目名称/镜像名称：版本号\nHarbor 部署HTTPS 1、生成SSL证书 certs.sh cfssl.sh\n# 需要根据harbor设置的域名来修改certs.sh中域名部分 mkdir /root/ssl;cd /root/ssl;sh cfssl.sh sh certs.sh 2、Harbor启用HTTPS\nvi harbor.yml 修改以下内容 https: port: 443 certificate: /root/harbor/ssl/reg.ctnrs.com.pem private_key: /root/harbor/ssl/reg.ctnrs.com-key.pem 3、重新配置并部署Harbor\n./prepare docker-compose down docker-compose up –d 4、将数字证书复制到Docker主机\n# 都以harbor最初设置的域名来命名 mkdir /etc/docker/certs.d/reg.myharbor.com cp reg.myharbor.com.pem /etc/docker/certs.d/reg.myharbor.com/reg.myharbor.com.crt 5、验证 docker login reg.myharbor.com 会自动跳转443的。 ","permalink":"https://wandong1.github.io/post/%E4%BC%81%E4%B8%9A%E7%BA%A7%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor/","summary":"概述 Harbor是由VMWare公司开源的容器镜像仓库。事实上，Harbor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制，AD/LDAP集成以及审计日志等，足以满足基本企业需求。 官方：https://goharbor.io/ Github：https://github.com/goharbor/harbor\n部署先决条件 服务器硬件配置： •最低要求：CPU2核/内存4G/硬盘40GB •推荐：CPU4核/内存8G/硬盘160GB 软件： •Docker CE 17.06版本+ •Docker Compose1.18版本+ Harbor安装有2种方式： •在线安装：从Docker Hub下载Harbor相关镜像，因此安装软件包非常小 •离线安装：安装包包含部署的相关镜像，因此安装包比较大\nHarbor部署HTTP 1、先安装Docker和Docker Compose https://github.com/docker/compose/releases\nmv docker-compose-Linux-x86_64 /usr/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/bin/docker-compose 2、部署Harbor HTTP https://github.com/goharbor/harbor/releases\ntar zxvf harbor-offline-installer-v2.0.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml vi harbor.yml hostname: reg.myharbor.com https: # 先注释https相关配置 harbor_admin_password: Harbor12345 ./prepare ./install.sh # 查看已安装的依赖容器 docker-compose ps # 重启docker-compose docker-compose down docker-compose up -d web界面登录：IP：80端口访问， 用户名 admin/Harbor12345\nhttp部署方式基本使用 1、配置http镜像仓库可信任\nvi /etc/docker/daemon.json {\u0026#34;insecure-registries\u0026#34;:[\u0026#34;reg.","title":"企业级镜像仓库harbor"},{"content":"display clock display license display version dir display boot-loader display cpu-usage display memory display environment display device verbose display device manuinfo display power display fan display alarm display transceiver alarm interface display transceiver diagnosis interface display stp brief display stp root display stp reg display arp display mac-address display vrrp display interface display interface brief display counters inbound interface display counters outbound interface display counters rate inbound interface display counters rate inbound interface display lldp neighbor-information list display irf display irf link display irf topology display logbuffer display current-configuration ","permalink":"https://wandong1.github.io/post/%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87%E5%B7%A1%E6%A3%80%E5%91%BD%E4%BB%A4/","summary":"display clock display license display version dir display boot-loader display cpu-usage display memory display environment display device verbose display device manuinfo display power display fan display alarm display transceiver alarm interface display transceiver diagnosis interface display stp brief display stp root display stp reg display arp display mac-address display vrrp display interface display interface brief display counters inbound interface display counters outbound interface display counters rate inbound interface display counters rate inbound interface display lldp neighbor-information list display irf display irf link display irf topology display logbuffer display current-configuration ","title":"网络设备巡检命令"},{"content":"import requests import json import os, sys class PushMessage: def __init__(self): # self.test_ip_lst = [\u0026#39;114.114.114.114\u0026#39;,\u0026#39;223.5.5.5\u0026#39;] self.test_ip_lst = [\u0026#34;23.134.37.253\u0026#34;,\u0026#34;59.214.26.21\u0026#34;,\u0026#34;23.137.37.1\u0026#34;] self.alarm_info = \u0026#39;电子政务外网地址探测:\\n\u0026#39; self.is_atall = \u0026#39;false\u0026#39; self.dingding_url = \u0026#34;https://oapi.dingtalk.com/robot/send?access_token=1c1\u0026#34; self.push = False def dingding_qun(self): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=utf-8\u0026#39; } url = \u0026#34;\u0026#34;\u0026#34;{}\u0026#34;\u0026#34;\u0026#34;.format(self.dingding_url) # data_markdown = { # \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, # \u0026#34;markdown\u0026#34;: { # \u0026#34;title\u0026#34;:\u0026#34;杭州天气\u0026#34;, # \u0026#34;text\u0026#34;: \u0026#34;#### 杭州天气 @18182294500 \\n\u0026gt; 9度，西北风1级，空气良89，相对温度73%\\n\u0026gt; ![screenshot](https://img.alicdn.com/tfs/TB1NwmBEL9TBuNjy1zbXXXpepXa-2400-1218.png)\\n\u0026gt; ###### 10点20分发布 [天气](https://www.dingalk.com) \\n\u0026#34; # }, # \u0026#34;at\u0026#34;: { # \u0026#34;atMobiles\u0026#34;: [ # \u0026#34;18500\u0026#34; # ], # \u0026#34;atUserIds\u0026#34;: [ # \u0026#34;user123\u0026#34; # ], # \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; # } # } data_text = { \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;{}\u0026#34;.format(self.is_atall) }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;\u0026#34;\u0026#34;{}\u0026#34;\u0026#34;\u0026#34;.format(self.alarm_info) }, \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34; } # post = requests.post(url=url, data=json.dumps(data_markdown), headers=headers) # print(post.json()) post = requests.post(url=url, data=json.dumps(data_text), headers=headers) print(post.json()) def ping_test(self): if sys.platform == \u0026#39;linux\u0026#39;: for ip in self.test_ip_lst: res = os.system(\u0026#39;ping -c 2 {}\u0026#39;.format(ip)) if res: print(\u0026#39;ping test faild!\u0026#39;) self.alarm_info += \u0026#39;[faild] :{} ping test finished!\\n\u0026#39;.format(ip) self.is_atall = \u0026#39;true\u0026#39; self.push = True else: print(\u0026#39;ping test success!\u0026#39;) self.alarm_info += \u0026#39;[success] :{} ping test finished!\\n\u0026#39;.format(ip) elif sys.platform == \u0026#39;win32\u0026#39;: for ip in self.test_ip_lst: res = os.system(\u0026#39;ping -n 2 {}\u0026#39;.format(ip)) if res: print(\u0026#39;ping test faild!\u0026#39;) self.alarm_info += \u0026#39;[faild] :{} ping test finished!\\n\u0026#39;.format(ip) self.is_atall = \u0026#39;true\u0026#39; self.push = True else: print(\u0026#39;ping test success!\u0026#39;) self.alarm_info += \u0026#39;[success] :{} ping test finished!\\n\u0026#39;.format(ip) if self.push: self.dingding_qun() if __name__ == \u0026#39;__main__\u0026#39;: a = PushMessage() a.ping_test() ","permalink":"https://wandong1.github.io/post/%E9%92%89%E9%92%89%E7%BE%A4%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8E%A8%E9%80%81%E8%84%9A%E6%9C%AC/","summary":"import requests import json import os, sys class PushMessage: def __init__(self): # self.test_ip_lst = [\u0026#39;114.114.114.114\u0026#39;,\u0026#39;223.5.5.5\u0026#39;] self.test_ip_lst = [\u0026#34;23.134.37.253\u0026#34;,\u0026#34;59.214.26.21\u0026#34;,\u0026#34;23.137.37.1\u0026#34;] self.alarm_info = \u0026#39;电子政务外网地址探测:\\n\u0026#39; self.is_atall = \u0026#39;false\u0026#39; self.dingding_url = \u0026#34;https://oapi.dingtalk.com/robot/send?access_token=1c1\u0026#34; self.push = False def dingding_qun(self): headers = { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json;charset=utf-8\u0026#39; } url = \u0026#34;\u0026#34;\u0026#34;{}\u0026#34;\u0026#34;\u0026#34;.format(self.dingding_url) # data_markdown = { # \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, # \u0026#34;markdown\u0026#34;: { # \u0026#34;title\u0026#34;:\u0026#34;杭州天气\u0026#34;, # \u0026#34;text\u0026#34;: \u0026#34;#### 杭州天气 @18182294500 \\n\u0026gt; 9度，西北风1级，空气良89，相对温度73%\\n\u0026gt; ![screenshot](https://img.alicdn.com/tfs/TB1NwmBEL9TBuNjy1zbXXXpepXa-2400-1218.png)\\n\u0026gt; ###### 10点20分发布 [天气](https://www.dingalk.com) \\n\u0026#34; # }, # \u0026#34;at\u0026#34;: { # \u0026#34;atMobiles\u0026#34;: [ # \u0026#34;18500\u0026#34; # ], # \u0026#34;atUserIds\u0026#34;: [ # \u0026#34;user123\u0026#34; # ], # \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; # } # } data_text = { \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;{}\u0026#34;.","title":"钉钉群机器人推送脚本"}]