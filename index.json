[{"content":"hexo博客框架的使用 安装node.js 官网： https://nodejs.org/en/\n安装国内淘宝npm npm install -g cnpm --registry=https://registry.npm.taobao.org 安装hexo cnpm install -g hexo-cli\rhexo -v hexo初始化 hexo init 目录 hexo server 新建文章 文章都在source\\_posts目录下\nhexo new \u0026#34;我的第一篇博客文章\u0026#34; 配置后刷新并重启服务 hexo clean hexo g hexo server 更换主题 进入项目目录\ngit clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改项目目录下配置文件_config.yml\ntheme: yilia 修复更换主题后全部文章无法显示问题 # 进入项目目录后 cnpm i hexo-generator-json-content --save # 随后在项目目录下的_config.yml文件后添加内容 jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 刷新并重启项目\n配置yilia主题显示文章目录 进入 themes\\yilia目录下，修改_config.yml文件\ntoc: 2 处理文章图片不显示问题 查看hexo版本，现在我安装的 hexo 版本已经是 4.3.0 了 (2022年)，该方法适用\nhexo version 进入项目目录执行命令\n# 可用cnpm替换 npm install hexo-asset-image --save 插件bug问题，需要做如下修改 进入你博客的根目录，然后下面顺序找到index.js: node_modules \u0026ndash;\u0026gt; hexo-asset-image \u0026ndash;\u0026gt; index.js 用VS Code 或者 记事本打开 index.js 在第 58 行，可以找到这么一行代码： $(this).attr(\u0026#39;src\u0026#39;, config.root + link + src); 把这一行代码改成下面这样 $(this).attr(\u0026#39;src\u0026#39;, src); 随后保存文件\n插入图片的用法 创建文章时，现在插件会自动在文章的_posts目录下创建md文件名相同的目录以存放图片。当然也可以手动创建目录和md文件。\n在文章中写如下内容便可插入图片（注意圆括号内无需写路径，仅写图片的全名即可，前提是图片在对应的文章的目录下）：\n![image-20220809152616143](image-20220809152616143.png) ","permalink":"https://wandong1.github.io/post/hexo%E5%8D%9A%E5%AE%A2%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8/","summary":"hexo博客框架的使用 安装node.js 官网： https://nodejs.org/en/\n安装国内淘宝npm npm install -g cnpm --registry=https://registry.npm.taobao.org 安装hexo cnpm install -g hexo-cli\rhexo -v hexo初始化 hexo init 目录 hexo server 新建文章 文章都在source\\_posts目录下\nhexo new \u0026#34;我的第一篇博客文章\u0026#34; 配置后刷新并重启服务 hexo clean hexo g hexo server 更换主题 进入项目目录\ngit clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 修改项目目录下配置文件_config.yml\ntheme: yilia 修复更换主题后全部文章无法显示问题 # 进入项目目录后 cnpm i hexo-generator-json-content --save # 随后在项目目录下的_config.yml文件后添加内容 jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true 刷新并重启项目","title":"hexo博客框架的使用"},{"content":"\r各类别常见状态码：\n2xx （3种） 200 OK：表示从客户端发送给服务器的请求被正常处理并返回；\n204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；\n206 Patial Content：表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求，响应报文中包含由Content-Range指定范围的实体内容。\n3xx （5种） 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；\n302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；\n301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）\n303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；\n302与303的区别：后者明确表示客户端应当采用GET方式获取资源 304 Not Modified：表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；\n307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；\n4xx （4种） 400 Bad Request：表示请求报文中存在语法错误；\n401 Unauthorized：未经许可，需要通过HTTP认证；\n403 Forbidden：服务器拒绝该次访问（访问权限出现问题）\n404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；\n429 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用。\n5xx （2种） 500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；\n502 bad gateway 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；\n","permalink":"https://wandong1.github.io/post/http%E5%B8%B8%E8%A7%81%E7%8A%B6%E6%80%81%E7%A0%81/","summary":"各类别常见状态码：\n2xx （3种） 200 OK：表示从客户端发送给服务器的请求被正常处理并返回；\n204 No Content：表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；\n206 Patial Content：表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求，响应报文中包含由Content-Range指定范围的实体内容。\n3xx （5种） 301 Moved Permanently：永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；\n302 Found：临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；\n301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）\n303 See Other：表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；\n302与303的区别：后者明确表示客户端应当采用GET方式获取资源 304 Not Modified：表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回改状态码；\n307 Temporary Redirect：临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；\n4xx （4种） 400 Bad Request：表示请求报文中存在语法错误；\n401 Unauthorized：未经许可，需要通过HTTP认证；\n403 Forbidden：服务器拒绝该次访问（访问权限出现问题）\n404 Not Found：表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；\n429 当你需要限制客户端请求某个服务的数量，也就是限制请求速度时，该状态码就会非常有用。\n5xx （2种） 500 Inter Server Error：表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；\n502 bad gateway 503 Server Unavailable：表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；","title":"http常见状态码"},{"content":"下载hugo二进制程序包 下载地址： https://github.com/gohugoio/hugo/releases\n下载后解压、将hugo路径添加到环境变量。先设置hugo变量，然后在path中添加\n验证安装 hugo version 新建站点 hugo new site myblog # 该命令会新建一个文件夹myblog ls ./myblog # archetypes/ config.toml content/ data/ # layouts/ static/ themes/ ##我目前了解如下 #config.toml 进行参数配置，与之后的theme相关 #content 之后博客(.md)的文件都储存在这里 #layout 可个性化修改博客的展示细节，需要懂网络架构知识 #static 储存一些静态文件，比如本地图片，插入到博客中 #themes 主题，接下来会介绍 下载主题（hugo没有默认主题） 有多种hugo主题可供下载：https://themes.gohugo.io/ 推荐主题： https://adityatelange.github.io/hugo-PaperMod/\ncd ./myblog git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod ls ./themes # PaperMod/ ls ./themes/PaperMod/ # LICENSE README.md assets/ go.mod i18n/ images/ layouts/ theme.toml 修改配置 papermod\n通用配置参数查询：https://gohugo.io/getting-started/configuration/\nPaperMod自定义参数查询：https://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n示例配置：https://www.sulvblog.cn/posts/blog/build_hugo/#4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6\n将 config.toml 重命名为 config.yml 然后进行修改\nbaseURL: https://www.sulvblog.cn # baseURL: https://www.sulvblog.cn # 绑定的域名 languageCode: zh-cn # en-us title: 万东的云计算运维博客 theme: hugo-PaperMod # 主题名字，和themes文件夹下的一致 enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false googleAnalytics: UA-123-45 minify: disableXML: true minifyOutput: true params: env: production # to enable google analytics, opengraph, twitter-cards and schema. title: ExampleSite description: \u0026#34;ExampleSite description\u0026#34; keywords: [Blog, Portfolio, PaperMod] author: Me # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;] DateFormat: \u0026#34;January 2, 2006\u0026#34; defaultTheme: dark # dark, light disableThemeToggle: false ShowReadingTime: true ShowShareButtons: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true disableSpecial1stPost: false disableScrollToTop: false comments: false hidemeta: false hideSummary: false showtoc: true tocopen: true searchHidden: true assets: # disableHLJS: true # to disable highlight.js # disableFingerprinting: true favicon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon16x16: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; favicon32x32: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; apple_touch_icon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; safari_pinned_tab: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34; label: text: \u0026#34;万东的云计算运维博客\u0026#34; icon: /apple-touch-icon.png iconHeight: 35 # profile-mode profileMode: enabled: false # needs to be explicitly set title: ExampleSite subtitle: \u0026#34;This is subtitle\u0026#34; imageUrl: \u0026#34;\u0026lt;img location\u0026gt;\u0026#34; imageWidth: 120 imageHeight: 120 imageTitle: my image buttons: - name: 文章 url: posts - name: 标签 url: tags # home-info mode homeInfoParams: Title: \u0026#34;技术学习笔记 \\U0001F44B\u0026#34; Content: 学无止境 socialIcons: - name: github url: \u0026#34;https://github.com/wandong1\u0026#34; analytics: google: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; bing: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; yandex: SiteVerificationTag: \u0026#34;XYZabc\u0026#34; cover: hidden: true # hide everywhere but not in structured data hiddenInList: true # hide on list pages and home hiddenInSingle: true # hide on single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link # for search # https://fusejs.io/api/options.html fuseOpts: isCaseSensitive: false shouldSort: true location: 0 distance: 1000 threshold: 0.4 minMatchCharLength: 0 keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu: main: - identifier: 搜索 name: 搜索 url: search weight: 80 - identifier: 分类 name: 分类 url: /categories/ weight: 10 - identifier: 标签 name: 标签 url: /tags/ weight: 20 - identifier: 归档 name: 归档 url: /archives/ weight: 41 # Read: https://github.com/adityatelange/hugo-PaperMod/wiki/FAQs#using-hugos-syntax-highlighter-chroma pygmentsUseClasses: true markup: highlight: noClasses: false # anchorLineNos: true # codeFences: true # guessSyntax: true # lineNos: true # style: monokai outputs: home: - HTML - RSS - JSON # is necessary 搜索 和 归档功能需要在content目录下创建对应的md文件\n$ cat archive.md\n--- title: \u0026#34;文章归档\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archives/\u0026#34; summary: \u0026#34;archives\u0026#34; --- $ cat search.md\n--- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; --- 个性化修改 转移目录至侧边栏 Pull Request #675 · adityatelange/hugo-PaperMod 提出将文章目录转移至侧边栏，可以轻松实现上下文跳转。截至发文这一特性还未并入主分支，我们可以让主题子模块追踪该远程 PR 分支：\ncd themes/PaperMod git fetch origin pull/675/head:toc-on-the-side --depth=1 git checkout toc-on-the-side cd ../.. 注意：文章名称中的英文字母使用小写，否则图片可能会显示不出来 把更新的博文更新到public目录 hugo --theme=hugo-PaperMod --baseUrl=\u0026#34;https://wandong1.github.io\u0026#34; --buildDrafts ","permalink":"https://wandong1.github.io/post/hugo-%E5%8D%9A%E5%AE%A2%E7%A8%8B%E5%BA%8F%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/","summary":"下载hugo二进制程序包 下载地址： https://github.com/gohugoio/hugo/releases\n下载后解压、将hugo路径添加到环境变量。先设置hugo变量，然后在path中添加\n验证安装 hugo version 新建站点 hugo new site myblog # 该命令会新建一个文件夹myblog ls ./myblog # archetypes/ config.toml content/ data/ # layouts/ static/ themes/ ##我目前了解如下 #config.toml 进行参数配置，与之后的theme相关 #content 之后博客(.md)的文件都储存在这里 #layout 可个性化修改博客的展示细节，需要懂网络架构知识 #static 储存一些静态文件，比如本地图片，插入到博客中 #themes 主题，接下来会介绍 下载主题（hugo没有默认主题） 有多种hugo主题可供下载：https://themes.gohugo.io/ 推荐主题： https://adityatelange.github.io/hugo-PaperMod/\ncd ./myblog git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod ls ./themes # PaperMod/ ls ./themes/PaperMod/ # LICENSE README.md assets/ go.mod i18n/ images/ layouts/ theme.toml 修改配置 papermod\n通用配置参数查询：https://gohugo.io/getting-started/configuration/\nPaperMod自定义参数查询：https://adityatelange.github.io/hugo-PaperMod/posts/papermod/papermod-features/\n示例配置：https://www.sulvblog.cn/posts/blog/build_hugo/#4%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6\n将 config.toml 重命名为 config.yml 然后进行修改","title":"Hugo 博客程序搭建教程"},{"content":"生成管理员证书 cat \u0026gt; admin-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 执行生成命令 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kubeconfig文件 # 设置集群参数 kubectl config set-cluster kubernetes \\ --server=https://192.168.0.149:6443 \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --kubeconfig=config # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=cluster-admin \\ --kubeconfig=config # 设置客户端认证参数 kubectl config set-credentials cluster-admin \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --client-key=admin-key.pem \\ --client-certificate=admin.pem \\ --kubeconfig=config # 设置默认上下文 kubectl config use-context default --kubeconfig=config \u0026ndash;certificate-authority： 验证 kube-apiserver 证书的根证书； \u0026ndash;client-certificate、\u0026ndash;client-key： 刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用； \u0026ndash;embed-certs=true： 将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)；\n设置客户端认证参数时 \u0026ndash;certificate-authority=ca.pem ##添加管理员权限，没有这一段则为普通用户\n指定config配置文件执行命令 kubectl --kubeconfig=config get node ","permalink":"https://wandong1.github.io/post/k8s%E6%A0%B9%E6%8D%AE%E7%8E%B0%E6%9C%89%E8%AF%81%E4%B9%A6%E7%94%9F%E6%88%90%E7%AE%A1%E7%90%86%E5%91%98kubeconfig%E6%96%87%E4%BB%B6/","summary":"生成管理员证书 cat \u0026gt; admin-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } EOF 执行生成命令 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建kubeconfig文件 # 设置集群参数 kubectl config set-cluster kubernetes \\ --server=https://192.168.0.149:6443 \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --kubeconfig=config # 设置上下文参数 kubectl config set-context default \\ --cluster=kubernetes \\ --user=cluster-admin \\ --kubeconfig=config # 设置客户端认证参数 kubectl config set-credentials cluster-admin \\ --certificate-authority=ca.","title":"K8S根据现有证书生成管理员kubeconfig文件"},{"content":"软件包下载 https://www.mongodb.com/try/download/community wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-5.0.8.tgz mongodb模式介绍 三节点复制集模式 常见的复制集架构由3个成员节点组成，其中存在几种不同的模式。 PSS模式（官方推荐模式） PSS模式由一个主节点和两个备节点所组成，即Primary+Secondary+Secondary。 此模式始终提供数据集的两个完整副本，如果主节点不可用，则复制集选择备节点作为主节 点并继续正常操作。旧的主节点在可用时重新加入复制集。 复制集部署注意事项 关于硬件: 因为正常的复制集节点都有可能成为主节点，它们的地位是一样的，因此硬件配 置上必须一致; 为了保证节点不会同时宕机，各节点使用的硬件必须具有独立性。\n关于软件: 复制集各节点软件版本必须一致，以避免出现不可预知的问题。 增加节点不会增加系统写性能；\n准备配置文件 复制集的每个mongod进程应该位于不同的服务器。我们现在在一台机器上运行3个进程， 因此要为它们各自配置：\n不同的端口 （28017/28018/28019）\n不同的数据目录 mkdir ‐p /data/db{1,2,3} 不同日志文件路径 (例如：/data/db1/mongod.log)\n创建配置文件/data/db1/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db1/mongod.log logAppend: true storage: dbPath: /data/db1 net: bindIp: 0.0.0.0 port: 28017 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db2/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db2/mongod.log logAppend: true storage: dbPath: /data/db2 net: bindIp: 0.0.0.0 port: 28018 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db3/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db3/mongod.log logAppend: true storage: dbPath: /data/db3 net: bindIp: 0.0.0.0 port: 28019 replication: replSetName: rs0 processManagement: fork: true 启动mongdb mongod -f /data/db1/mongod.conf mongod -f /data/db2/mongod.conf mongod -f /data/db3/mongod.conf 初始化配置复制集 mongo --port 28017 rs.initiate({ _id:\u0026#34;rs0\u0026#34;, members:[{ _id:0, host:\u0026#34;localhost:28017\u0026#34; },{ _id:1, host:\u0026#34;localhost:28018\u0026#34; },{ _id:2, host:\u0026#34;localhost:28019\u0026#34; }] }) 开启安全认证 创建认证用户\nuse admin db.createUser( { user: \u0026#34;root\u0026#34;, pwd: \u0026#34;Aliyun2022\u0026#34;, roles: [ { role: \u0026#34;clusterAdmin\u0026#34;, db: \u0026#34;admin\u0026#34; } , { role: \u0026#34;userAdminAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}, { role: \u0026#34;userAdminAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}, { role: \u0026#34;readWriteAnyDatabase\u0026#34;, db: \u0026#34;admin\u0026#34;}] }) 生成复制集间通信的认证key文件\nopenssl rand -base64 756 \u0026gt; /data/mongo.key 重新启动mongdb mongod -f /data/db1/mongod.conf --keyFile /data/mongo.key mongod -f /data/db2/mongod.conf --keyFile /data/mongo.key mongod -f /data/db3/mongod.conf --keyFile /data/mongo.key 使用账户密码登录 mongo \u0026ndash;port 28019 -uroot -pAliyun2022 \u0026ndash;authenticationDatabase=admin\n或者用uri方式登录 mongodb://root:Aliyun2022@192.168.65.174:28017,192.168.65.174:28018,192.168.65.174:28019/test?authSource=admin\u0026amp;replicaSet=rs0\n","permalink":"https://wandong1.github.io/post/mongodb%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","summary":"软件包下载 https://www.mongodb.com/try/download/community wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-5.0.8.tgz mongodb模式介绍 三节点复制集模式 常见的复制集架构由3个成员节点组成，其中存在几种不同的模式。 PSS模式（官方推荐模式） PSS模式由一个主节点和两个备节点所组成，即Primary+Secondary+Secondary。 此模式始终提供数据集的两个完整副本，如果主节点不可用，则复制集选择备节点作为主节 点并继续正常操作。旧的主节点在可用时重新加入复制集。 复制集部署注意事项 关于硬件: 因为正常的复制集节点都有可能成为主节点，它们的地位是一样的，因此硬件配 置上必须一致; 为了保证节点不会同时宕机，各节点使用的硬件必须具有独立性。\n关于软件: 复制集各节点软件版本必须一致，以避免出现不可预知的问题。 增加节点不会增加系统写性能；\n准备配置文件 复制集的每个mongod进程应该位于不同的服务器。我们现在在一台机器上运行3个进程， 因此要为它们各自配置：\n不同的端口 （28017/28018/28019）\n不同的数据目录 mkdir ‐p /data/db{1,2,3} 不同日志文件路径 (例如：/data/db1/mongod.log)\n创建配置文件/data/db1/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db1/mongod.log logAppend: true storage: dbPath: /data/db1 net: bindIp: 0.0.0.0 port: 28017 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db2/mongod.conf，内容如下：\nsystemLog: destination: file path: /data/db2/mongod.log logAppend: true storage: dbPath: /data/db2 net: bindIp: 0.0.0.0 port: 28018 replication: replSetName: rs0 processManagement: fork: true 创建配置文件/data/db3/mongod.","title":"mongodb的安装部署"},{"content":"参考地址：https://www.bilibili.com/read/cv15128680\n","permalink":"https://wandong1.github.io/post/navicat15%E9%83%A8%E7%BD%B2/","summary":"参考地址：https://www.bilibili.com/read/cv15128680","title":"navicat15部署"},{"content":"user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } # 4层 stream配置 stream { log_format main \u0026#39;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent\u0026#39;; access_log /var/log/nginx/dingding-access.log main; upstream dingding { server oapi.dingtalk.com:80; } upstream dingding_v2 { server oapi.dingtalk.com:443; } upstream apsoar { server soar.apsoar.com:22; } upstream timor { server timor.tech:443; } upstream timor80 { server timor.tech:80; } server { listen 22; proxy_pass apsoar; } server { listen 7003; proxy_pass timor80; } server { listen 80; proxy_pass dingding; } server { listen 443; proxy_pass dingding_v2; } } # 7层http配置 http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; server { listen 9999 default_server; location / { proxy_pass http://timor.tech; } } server { listen 19999 default_server; location / { proxy_pass https://timor.tech; } } include /etc/nginx/conf.d/*.conf; } ","permalink":"https://wandong1.github.io/post/nginx%E7%9A%84%E5%9B%9B%E5%B1%82%E8%BD%AC%E5%8F%91%E5%92%8C%E4%B8%83%E5%B1%82%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B/","summary":"user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } # 4层 stream配置 stream { log_format main \u0026#39;$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent\u0026#39;; access_log /var/log/nginx/dingding-access.log main; upstream dingding { server oapi.dingtalk.com:80; } upstream dingding_v2 { server oapi.dingtalk.com:443; } upstream apsoar { server soar.apsoar.com:22; } upstream timor { server timor.tech:443; } upstream timor80 { server timor.tech:80; } server { listen 22; proxy_pass apsoar; } server { listen 7003; proxy_pass timor80; } server { listen 80; proxy_pass dingding; } server { listen 443; proxy_pass dingding_v2; } } # 7层http配置 http { include /etc/nginx/mime.","title":"nginx的四层转发和七层转发配置示例"},{"content":"一、优化nginx进程数以及cpu分布 修改配置文件 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 几核就有几位二进制数，1在哪位就表示在哪个核心上。\n查看nginx worker进程分布在cpu的情况 ps -axo pid,psr,cmd,ni | grep -i \u0026#34;worker process\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sort -n | uniq -c 二、优化文件数\nulimit -n #查看文件数限制 ulimit -SHn 65535 （注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) #有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535 nginx配置修改：\nworker_rlimit_nofile 65535; 三、使用epoll的I/O模型，用这个模型来高效处理异步事件 在events区块中添加 use epoll; 四、每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为 # worker_processes*worker_connections。为理论上最大连接数 在events区块中添加或修改 worker_connections 65535; 五、http连接超时时间 默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！\nkeepalive_timeout 60; 六、linux内核参数优化 cat \u0026gt; /etc/sysctl.d/youhua.conf \u0026lt;\u0026lt; EOF net.ipv4.tcp_syncookies=1 net.core.somaxconn=32768 net.ipv4.tcp_max_syn_backlog=32768 net.ipv4.ip_local_port_range=15000 65000 EOF sysctl --system [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_syncookies=1\u0026#34; [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_tw_reuse=1\u0026#34; [root@master01 ~]# ansible all -m shell -a \u0026#34;sysctl -w net.ipv4.tcp_tw_recycle=1\u0026#34; 七、AB简单压测命令 yum -y install httpd-tools ab -n 500000 -c 2000 -k http://10.43.152.61/\nsysctl -w net.ipv4.tcp_max_tw_buckets=30000\njmeter压测配置 http请求\n","permalink":"https://wandong1.github.io/post/nginx%E8%B0%83%E4%BC%98%E6%96%B9%E6%B3%95/","summary":"一、优化nginx进程数以及cpu分布 修改配置文件 worker_processes 8; worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 几核就有几位二进制数，1在哪位就表示在哪个核心上。\n查看nginx worker进程分布在cpu的情况 ps -axo pid,psr,cmd,ni | grep -i \u0026#34;worker process\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | sort -n | uniq -c 二、优化文件数\nulimit -n #查看文件数限制 ulimit -SHn 65535 （注ulimit -SHn 65535 等效 ulimit -n 65535，-S指soft，-H指hard) #有如下三种修改方式： 1.在/etc/rc.local 中增加一行 ulimit -SHn 65535 2.在/etc/profile 中增加一行 ulimit -SHn 65535 3.在/etc/security/limits.conf最后增加如下两行记录 * soft nofile 65535 * hard nofile 65535 nginx配置修改：\nworker_rlimit_nofile 65535; 三、使用epoll的I/O模型，用这个模型来高效处理异步事件 在events区块中添加 use epoll; 四、每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为 # worker_processes*worker_connections。为理论上最大连接数 在events区块中添加或修改 worker_connections 65535; 五、http连接超时时间 默认是60s，功能是使客户端到服务器端的连接在设定的时间内持续有效，当出现对服务器的后继请求时，该功能避免了建立或者重新建立连接。切记这个参数也不能设置过大！否则会导致许多无效的http连接占据着nginx的连接数，终nginx崩溃！","title":"nginx调优方法"},{"content":"一、prometheus监控 https://prometheus.io/download/ ###下载源码解压即可 https://grafana.com/grafana/dashboards ###搜索数据源为prometheus的\n安装docker mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce systemctl enable docker --now 安装grafana wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpm yum localinstall grafana-5.3.4-1.x86_64.rpm systemctl start grafana-server 默认密码 admin/admin prometheus安装 tar -xvzf prometheus-2.34.0.linux-amd64.tar.gz -C /opt/ mv /opt/prometheus-2.34.0.linux-amd64 /opt/prometheus cd /opt/prometheus \u0026amp;\u0026amp; mkdir data # 创建启动脚本 cat \u0026lt;\u0026lt;EOF \u0026gt;start.sh #!/bin/bash ./prometheus --storage.tsdb.path=./data --storage.tsdb.retention.time=744h --web.enable-lifecycle --storage.tsdb.no-lockfile EOF # storage.tsdb.retention.time为数据保存时间 注：创建data目录，最好单独挂载到一块盘\n修改Prometheus配置文件 # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;. rule_files: # - \u0026#34;first_rules.yml\u0026#34; # - \u0026#34;second_rules.yml\u0026#34; # A scrape configuration containing exactly one endpoint to scrape: # Here it\u0026#39;s Prometheus itself. scrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: \u0026#34;prometheus\u0026#34; # metrics_path defaults to \u0026#39;/metrics\u0026#39; # scheme defaults to \u0026#39;http\u0026#39;. static_configs: - targets: [\u0026#34;localhost:9090\u0026#34;] - job_name: \u0026#39;K8S-Cluster\u0026#39; file_sd_configs: - refresh_interval: 10s files: - \u0026#34;/opt/prometheus/sd_config/K8S.yml\u0026#34; sd_config K8S.yml模板文件：\n[ {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.50:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.50\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.51:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.51\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.52:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.52\u0026#34;}}, {\u0026#34;targets\u0026#34;: [\u0026#34;10.43.152.56:9100\u0026#34;],\u0026#34;labels\u0026#34;: {\u0026#34;instance\u0026#34;: \u0026#34;10.43.152.56\u0026#34;}}, ] 启动Prometheus cd /opt/prometheus \u0026amp;\u0026amp; nohup sh start.sh 2\u0026gt;\u0026amp;1 \u0026gt; prometheus.log \u0026amp; 安装监控插件 node主机监控 grafana 大屏 node监控 下载node_exporter-1.3.1.linux-amd64.tar.gz 放至被监控机器的tmp目录。\ntar -xzvf /tmp/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/ mv /usr/local/node_exporter-1.3.1.linux-amd64 /usr/local/node_exporter groupadd prometheus ;useradd -g prometheus -s /sbin/nologin prometheus chown -Rf prometheus:prometheus /usr/local/node_exporter cat \u0026gt; /usr/lib/systemd/system/node_exporter.service \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; [Unit] Description=node_exporter Documentation=https://prometheus.io/ After=network.target [Service] Type=simple User=prometheus ExecStart=/usr/local/node_exporter/node_exporter Restart=on-failure [Install] WantedBy=multi-user.target EOF systemctl enable node_exporter --now 注：在启动项添加 \u0026ndash;web.listen-address=\u0026quot;:9200\u0026quot; 可修改默认端口9100\nwindows主机监控 监控redis grafana 大屏 Redis Dashboard-1562834847307\ndocker run -d -p 9121:9121 -e REDIS_ADDR=\u0026#34;redis://10.43.152.50:10001\u0026#34; -e REDIS_PASSWORD=\u0026#34;f1543f7c\u0026#34; oliver006/redis_exporter 参考配置 ","permalink":"https://wandong1.github.io/post/prometheus%E7%9B%B4%E6%8E%A5%E5%AE%89%E8%A3%85/","summary":"一、prometheus监控 https://prometheus.io/download/ ###下载源码解压即可 https://grafana.com/grafana/dashboards ###搜索数据源为prometheus的\n安装docker mkdir /etc/yum.repos.d/back mv /etc/yum.repos.d/* /etc/yum.repos.d/back wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum install -y yum-utils yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce systemctl enable docker --now 安装grafana wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpm yum localinstall grafana-5.3.4-1.x86_64.rpm systemctl start grafana-server 默认密码 admin/admin prometheus安装 tar -xvzf prometheus-2.34.0.linux-amd64.tar.gz -C /opt/ mv /opt/prometheus-2.34.0.linux-amd64 /opt/prometheus cd /opt/prometheus \u0026amp;\u0026amp; mkdir data # 创建启动脚本 cat \u0026lt;\u0026lt;EOF \u0026gt;start.sh #!/bin/bash ./prometheus --storage.tsdb.path=./data --storage.tsdb.retention.time=744h --web.enable-lifecycle --storage.tsdb.no-lockfile EOF # storage.","title":"prometheus直接安装"},{"content":"https://www.rainbond.com/docs/ops-guide/storage/deploy-nfsclient\n添加helm仓库 helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond helm repo update nfs或者nas配置文件 nfs-client.yaml\nnfs: server: 1afc54bbca-jna4.cn-chongqing-cqzwy-d01.nas.alinetops.cqzwy.com #nfs server地址 path: / #nfs server 的路径 mountOptions: #添加参数 - hard - vers=4 - nolock - proto=tcp - rsize=1048576 - wsize=1048576 - timeo=600 - retrans=2 - noresvport 部署 helm install nfs-client-provisioner rainbond/nfs-client-provisioner \\ -f nfs-client.yaml \\ --version 1.2.8 创建有状态应用进行绑定 ","permalink":"https://wandong1.github.io/post/rainbond%E5%AF%B9%E6%8E%A5nfs%E6%88%96%E8%80%85nas%E5%AD%98%E5%82%A8/","summary":"https://www.rainbond.com/docs/ops-guide/storage/deploy-nfsclient\n添加helm仓库 helm repo add rainbond https://openchart.goodrain.com/goodrain/rainbond helm repo update nfs或者nas配置文件 nfs-client.yaml\nnfs: server: 1afc54bbca-jna4.cn-chongqing-cqzwy-d01.nas.alinetops.cqzwy.com #nfs server地址 path: / #nfs server 的路径 mountOptions: #添加参数 - hard - vers=4 - nolock - proto=tcp - rsize=1048576 - wsize=1048576 - timeo=600 - retrans=2 - noresvport 部署 helm install nfs-client-provisioner rainbond/nfs-client-provisioner \\ -f nfs-client.yaml \\ --version 1.2.8 创建有状态应用进行绑定 ","title":"rainbond对接nfs或者nas存储"},{"content":"https://lequ7.com/guan-yu-paas-ping-tai-rainbond-tong-guo-cha-jian-zheng-he-elkefk-shi-xian-ri-zhi-shou-ji.html\n通过helm部署filebeat采集容器日志 helm repo add elastic https://helm.elastic.co helm pull elastic/filebeat tar -xvzf filebeat-7.17.3.tgz \u0026amp;\u0026amp; cd filebeat hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#39;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; 进入容器内ping es的svc即可获得svc全称\nhelm install filebeat -n 013497775a1b4580924a00009a20c887 ./filebeat helm list -A 或者使用yaml直接不部署\n--- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: # Mounted `filebeat-inputs` configmap: path: ${path.config}/inputs.d/*.yml # Reload inputs configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.enabled: false output.elasticsearch: hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce:9200\u0026#39;] indices: - index: \u0026#34;filebeat-7.15.2-2022.06.02-000001\u0026#34; username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-inputs namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: kubernetes.yml: |- - type: docker containers.ids: - \u0026#34;*\u0026#34; processors: - add_kubernetes_metadata: in_cluster: true --- apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: elastic/filebeat:7.15.2 args: [ \u0026#34;-c\u0026#34;, \u0026#34;/etc/filebeat.yml\u0026#34;, \u0026#34;-e\u0026#34;, ] securityContext: runAsUser: 0 # If using Red Hat OpenShift uncomment this: #privileged: true resources: limits: memory: 500Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs # data folder stores a registry of read status for all files, so we don\u0026#39;t send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: 013497775a1b4580924a00009a20c887 roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat ","permalink":"https://wandong1.github.io/post/rainbond%E5%B9%B3%E5%8F%B0%E9%83%A8%E7%BD%B2es-filebeat/","summary":"https://lequ7.com/guan-yu-paas-ping-tai-rainbond-tong-guo-cha-jian-zheng-he-elkefk-shi-xian-ri-zhi-shou-ji.html\n通过helm部署filebeat采集容器日志 helm repo add elastic https://helm.elastic.co helm pull elastic/filebeat tar -xvzf filebeat-7.17.3.tgz \u0026amp;\u0026amp; cd filebeat hosts: [\u0026#39;cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#39;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; 进入容器内ping es的svc即可获得svc全称\nhelm install filebeat -n 013497775a1b4580924a00009a20c887 ./filebeat helm list -A 或者使用yaml直接不部署\n--- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: 013497775a1b4580924a00009a20c887 labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: # Mounted `filebeat-inputs` configmap: path: ${path.config}/inputs.d/*.yml # Reload inputs configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.","title":"rainbond平台部署ES-filebeat"},{"content":"Rancher Rancher 是一套容器管理平台，它可以帮助组织在生产环境中轻松快捷的部署和管理容器。 Rancher 可以轻松地管理各种环境的 Kubernetes，满足 IT 需求并为 DevOps 团队提供支持。\nRancher 四个组成部分 Rancher 由以下四个部分组成：\n1、基础设施编排\nRancher 可以使用任何公有云或者私有云的 Linux 主机资源。Linux 主机可以是虚拟机，也可以是 物理机。\n2、容器编排与调度\n很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher 包含了当前全部主流的编排 调度引擎，例如 Docker Swarm， Kubernetes， 和 Mesos。同一个用户可以创建 Swarm 或者 Kubernetes 集群。并且可以使用原生的 Swarm 或者 Kubernetes 工具管理应用。 除了 Swarm，Kubernetes 和 Mesos 之外，Rancher 还支持自己的 Cattle 容器编排调度引擎。 Cattle 被广泛用于编排 Rancher 自己的基础设施服务以及用于 Swarm 集群，Kubernetes 集群和 Mesos 集群的配置，管理与升级。\n3、应用商店\nRancher 的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应 用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher 提供了一个由 Rancher 社区维 护的应用商店，其中包括了一系列的流行应用。Rancher 的用户也可以创建自己的私有应用商店。\n4、企业级权限管理\nRancher 支持灵活的插件式的用户认证。支持 Active Directory，LDAP， Github 等 认证方 式。\n使用 Rancher 搭建 k8s 集群 初始化安装机环境 关闭selinux和防火墙\nsystemctl stop firewalld.service \u0026amp;\u0026amp; systemctl disable firewalld.service \u0026amp;\u0026amp; iptables -F \u0026amp;\u0026amp;setenforce 0 hostnamectl set-hostname master01\rhostnamectl set-hostname node01\rhostnamectl set-hostname node02 安装 docker 环境依赖\n在线\nyum install -y yum-utils yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install docker-ce docker-ce-cli containerd.io -y systemctl start docker \u0026amp;\u0026amp; systemctl enable docker.service \u0026amp;\u0026amp; systemctl status docker 离线安装docker\n#!/bin/bash\r#解压\rdockers images \u0026gt;\u0026gt;/dev/null\rif [ $? -eq 0 ];then\recho \u0026#34;docker is installed!!\u0026#34;\relse\rtar -xvzf docker-20.10.14.tgz -C /opt/\rchown root:root -R /opt/docker/\rcp /opt/docker/* /usr/bin\rcat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service\r[Unit]\rDescription=Docker Application Container Engine\rDocumentation=https://docs.docker.com\rAfter=network-online.target firewalld.service\rWants=network-online.target\r[Service]\rType=notify\rExecStart=/usr/bin/dockerd\rExecReload=/bin/kill -s HUP \\$MAINPID\rLimitNOFILE=infinity\rLimitNPROC=infinity\rTimeoutStartSec=0\rDelegate=yes\rKillMode=process\rRestart=on-failure\rStartLimitBurst=3\rStartLimitInterval=60s\r[Install]\rWantedBy=multi-user.target\rEOF\rchmod +x /etc/systemd/system/docker.service\r# 加载service配置\rsystemctl daemon-reload \u0026amp;\u0026amp; echo \u0026#34;加载service配置 success!\u0026#34;\r# dockerDamon\rtee /etc/docker/daemon.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39;\r{\r\u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://dockerhub.azk8s.cn\u0026#34;,\u0026#34;http://hubmirror.c.163.com\u0026#34;,\u0026#34;http://qtid6917.mirror.aliyuncs.com\u0026#34;]\r}\rEOF\recho \u0026#34;docker daemon.json edit success!\u0026#34;\r#设置开机启动 并立即启动\rsystemctl enable docker.service --now \u0026amp;\u0026amp; echo \u0026#34;docker start success!\u0026#34;\rfi 镜像加速，每台机器执行\ntee /etc/docker/daemon.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;:[\u0026#34;https://dockerhub.azk8s.cn\u0026#34;,\u0026#34;http://hubmirror.c.163.com\u0026#34;,\u0026#34;http://qtid6917.mirror.aliyuncs.com\u0026#34;] } EOF 安装 rancher 平台\n离线导入rancher镜像\ndocker run -d --restart=unless-stopped -p 80:80 -p 443:443 -- privileged rancher/rancher 注：\u0026ndash;restart=unless-stopped ，在容器退出时总是重启容器，但是不考虑在 Docker 守护进程 启动时就已经停止了的容器\n安装过程如果失败需要清理后再重新安装K8S节点\n#!/bin/bash #df -h|grep kubelet |awk -F % \u0026#39;{print $2}\u0026#39;|xargs umount sudo rm /var/lib/kubelet/* -rf sudo rm /etc/kubernetes/* -rf sudo rm /etc/cni/* -rf sudo rm /var/lib/rancher/* -rf sudo rm /var/lib/etcd/* -rf sudo rm /var/lib/cni/* -rf sudo rm /opt/cni/* -rf sudo ip link del flannel.1 ip link del cni0 iptables -F \u0026amp;\u0026amp; iptables -t nat -F docker rm -f `docker ps -a -q` docker volume ls|awk \u0026#39;{print $2}\u0026#39;|xargs docker volume rm systemctl restart docker ","permalink":"https://wandong1.github.io/post/rancher/","summary":"Rancher Rancher 是一套容器管理平台，它可以帮助组织在生产环境中轻松快捷的部署和管理容器。 Rancher 可以轻松地管理各种环境的 Kubernetes，满足 IT 需求并为 DevOps 团队提供支持。\nRancher 四个组成部分 Rancher 由以下四个部分组成：\n1、基础设施编排\nRancher 可以使用任何公有云或者私有云的 Linux 主机资源。Linux 主机可以是虚拟机，也可以是 物理机。\n2、容器编排与调度\n很多用户都会选择使用容器编排调度框架来运行容器化应用。Rancher 包含了当前全部主流的编排 调度引擎，例如 Docker Swarm， Kubernetes， 和 Mesos。同一个用户可以创建 Swarm 或者 Kubernetes 集群。并且可以使用原生的 Swarm 或者 Kubernetes 工具管理应用。 除了 Swarm，Kubernetes 和 Mesos 之外，Rancher 还支持自己的 Cattle 容器编排调度引擎。 Cattle 被广泛用于编排 Rancher 自己的基础设施服务以及用于 Swarm 集群，Kubernetes 集群和 Mesos 集群的配置，管理与升级。\n3、应用商店\nRancher 的用户可以在应用商店里一键部署由多个容器组成的应用。用户可以管理这个部署的应 用，并且可以在这个应用有新的可用版本时进行自动化的升级。Rancher 提供了一个由 Rancher 社区维 护的应用商店，其中包括了一系列的流行应用。Rancher 的用户也可以创建自己的私有应用商店。\n4、企业级权限管理\nRancher 支持灵活的插件式的用户认证。支持 Active Directory，LDAP， Github 等 认证方 式。","title":"rancher的安装和使用"},{"content":"1、缓存击穿，某些场景下，大量的key同时失效，请求直接穿过redis缓存层打到数据库上。 解决方法：对key的失效时间设置随机值避免同时失效。\n2、缓存穿透，请求进来请求本就不存在的数据，redis层找不到数据库也找不到，每个这种请求都会打到数据库造成压力。 解决方法：对请求的后端数据库不存在的数据，设置空缓存，避免恶意请求对后端数据库造成压力。\n3、雪崩，指流量进来打到redis，redis由于某些原因扛不住，流量又会打到数据库，数据库很显然更抗不住。造成系统雪崩。 解决方法：redis采用高可用的集群架构，针对某些bigkey进行打散操作。\n4、redis常见数据类型。 string，hash，list，set ，有序set。\n5、Redis有哪些适合的场景? (1)Session共享(单点登录);(2)页面缓存;(3)队列;(4)排行榜/计数器;(5)发布/订阅;\n(1)LUA脚本：在事务的基础上，假如，需要在服务端一次性的执行更复杂的操作，那么，这个时候lua就可以上场了。\n","permalink":"https://wandong1.github.io/post/redis%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","summary":"1、缓存击穿，某些场景下，大量的key同时失效，请求直接穿过redis缓存层打到数据库上。 解决方法：对key的失效时间设置随机值避免同时失效。\n2、缓存穿透，请求进来请求本就不存在的数据，redis层找不到数据库也找不到，每个这种请求都会打到数据库造成压力。 解决方法：对请求的后端数据库不存在的数据，设置空缓存，避免恶意请求对后端数据库造成压力。\n3、雪崩，指流量进来打到redis，redis由于某些原因扛不住，流量又会打到数据库，数据库很显然更抗不住。造成系统雪崩。 解决方法：redis采用高可用的集群架构，针对某些bigkey进行打散操作。\n4、redis常见数据类型。 string，hash，list，set ，有序set。\n5、Redis有哪些适合的场景? (1)Session共享(单点登录);(2)页面缓存;(3)队列;(4)排行榜/计数器;(5)发布/订阅;\n(1)LUA脚本：在事务的基础上，假如，需要在服务端一次性的执行更复杂的操作，那么，这个时候lua就可以上场了。","title":"redis的常见问题"},{"content":"zookeeper 官网 https://zookeeper.apache.org/ 找download\n一、下载软件包 https://dlcdn.apache.org/zookeeper/zookeeper-3.8.0/apache-zookeeper-3.8.0-bin.tar.gz\n二、集群部署 1、安装JDK centos\nyum install java-1.8.0-openjdk* -y 2、zk配置文件 # The number of milliseconds of each tick\rtickTime=2000\r# The number of ticks that the initial # synchronization phase can take\rinitLimit=10\r# The number of ticks that can pass between # sending a request and getting an acknowledgement\rsyncLimit=5\r# the directory where the snapshot is stored.\r# do not use /tmp for storage, /tmp here is just # example sakes.\rdataDir=/opt/zookeeper/data\r# the port at which the clients will connect\rclientPort=2181\r# the maximum number of client connections.\r# increase this if you need to handle more clients\r#maxClientCnxns=60\r#\r# Be sure to read the maintenance section of the # administrator guide before turning on autopurge.\r#\r# https://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance\r#\r# The number of snapshots to retain in dataDir\r#autopurge.snapRetainCount=3\r# Purge task interval in hours\r# Set to \u0026#34;0\u0026#34; to disable auto purge feature\r#autopurge.purgeInterval=1\r## Metrics Providers\r#\r# https://prometheus.io Metrics Exporter\r#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider\r#metricsProvider.httpHost=0.0.0.0\r#metricsProvider.httpPort=7000\r#metricsProvider.exportJvmInfo=true\rserver.1=10.43.152.67:2888:3888\rserver.2=10.43.152.68:2888:3888\rserver.3=10.43.152.69:2888:3888 注：每个zk节点，的dataDir目录下必须有myid文件，myid文件中的内容必须为对应的 server.【编号】=10.43.152.67:2888:3888 中的编号\n3、解压zk并启动 zookeeper前台启动命令\nbin/zkServer.sh start-foreground conf/zoo.cfg ","permalink":"https://wandong1.github.io/post/zookeeper/","summary":"zookeeper 官网 https://zookeeper.apache.org/ 找download\n一、下载软件包 https://dlcdn.apache.org/zookeeper/zookeeper-3.8.0/apache-zookeeper-3.8.0-bin.tar.gz\n二、集群部署 1、安装JDK centos\nyum install java-1.8.0-openjdk* -y 2、zk配置文件 # The number of milliseconds of each tick\rtickTime=2000\r# The number of ticks that the initial # synchronization phase can take\rinitLimit=10\r# The number of ticks that can pass between # sending a request and getting an acknowledgement\rsyncLimit=5\r# the directory where the snapshot is stored.\r# do not use /tmp for storage, /tmp here is just # example sakes.","title":"zookeeper的安装和使用"},{"content":"Helm Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前\n打包好的yaml文件部署到kubernetes上。\nHelm有3个重要概念：\n• **helm：**一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。\n• **Chart：**应用描述，一系列用于描述 k8s 资源相关文件的集合。\n• **Release：**基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在\nk8s中创建出真实运行的资源对象。\nHelm客户端 使用helm很简单，你只需要下载一个二进制客户端包即可，会通过kubeconfig配置（通常$HOME/.kube/config）来连接Kubernetes。\n项目地址：https://github.com/helm/helm\n下载Helm客户端：\nwget https://get.helm.sh/helm-v3.4.2-linux-amd64.tar.gz tar zxvf helm-v3.4.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ Helm常用命令 Helm管理应用生命周期： • helm create 创建Chart示例\n• helm install 部署\n• helm upgrade 更新\n• helm rollback 回滚\n• helm uninstall 卸载\nHelm基本使用：创建Chart示例 创建chart：\n# 默认示例中部署的是一个nginx服务 helm create mychart 打包chart：\nhelm package mychart • charts：目录里存放这个chart依赖的所有子chart。\n• Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。\n• values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。\n• Templates： 目录里面存放所有yaml模板文件。\n• NOTES.txt ：用于介绍Chart帮助信息， helm install 部署后展示给用户。例如：\n如何使用这个 Chart、列出缺省的设置等。\n• _helpers.tpl：放置模板的地方，可以在整个 chart 中重复使用。\nHelm基本使用：部署 部署Chart：\nhelm install web mychart 查看Release：\nhelm list -n default 查看部署的Pod：\nkubectl get pods,svc Helm基本使用：升级 使用Chart升级应用有两种方法：\n• \u0026ndash;values，-f：指定YAML文件覆盖值\n• \u0026ndash;set：在命令行上指定覆盖值\n注：如果一起使用，\u0026ndash;set优先级高\n例如将nginx服务升级到1.17版本：\n第一种方式： # vi values.yaml #任意路径 image: tag: \u0026#34;1.17“ helm upgrade -f values.yaml web mychart 第二种方式： helm upgrade --set image.tag=1.17 web mychart Helm基本使用：回滚、卸载 回滚到上一个版本：\nhelm rollback web 查看历史版本：\nhelm history web 回滚到指定版本：\nhelm rollback web 2 卸载应用：\nhelm uninstall web Helm工作流程 Helm模板 Helm核心是模板，即模板化K8s YAML文件。\n通过模板实现Chart高效复用，当部署多个应用时，可以将差异化的字段进行模板化，在部署时使用-f或\n者\u0026ndash;set动态覆盖默认值，从而适配多个应用。\nHelm模板由Go Template编写，指令由{{ }}包裹。\n# values.yaml\nreplicaCount: 1 image: repository: nginx tag: \u0026#34;latest\u0026#34; selectorLabels: \u0026#34;nginx\u0026#34; # templates/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: {{ .Release.Name }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Values.selectorLabels }} template: metadata: labels: app: {{ .Values.selectorLabels }} spec: containers: - image: {{ .Values.image.repository }}:{{ .Values.image.tag }} name: web Chart模板：内置对象 在上面示例中，模板文件中.Release、.Values是Helm内置对象，顶级开头写。\n**Release对象：**获取发布记录信息\n**Values对象：**为Chart模板提供值，这个对象的值有4个来源：\n• chart包中的values.yaml文件\n• helm install或者helm upgrade的-f或者\u0026ndash;values参数传入的自定义yaml文件\n• \u0026ndash;set参数传入值\n**Chart对象：**可以通过Chart对象访问Chart.yaml文件的内容，例如：{{ .Chart.AppVersion }}\nChart模板：调试 使用helm install提供了\u0026ndash;dry-run和\u0026ndash;debug调试参数，帮助你验证模板正确性，并把渲染后的模板打印出来，而\n不会真正的去部署。\n# helm install \u0026ndash;dry-run web mychart\n","permalink":"https://wandong1.github.io/post/helm%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","summary":"Helm Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前\n打包好的yaml文件部署到kubernetes上。\nHelm有3个重要概念：\n• **helm：**一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。\n• **Chart：**应用描述，一系列用于描述 k8s 资源相关文件的集合。\n• **Release：**基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在\nk8s中创建出真实运行的资源对象。\nHelm客户端 使用helm很简单，你只需要下载一个二进制客户端包即可，会通过kubeconfig配置（通常$HOME/.kube/config）来连接Kubernetes。\n项目地址：https://github.com/helm/helm\n下载Helm客户端：\nwget https://get.helm.sh/helm-v3.4.2-linux-amd64.tar.gz tar zxvf helm-v3.4.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ Helm常用命令 Helm管理应用生命周期： • helm create 创建Chart示例\n• helm install 部署\n• helm upgrade 更新\n• helm rollback 回滚\n• helm uninstall 卸载\nHelm基本使用：创建Chart示例 创建chart：\n# 默认示例中部署的是一个nginx服务 helm create mychart 打包chart：\nhelm package mychart • charts：目录里存放这个chart依赖的所有子chart。\n• Chart.yaml：用于描述这个 Chart的基本信息，包括名字、描述信息以及版本等。\n• values.yaml ：用于存储 templates 目录中模板文件中用到变量的值。","title":"helm的使用方法介绍"},{"content":"推荐博客 https://www.liwenzhou.com/posts/Go/golang-menu/\nLinux 安装go语言环境 # 下载地址 https://golang.google.cn/dl/ tar -xvzf go1.17.11.linux-386.tar.gz -C /usr/local yum install glibc.i686 -y mkdir -p /root/workspace cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPATH=\u0026#34;$HOME/workspace\u0026#34; EOF source /etc/profile #设置国内代理 go env -w GOPROXY=https://goproxy.cn,direct #查看go env go env Linux下创建一个go项目 #查看GOPATH go env | grep -i gopath #GOPATH=\u0026#34;/root/workspace\u0026#34; cd /root/workspace;mkdir {src,bin,pkg} #进入src目录创建项目 cd src \u0026amp;\u0026amp; mkdir GoRedis #随后编写main.go文件 构建多平台运行代码 go env -w GOOS=linux go env -w GOARCH=amd64 go build go env -w GOOS=windwos go env -w GOARCH=amd64 go build main.go文件内容 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) var ctx = context.Background() func main() { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;10.43.152.50:10001\u0026#34;, Password: \u0026#34;f1543f7c\u0026#34;, // 密码 DB: 1, // 数据库 PoolSize: 20, // 连接池大小 }) pong, err := rdb.Ping(ctx).Result() fmt.Println(pong, err) } # 执行命令初始化mod go mod init GoRedis # 安装三方依赖包 go get github.com/go-redis/redis/v8 # 整个目录编译，编译后会生成对应系统的执行文件 go build # 运行文件 ./GoRedis Golang指针 任何程序数据载入内存后，在内存都有他们的地址，这就是指针。而为了保存一个数据在内存中的地址，我们就需要指针变量。\n比如，“永远不要高估自己”这句话是我的座右铭，我想把它写入程序中，程序一启动这句话是要加载到内存（假设内存地址0x123456），我在程序中把这段话赋值给变量A，把内存地址赋值给变量B。这时候变量B就是一个指针变量。通过变量A和变量B都能找到我的座右铭。\nGo语言中的指针不能进行偏移和运算，因此Go语言中的指针操作非常简单，我们只需要记住两个符号：\u0026amp;（取地址）和*（根据地址取值）。\nx := 1 p := \u0026amp;x fmt.Println(p) // 变量的内存地址 指针 fmt.Println(*p) //输出 1 *p = 2 // 相当于 x = 2 fmt.Println(x) // 输出2 var x,y int fmt.Println(\u0026amp;x == \u0026amp;x,\u0026amp;x == \u0026amp;y, \u0026amp;x == nil) // 输出 true false false 总结： 取地址操作符\u0026amp;和取值操作符*是一对互补操作符，\u0026amp;取出地址，*根据地址取出地址指向的值。\n变量、指针地址、指针变量、取地址、取值的相互关系和特性如下：\n对变量进行取地址（\u0026amp;）操作，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值（*）操作，可以获得指针变量指向的原变量的值。 Golang并发编程 多线程介绍 A. 线程是由操作系统进行管理，也就是处于内核态。\nB. 线程之间进行切换，需要发生用户态到内核态的切换。\nC. 当系统中运行大量线程，系统会变的非常慢。\nD. 用户态的线程，支持大量线程创建。也叫协程或goroutine。\n1、串行、并发与并行 串行：我们都是先读小学，小学毕业后再读初中，读完初中再读高中。\n并发：同一时间段内执行多个任务（你在用微信和两个女朋友聊天）。\n并行：同一时刻执行多个任务（你和你朋友都在用微信和女朋友聊天）。\n2、进程、线程和协程 进程（process）：程序在操作系统中的一次执行过程，系统进行资源分配和调度的一个独立单位。\n线程（thread）：操作系统基于进程开启的轻量级进程，是操作系统调度执行的最小单位。\n协程（coroutine）：非操作系统提供而是由用户自行创建和控制的用户态‘线程’，比线程更轻量级。\n3、并发模型 业界将如何实现并发编程总结归纳为各式各样的并发模型，常见的并发模型有以下几种：\n线程\u0026amp;锁模型\nActor模型\nCSP模型\nFork\u0026amp;Join模型\nGo语言中的并发程序主要是通过基于CSP（communicating sequential processes）的goroutine和channel来实现，当然也支持使用传统的多线程共享内存的并发方式。\npackage main import ( \u0026#34;fmt\u0026#34; ) func hello() { fmt.Println(\u0026#34;hello\u0026#34;) } func main() { go hello() // 开启线程 fmt.Println(\u0026#34;你好\u0026#34;) } 多线程问题 执行以上代码，程序会退出，主线程执行完退出，主线程下开启的所有线程都会结束，hello()还没来得及执行就被打断了。\n解决办法：\n方法一： 在主函数中添加sleep，使用time.Sleep让 main goroutine 等待 hello goroutine执行结束是不优雅的，当然也是不准确的。\nfunc main() { go hello() fmt.Println(\u0026#34;你好\u0026#34;) time.Sleep(time.Second) } 方法二： Go 语言中通过sync包为我们提供了一些常用的并发原语，我们会在后面的小节单独介绍sync包中的内容。在这一小节，我们会先介绍一下 sync 包中的WaitGroup。当你并不关心并发操作的结果或者有其它方式收集并发操作的结果时，WaitGroup是实现等待一组并发操作完成的好方法。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) // 声明全局等待组变量 var wg sync.WaitGroup func hello() { fmt.Println(\u0026#34;hello\u0026#34;) wg.Done() // 告知当前goroutine完成 } func main() { wg.Add(1) // 登记1个goroutine。启动一个goroutine，就需登记一个 go hello() fmt.Println(\u0026#34;你好\u0026#34;) wg.Wait() // 阻塞等待登记的goroutine完成 } 登记多个goroutine\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) // 声明全局等待组变量 var wg sync.WaitGroup func hello(num int) { fmt.Println(\u0026#34;hello\u0026#34;, num) wg.Done() // 告知当前goroutine完成 } func main() { for i := 0; i \u0026lt; 10; i++ { wg.Add(1) // 登记1个goroutine go hello(i) } fmt.Println(\u0026#34;你好\u0026#34;) wg.Wait() // 阻塞等待登记的goroutine完成 } 方法三： 利用channel。。。\npackage main import \u0026#34;fmt\u0026#34; func hello(num int, ch chan bool) { ch \u0026lt;- true fmt.Println(\u0026#34;hellooooo\u0026#34;, num) } func main() { var num = 15 ch := make(chan bool, num) for i := 0; i \u0026lt; num; i++ { go hello(i, ch) } for j := 0; j \u0026lt; num; j++ { \u0026lt;-ch } fmt.Println(\u0026#34;All go routines finished executing\u0026#34;) } GOMAXPROCS Go运行时的调度器使用GOMAXPROCS参数来确定需要使用多少个 OS 线程来同时执行 Go 代码。默认值是机器上的 CPU 核心数。例如在一个 8 核心的机器上，GOMAXPROCS 默认为 8。Go语言中可以通过runtime.GOMAXPROCS函数设置当前程序并发时占用的 CPU逻辑核心数。（Go1.5版本之前，默认使用的是单核心执行。Go1.5 版本之后，默认使用全部的CPU 逻辑核心数。）\nchannel 单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。\n虽然可以使用共享内存进行数据交换，但是共享内存在不同的 goroutine 中容易发生竞态问题。为了保证数据交换的正确性，很多并发模型中必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。\nGo语言采用的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\n如果说 goroutine 是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个 goroutine 发送特定值到另一个 goroutine 的通信机制。\nGo 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。\nchannel类型 channel是 Go 语言中一种特有的类型。声明通道类型变量的格式如下：\nvar 变量名称 chan 元素类型 //元素类型：是指通道中传递元素的类型 //举例 var ch1 chan int // 声明一个传递整型的通道 var ch2 chan bool // 声明一个传递布尔型的通道 var ch3 chan []int // 声明一个传递int切片的通道 channel零值 未初始化的通道类型变量其默认零值是nil。\nvar ch chan int fmt.Println(ch) // \u0026lt;nil\u0026gt; 初始化channel 声明的通道类型变量需要使用内置的make函数初始化之后才能使用。具体格式如下：\nmake(chan 元素类型, [缓冲大小]) //channel的缓冲大小是可选的。 // 举个例子 ch4 := make(chan int) ch5 := make(chan bool, 1) // 声明一个缓冲区大小为1的通道 channel操作 通道共有发送（send）、接收(receive）和关闭（close）三种操作。而发送和接收操作都使用\u0026lt;-符号。\nch := make(chan int) //发送 ch \u0026lt;- 10 // 把10发送到ch中 //接收 x := \u0026lt;- ch // 从ch中接收值并赋值给变量x \u0026lt;-ch // 从ch中接收值，忽略结果 //关闭 close(ch) 关闭后的通道有以下特点：\n对一个关闭的通道再发送值就会导致 panic。 对一个关闭的通道进行接收会一直获取值直到通道为空。 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致 panic。 无缓冲的通道 无缓冲的通道又称为阻塞的通道。我们来看一下如下代码片段。\nfunc main() { ch := make(chan int) ch \u0026lt;- 10 fmt.Println(\u0026#34;发送成功\u0026#34;) } 有缓冲的通道 func main() { ch := make(chan int, 1) // 创建一个容量为1的有缓冲区通道 ch \u0026lt;- 10 fmt.Println(\u0026#34;发送成功\u0026#34;) } 有缓冲通道的死锁情况 package main import ( \u0026#34;fmt\u0026#34; ) func main() { ch := make(chan string, 2) \u0026lt;-ch //死锁 ch \u0026lt;- \u0026#34;hello\u0026#34; ch \u0026lt;- \u0026#34;world\u0026#34; ch \u0026lt;- \u0026#34;!\u0026#34; // 死锁 fmt.Println(\u0026lt;-ch) fmt.Println(\u0026lt;-ch) } 多返回值模式 value, ok := \u0026lt;- ch //value：从通道中取出的值，如果通道被关闭则返回对应类型的零值。 //ok：通道ch关闭时返回 false，否则返回 true。 for range接收值 通常我们会选择使用for range循环从通道中接收值，当通道被关闭后，会在通道内的所有值被接收完毕后会自动退出循环。上面那个示例我们使用for range改写后会很简洁。\nfunc f3(ch chan int) { for v := range ch { fmt.Println(v) } } select多路复用 package main import \u0026#34;fmt\u0026#34; func main() { ch := make(chan int, 1) // 打印基数 for i := 1; i \u0026lt;= 10; i++ { select { case x := \u0026lt;-ch: fmt.Println(x) case ch \u0026lt;- i: } } } channel练习 解法1： package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; \u0026#34;math/rand\u0026#34; ) type Result struct { TaskId int Res string } var resultChan chan *Result func wsum(num int) string { needSum := []int{} for { if num \u0026gt;= 10 { numOne := num % 10 needSum = append(needSum, numOne) num_buf := num / 10 num = int(math.Floor(float64(num_buf))) } else { needSum = append(needSum, num) break } } ret_sum := 0 for i := 0; i \u0026lt; len(needSum); i++ { ret_sum += needSum[i] } // for v := range needSum { // ret_sum += v // fmt.Println(ret_sum) // } // fmt.Printf(\u0026#34;%v sum is %d\\n\u0026#34;, needSum, ret_sum) return fmt.Sprintf(\u0026#34;%v sum is %d\\n\u0026#34;, needSum, ret_sum) } func worker(tId int) { new_num := rand.Int() resInfo := Result{ TaskId: tId, Res: fmt.Sprintf(\u0026#34;%v,%v\u0026#34;, new_num, wsum(new_num)), } // fmt.Println(resInfo) resultChan \u0026lt;- \u0026amp;resInfo } func main() { times := 100 resultChan = make(chan *Result, times) for i := 0; i \u0026lt; times; i++ { go worker(i) } // for v := range resultChan { // fmt.Println(v) // } for j := 0; j \u0026lt; times; j++ { fmt.Println(\u0026lt;-resultChan) } } 解法2 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; ) type res struct { Num int EachSum int } var jobChan chan int var resultChan chan *res func computeInt(num int) (sumNum int) { time.Sleep(time.Second) strNum := strconv.Itoa(num) fmt.Printf(\u0026#34;%v is %T\\n\u0026#34;, strNum, strNum) for i := 0; i \u0026lt; len(strNum); i++ { eachNum := string(strNum[i]) eachNumInt, _ := strconv.Atoi(eachNum) sumNum += eachNumInt } return } func computeInt2(num int) (sumNum int) { for num \u0026gt; 10 { sumNum += num % 10 num = num / 10 } time.Sleep(time.Second) return } func makeRandInt(jobChan chan int) { rand.Seed(time.Nanosecond.Nanoseconds()) for { num := rand.Int() jobChan \u0026lt;- num } } func computeEachNumSum(jobChan chan int, resultChan chan *res) { for { num := \u0026lt;-jobChan res := \u0026amp;res{Num: num, EachSum: computeInt2(num)} resultChan \u0026lt;- res } } func main() { jobChan = make(chan int) resultChan = make(chan *res) go makeRandInt(jobChan) for i := 0; i \u0026lt; 24; i++ { go computeEachNumSum(jobChan, resultChan) } for v := range resultChan { fmt.Printf(\u0026#34;%#v\\n\u0026#34;, *v) } } Golang Gin框架 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.io,direct # 安装gin框架 go get -u github.com/gin-gonic/gin # 解决飘红 go mod init gin go mod edit -require github.com/gin-gonic/gin@latest go mod vendor go env -w GO111MODULE=on\rgo env -w GOPROXY=https://goproxy.io,direct 创建项目后，执行初始命令\ngo mod init 项目名 第一个Gin程序 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func main() { r := gin.Default() // func 匿名函数 r.GET(\u0026#34;/ping\u0026#34;, func(c *gin.Context) { //输出json结果给调用方 c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 另外一种写法 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func testping(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;pong\u0026#34;, }) } func main() { r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, testping) r.Run() // listen and serve on 0.0.0.0:8080 } GET和POST方法 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func getPing(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;get pong\u0026#34;, }) } func postPing(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;post pong\u0026#34;, }) } func main() { r := gin.Default() r.GET(\u0026#34;/ping\u0026#34;, getPing) r.POST(\u0026#34;/ping\u0026#34;, postPing) r.Run() // listen and serve on 0.0.0.0:8080 } Gin框架路由分组 package main import \u0026#34;github.com/gin-gonic/gin\u0026#34; func login(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func submit(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func main() { //Default返回一个默认的路由引擎 router := gin.Default() // Simple group: v1 v1 := router.Group(\u0026#34;/api/v1\u0026#34;) { v1.POST(\u0026#34;/login\u0026#34;, login) v1.POST(\u0026#34;/submit\u0026#34;, submit) } // Simple group: v2 v2 := router.Group(\u0026#34;/api/v2\u0026#34;) { v2.POST(\u0026#34;/login\u0026#34;, login) v2.POST(\u0026#34;/submit\u0026#34;, submit) } _ = router.Run(\u0026#34;:8080\u0026#34;) } 参数绑定 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) // Binding from JSON type Login struct { User string `form:\u0026#34;user\u0026#34; json:\u0026#34;user\u0026#34; binding:\u0026#34;required\u0026#34;` UserId int `form:\u0026#34;user_id\u0026#34; json:\u0026#34;user_id\u0026#34;` Password string `form:\u0026#34;password\u0026#34; json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } func main() { router := gin.Default() // Example for binding JSON ({\u0026#34;user\u0026#34;: \u0026#34;manu\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;123\u0026#34;}) router.POST(\u0026#34;/loginJSON\u0026#34;, func(c *gin.Context) { var login Login if err := c.ShouldBindJSON(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;user_id\u0026#34;: login.UserId, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // Example for binding a HTML form (user=manu\u0026amp;password=123) router.POST(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login // This will infer what binder to use depending on the content-type header. if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // Example for binding a HTML querystring (user=manu\u0026amp;password=123) router.GET(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) _ = router.Run(\u0026#34;:8080\u0026#34;) } json返回数据渲染 package main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() // gin.H is a shortcut for map[string]interface{} r.GET(\u0026#34;/someJSON\u0026#34;, func(c *gin.Context) { //第一种方式,自己拼json c.JSON(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;hey\u0026#34;, \u0026#34;status\u0026#34;: http.StatusOK}) }) r.GET(\u0026#34;/moreJSON\u0026#34;, func(c *gin.Context) { // 第二种方式 You also can use a struct var msg struct { Name string `json:\u0026#34;user\u0026#34;` Message string Number int } msg.Name = \u0026#34;Lena\u0026#34; msg.Message = \u0026#34;hey\u0026#34; msg.Number = 123 // Note that msg.Name becomes \u0026#34;user\u0026#34; in the JSON c.JSON(http.StatusOK, msg) }) // Listen and serve on 0.0.0.0:8080 r.Run(\u0026#34;:8080\u0026#34;) } swagger 生成接口文档 go get -u github.com/swaggo/swag/cmd/swag swag init package main import ( _ \u0026#34;GinTest2/docs\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; ginSwagger \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) func login(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func submit(ctx *gin.Context) { ctx.JSON(200, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;success\u0026#34;, }) } func main() { //Default返回一个默认的路由引擎 router := gin.Default() // Simple group: v1 v1 := router.Group(\u0026#34;/api/v1\u0026#34;) { v1.POST(\u0026#34;/login\u0026#34;, login) v1.POST(\u0026#34;/submit\u0026#34;, submit) } // Simple group: v2 v2 := router.Group(\u0026#34;/api/v2\u0026#34;) { v2.POST(\u0026#34;/login\u0026#34;, login) v2.POST(\u0026#34;/submit\u0026#34;, submit) } router.GET(\u0026#34;/swagger/*any\u0026#34;, ginSwagger.WrapHandler(swaggerFiles.Handler)) _ = router.Run(\u0026#34;:8080\u0026#34;) } 访问接口文档 http://localhost:8080/swagger/index.html Golang 使用sqlx连接mysql数据库 安装三方包 go get github.com/go-sql-driver/mysql\rgo get github.com/jmoiron/sqlx main.go文件代码 package main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; ) var db *sqlx.DB type user struct { Id int64 UserName string Pwd string } func initDB() (err error) { dsn := \u0026#34;slbtraffic:1qaz#EDC@tcp(10.47.69.231:7306)/billing?charset=utf8mb4\u0026amp;parseTime=True\u0026#34; // 也可以使用MustConnect连接不成功就panic db, err = sqlx.Connect(\u0026#34;mysql\u0026#34;, dsn) if err != nil { fmt.Printf(\u0026#34;connect DB failed, err:%v\\n\u0026#34;, err) return } db.SetMaxOpenConns(20) db.SetMaxIdleConns(10) return } // 查询单条数据示例 func queryRowDemo() { sqlStr := \u0026#34;select id, username, pwd from users where id=?\u0026#34; var u user err := db.Get(\u0026amp;u, sqlStr, 1) if err != nil { fmt.Printf(\u0026#34;get failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.Id, u.UserName, u.Pwd) } // 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \u0026#34;select id, username, pwd from users\u0026#34; var u []user err := db.Select(\u0026amp;u, sqlStr) if err != nil { fmt.Printf(\u0026#34;get multi row failed, err:%v\\n\u0026#34;, err) return } for _, val := range u { fmt.Printf(\u0026#34;id:%d name:%s pwd:%s\\n\u0026#34;, val.Id, val.UserName, val.Pwd) } } // 插入数据 func insertRowDemo() { sqlStr := \u0026#34;insert into users(username, pwd) values (?,?)\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;沙河小王子\u0026#34;, \u0026#34;1235asd124142\u0026#34;) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\u0026#34;get lastinsert ID failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;insert success, the id is %d.\\n\u0026#34;, theID) } // 更新数据 func updateRowDemo() { sqlStr := \u0026#34;update users set username=? where username=? limit 1\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;沙河小王子sb\u0026#34;, \u0026#34;沙河小王子\u0026#34;) if err != nil { fmt.Printf(\u0026#34;upadte failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;update success, RowsAffected is %d.\\n\u0026#34;, n) } // 删除数据 func deleteRowDemo() { sqlStr := \u0026#34;delete from users where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\u0026#34;delete failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;delete success, affected rows:%d\\n\u0026#34;, n) } func main() { initDB() queryRowDemo() // insertRowDemo() updateRowDemo() deleteRowDemo() queryMultiRowDemo() } Golang GORM教程 安装和连接 官方文档 https://gorm.io/zh_CN/docs/index.html\n# 创建项目 go mod init 项目名称 go get -u gorm.io/gorm go get -u gorm.io/driver/mysql package main import ( \u0026#34;fmt\u0026#34; \u0026#34;gorm.io/driver/mysql\u0026#34; \u0026#34;gorm.io/gorm\u0026#34; ) type Product struct { gorm.Model Code string `gorm:\u0026#34;type:varchar(255)\u0026#34;` Price uint } func main() { dsn := \u0026#34;slbtraffic:1qaz#EDC@tcp(10.47.69.231:7306)/billing?charset=utf8mb4\u0026amp;parseTime=True\u0026amp;loc=Local\u0026#34; db, err := gorm.Open(mysql.Open(dsn), \u0026amp;gorm.Config{}) if err != nil { panic(\u0026#34;failed to connect database\u0026#34;) } // 迁移 schema // db.AutoMigrate(\u0026amp;Product{}) // // Create // db.Create(\u0026amp;Product{Code: \u0026#34;D43\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D44\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D45\u0026#34;, Price: 120}) // db.Create(\u0026amp;Product{Code: \u0026#34;D46\u0026#34;, Price: 120}) // 查询记录 var productData Product db.First(\u0026amp;productData, 1) fmt.Printf(\u0026#34;productData=%v\\n\u0026#34;, productData) // db.First(\u0026amp;productData, \u0026#34;code = ?\u0026#34;, \u0026#34;D46\u0026#34;) // fmt.Printf(\u0026#34;productData=%#v\\n\u0026#34;, productData) // 修改记录 将D46 修改为D666 // db.Model(\u0026amp;productData).Update(\u0026#34;code\u0026#34;, \u0026#34;D666\u0026#34;) // Update - 更新多个字段 // db.Model(\u0026amp;productData).Updates(Product{Price: 200, Code: \u0026#34;F42\u0026#34;}) // 仅更新非零值字段 // db.Model(\u0026amp;productData).Updates(map[string]interface{}{\u0026#34;Price\u0026#34;: 200, \u0026#34;Code\u0026#34;: \u0026#34;F42\u0026#34;}) // 删除 // db.Delete(\u0026amp;productData, 1) } Golang 操作redis 连接池 安装redis连接库 # 安装三方包 go get github.com/go-redis/redis/v8 # 插件官方 https://github.com/go-redis/redis # 官方文档 https://redis.uptrace.dev/guide/go-redis.html#installation package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; \u0026#34;time\u0026#34; ) var ctx = context.Background() func main() { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;10.43.152.50:10001\u0026#34;, Password: \u0026#34;f1543f7c\u0026#34;, // 密码 DB: 1, // 数据库 PoolSize: 20, // 连接池大小 }) pong, err := rdb.Ping(ctx).Result() fmt.Println(pong, err) // 直接执行命令获取错误 err = rdb.Set(ctx, \u0026#34;key\u0026#34;, 520, time.Hour).Err() // 直接执行命令获取值 value := rdb.Get(ctx, \u0026#34;key\u0026#34;).Val() fmt.Println(value) } Gin 框架 JWT的实现（重要） package main import ( \u0026#34;errors\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v4\u0026#34; ) // jwt 过期时间 const TokenExpireDuration = time.Hour * 2 // CustomSecret 用于加盐的字符串 var CustomSecret = []byte(\u0026#34;夏天夏天悄悄过去\u0026#34;) type UserInfo struct { Username string Password string } type CustomClaims struct { // 可根据需要自行添加字段 Username string `json:\u0026#34;username\u0026#34;` jwt.RegisteredClaims // 内嵌标准的声明 } // GenToken 生成JWT func GenToken(username string) (string, error) { // 创建一个我们自己的声明 claims := CustomClaims{ username, // 自定义字段 jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(TokenExpireDuration)), Issuer: \u0026#34;my-project\u0026#34;, // 签发人 }, } // 使用指定的签名方法创建签名对象 token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) // 使用指定的secret签名并获得完整的编码后的字符串token return token.SignedString(CustomSecret) } func authHandler(c *gin.Context) { // 用户发送用户名和密码过来 var user UserInfo err := c.ShouldBind(\u0026amp;user) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2001, \u0026#34;msg\u0026#34;: \u0026#34;无效的参数\u0026#34;, }) return } // 校验用户名和密码是否正确 if user.Username == \u0026#34;q1mi\u0026#34; \u0026amp;\u0026amp; user.Password == \u0026#34;q1mi123\u0026#34; { // 生成Token tokenString, _ := GenToken(user.Username) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;token\u0026#34;: tokenString}, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2002, \u0026#34;msg\u0026#34;: \u0026#34;鉴权失败\u0026#34;, }) return } // ParseToken 解析JWT func ParseToken(tokenString string) (*CustomClaims, error) { // 解析token // 如果是自定义Claim结构体则需要使用 ParseWithClaims 方法 token, err := jwt.ParseWithClaims(tokenString, \u0026amp;CustomClaims{}, func(token *jwt.Token) (i interface{}, err error) { // 直接使用标准的Claim则可以直接使用Parse方法 //token, err := jwt.Parse(tokenString, func(token *jwt.Token) (i interface{}, err error) { return CustomSecret, nil }) if err != nil { return nil, err } // 对token对象中的Claim进行类型断言 if claims, ok := token.Claims.(*CustomClaims); ok \u0026amp;\u0026amp; token.Valid { // 校验token return claims, nil } return nil, errors.New(\u0026#34;invalid token\u0026#34;) } // JWTAuthMiddleware 基于JWT的认证中间件 func JWTAuthMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带Token有三种方式 1.放在请求头 2.放在请求体 3.放在URI // 这里假设Token放在Header的Authorization中，并使用Bearer开头 // 这里的具体实现方式要依据你的实际业务情况决定 authHeader := c.Request.Header.Get(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2003, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth为空\u0026#34;, }) c.Abort() return } // 按空格分割 parts := strings.SplitN(authHeader, \u0026#34; \u0026#34;, 2) if !(len(parts) == 2 \u0026amp;\u0026amp; parts[0] == \u0026#34;Bearer\u0026#34;) { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2004, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth格式有误\u0026#34;, }) c.Abort() return } // parts[1]是获取到的tokenString，我们使用之前定义好的解析JWT的函数来解析它 mc, err := ParseToken(parts[1]) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2005, \u0026#34;msg\u0026#34;: \u0026#34;无效的Token\u0026#34;, }) c.Abort() return } // 将当前请求的username信息保存到请求的上下文c上 c.Set(\u0026#34;username\u0026#34;, mc.Username) c.Next() // 后续的处理函数可以用过c.Get(\u0026#34;username\u0026#34;)来获取当前请求的用户信息 } } func homeHandler(c *gin.Context) { username := c.MustGet(\u0026#34;username\u0026#34;).(string) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;username\u0026#34;: username}, }) } func main() { r := gin.Default() r.POST(\u0026#34;/auth\u0026#34;, authHandler) r.GET(\u0026#34;/home\u0026#34;, JWTAuthMiddleware(), homeHandler) r.Run() } swagger接口文档 用法步骤 按照swagger要求给接口代码添加声明式注释，具体参照声明式注释格式。 使用swag工具扫描代码自动生成API接口文档数据 使用gin-swagger渲染在线接口文档页面 安装swag工具，在注释编写完成后使用 go get -u github.com/swaggo/swag/cmd/swag swag init # 会生成docs目录 #./docs #├── docs.go #├── swagger.json #└── swagger.yaml 引入gin-swagger import ( _ \u0026#34;GinJwt/docs\u0026#34; // 千万不要忘了导入把你上一步生成的docs \u0026#34;github.com/gin-gonic/gin\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; gs \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) 下载三方包 go get github.com/swaggo/gin-swagger go get github.com/swaggo/files package main import ( \u0026#34;errors\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;GinJwt/docs\u0026#34; // 千万不要忘了导入把你上一步生成的docs \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v4\u0026#34; swaggerFiles \u0026#34;github.com/swaggo/files\u0026#34; gs \u0026#34;github.com/swaggo/gin-swagger\u0026#34; ) // jwt 过期时间 const TokenExpireDuration = time.Hour * 2 // CustomSecret 用于加盐的字符串 var CustomSecret = []byte(\u0026#34;夏天夏天悄悄过去\u0026#34;) type UserInfo struct { Username string Password string } type CustomClaims struct { // 可根据需要自行添加字段 Username string `json:\u0026#34;username\u0026#34;` jwt.RegisteredClaims // 内嵌标准的声明 } // GenToken 生成JWT func GenToken(username string) (string, error) { // 创建一个我们自己的声明 claims := CustomClaims{ username, // 自定义字段 jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(TokenExpireDuration)), Issuer: \u0026#34;my-project\u0026#34;, // 签发人 }, } // 使用指定的签名方法创建签名对象 token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) // 使用指定的secret签名并获得完整的编码后的字符串token return token.SignedString(CustomSecret) } func authHandler(c *gin.Context) { // 用户发送用户名和密码过来 var user UserInfo err := c.ShouldBind(\u0026amp;user) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2001, \u0026#34;msg\u0026#34;: \u0026#34;无效的参数\u0026#34;, }) return } // 校验用户名和密码是否正确 if user.Username == \u0026#34;q1mi\u0026#34; \u0026amp;\u0026amp; user.Password == \u0026#34;q1mi123\u0026#34; { // 生成Token tokenString, _ := GenToken(user.Username) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;token\u0026#34;: tokenString}, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2002, \u0026#34;msg\u0026#34;: \u0026#34;鉴权失败\u0026#34;, }) return } // ParseToken 解析JWT func ParseToken(tokenString string) (*CustomClaims, error) { // 解析token // 如果是自定义Claim结构体则需要使用 ParseWithClaims 方法 token, err := jwt.ParseWithClaims(tokenString, \u0026amp;CustomClaims{}, func(token *jwt.Token) (i interface{}, err error) { // 直接使用标准的Claim则可以直接使用Parse方法 //token, err := jwt.Parse(tokenString, func(token *jwt.Token) (i interface{}, err error) { return CustomSecret, nil }) if err != nil { return nil, err } // 对token对象中的Claim进行类型断言 if claims, ok := token.Claims.(*CustomClaims); ok \u0026amp;\u0026amp; token.Valid { // 校验token return claims, nil } return nil, errors.New(\u0026#34;invalid token\u0026#34;) } // JWTAuthMiddleware 基于JWT的认证中间件 func JWTAuthMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带Token有三种方式 1.放在请求头 2.放在请求体 3.放在URI // 这里假设Token放在Header的Authorization中，并使用Bearer开头 // 这里的具体实现方式要依据你的实际业务情况决定 authHeader := c.Request.Header.Get(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2003, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth为空\u0026#34;, }) c.Abort() return } // 按空格分割 parts := strings.SplitN(authHeader, \u0026#34; \u0026#34;, 2) if !(len(parts) == 2 \u0026amp;\u0026amp; parts[0] == \u0026#34;Bearer\u0026#34;) { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2004, \u0026#34;msg\u0026#34;: \u0026#34;请求头中auth格式有误\u0026#34;, }) c.Abort() return } // parts[1]是获取到的tokenString，我们使用之前定义好的解析JWT的函数来解析它 mc, err := ParseToken(parts[1]) if err != nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2005, \u0026#34;msg\u0026#34;: \u0026#34;无效的Token\u0026#34;, }) c.Abort() return } // 将当前请求的username信息保存到请求的上下文c上 c.Set(\u0026#34;username\u0026#34;, mc.Username) c.Next() // 后续的处理函数可以用过c.Get(\u0026#34;username\u0026#34;)来获取当前请求的用户信息 } } // homeHandler 测试jwt功能接口 // @Summary 测试jwt功能接口 // @Description 测试jwt功能接口 // @Tags 测试jwt功能接口 // @Accept application/json // @Produce application/json // @Param Authorization header string false \u0026#34;Bearer 用户令牌\u0026#34; // @Security ApiKeyAuth // @Success 200 // @Router /home [get] {\u0026#34;username\u0026#34;: username} func homeHandler(c *gin.Context) { username := c.MustGet(\u0026#34;username\u0026#34;).(string) c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 2000, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: gin.H{\u0026#34;username\u0026#34;: username}, }) } func main() { r := gin.Default() r.POST(\u0026#34;/auth\u0026#34;, authHandler) r.GET(\u0026#34;/home\u0026#34;, JWTAuthMiddleware(), homeHandler) r.GET(\u0026#34;/swagger/*any\u0026#34;, gs.WrapHandler(swaggerFiles.Handler)) r.Run() } 访问地址 http://localhost:8080/swagger/index.html\nGolang三方库 gopsutil psutil是一个跨平台进程和系统监控的Python库，而gopsutil是其Go语言版本的实现。本文介绍了它的基本使用。\nGo语言部署简单、性能好的特点非常适合做一些诸如采集系统信息和监控的服务，本文介绍的gopsutil库是知名Python库：psutil的一个Go语言版本的实现。\n安装 go get github.com/shirou/gopsutil windows额外安装\ngo get github.com/yusufpapurcu/wmi go get golang.org/x/sys/windows 使用 采集CPU相关信息 import \u0026#34;github.com/shirou/gopsutil/cpu\u0026#34; import \u0026#34;github.com/shirou/gopsutil/load\u0026#34; import \u0026#34;github.com/shirou/gopsutil/mem\u0026#34; import \u0026#34;github.com/shirou/gopsutil/host\u0026#34; import \u0026#34;github.com/shirou/gopsutil/disk\u0026#34; import \u0026#34;github.com/shirou/gopsutil/net\u0026#34; // cpu info func getCpuInfo() { cpuInfos, err := cpu.Info() if err != nil { fmt.Printf(\u0026#34;get cpu info failed, err:%v\u0026#34;, err) } for _, ci := range cpuInfos { fmt.Println(ci) } // CPU使用率 for { percent, _ := cpu.Percent(time.Second, false) fmt.Printf(\u0026#34;cpu percent:%v\\n\u0026#34;, percent) } } func getCpuLoad() { info, _ := load.Avg() fmt.Printf(\u0026#34;%v\\n\u0026#34;, info) } // mem info func getMemInfo() { memInfo, _ := mem.VirtualMemory() fmt.Printf(\u0026#34;mem info:%v\\n\u0026#34;, memInfo) } // host info func getHostInfo() { hInfo, _ := host.Info() fmt.Printf(\u0026#34;host info:%v uptime:%v boottime:%v\\n\u0026#34;, hInfo, hInfo.Uptime, hInfo.BootTime) } // disk info func getDiskInfo() { parts, err := disk.Partitions(true) if err != nil { fmt.Printf(\u0026#34;get Partitions failed, err:%v\\n\u0026#34;, err) return } for _, part := range parts { fmt.Printf(\u0026#34;part:%v\\n\u0026#34;, part.String()) diskInfo, _ := disk.Usage(part.Mountpoint) fmt.Printf(\u0026#34;disk info:used:%v free:%v\\n\u0026#34;, diskInfo.UsedPercent, diskInfo.Free) } ioStat, _ := disk.IOCounters() for k, v := range ioStat { fmt.Printf(\u0026#34;%v:%v\\n\u0026#34;, k, v) } } // net IO func getNetInfo() { info, _ := net.IOCounters(true) for index, v := range info { fmt.Printf(\u0026#34;%v:%v send:%v recv:%v\\n\u0026#34;, index, v, v.BytesSent, v.BytesRecv) } } // get ip func GetLocalIP() (ip string, err error) { addrs, err := net.InterfaceAddrs() if err != nil { return } for _, addr := range addrs { ipAddr, ok := addr.(*net.IPNet) if !ok { continue } if ipAddr.IP.IsLoopback() { continue } if !ipAddr.IP.IsGlobalUnicast() { continue } return ipAddr.IP.String(), nil } return } 适用工具，上传下载文件 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) var ( FileDir = flag.String(\u0026#34;d\u0026#34;, \u0026#34;D:\\\\golang_project\\\\src\\\\uploadDownload\\\\file_dir\u0026#34;, `-d 绝对路径 指定上传和下载的目录`) Port = flag.Int(\u0026#34;p\u0026#34;, 8080,`-p 监听端口号 指定程序运行端口号`) ) func main() { flag.Parse() router := gin.Default() // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026#34;/upload\u0026#34;, func(c *gin.Context) { // Multipart form form, _ := c.MultipartForm() files := form.File[\u0026#34;file\u0026#34;] for _, file := range files { log.Println(file.Filename) dst := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, *FileDir, file.Filename) fmt.Println(dst) // 上传文件到指定的目录 c.SaveUploadedFile(file, dst) } c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: fmt.Sprintf(\u0026#34;%d files uploaded!\u0026#34;, len(files)), }) }) router.GET(\u0026#34;/listfile\u0026#34;, func(c *gin.Context) { fileInfoList, err := ioutil.ReadDir(*FileDir) var FileList []string for i := range fileInfoList { FileList = append(FileList,fileInfoList[i].Name()) } if err != nil { c.JSON(http.StatusBadRequest, gin.H{ \u0026#34;code\u0026#34;: 400, \u0026#34;msg\u0026#34;: \u0026#34;Error\u0026#34;, }) }else { c.JSON(http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: 200, \u0026#34;file_list\u0026#34;: FileList, }) } }) router.GET(\u0026#34;/download/:file\u0026#34;, func(c *gin.Context) { filename := c.Param(\u0026#34;file\u0026#34;) filepath := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, *FileDir, filename) c.File(filepath) }) port := fmt.Sprintf(\u0026#34;:%d\u0026#34;,*Port) router.Run(port) } 常用函数 时间和日期相关函数 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { // 日期和时间相关的函数 now := time.Now() fmt.Printf(\u0026#34;%v \\n\u0026#34;, now) //2022-08-16 17:08:36.0952186 +0800 CST m=+0.007682101 // 获取时间其他相关信息 fmt.Printf(\u0026#34;%v-%v-%v %v:%v:%v\\n\u0026#34;, now.Year(), int(now.Month()), now.Day(), now.Hour(), now.Minute(), now.Second()) // 格式化日期和时间 // 1、printf fmtTime := fmt.Sprintf(\u0026#34;%v-%v-%v %v:%v:%v\u0026#34;, now.Year(), int(now.Month()), now.Day(), now.Hour(), now.Minute(), now.Second()) fmt.Println(fmtTime) // 2、使用官方的format函数 (推荐) fmt.Println(time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;)) fmt.Println(time.Now().Format(\u0026#34;01-02\u0026#34;)) //时间 to 时间戳 loc, _ := time.LoadLocation(\u0026#34;Asia/Shanghai\u0026#34;) //设置时区 tt, _ := time.ParseInLocation(\u0026#34;2006-01-02 15:04:05\u0026#34;, \u0026#34;2018-07-11 15:07:51\u0026#34;, loc) //2006-01-02 15:04:05是转换的格式如php的\u0026#34;Y-m-d H:i:s\u0026#34; fmt.Println(tt.Unix()) // 时间常量 const ( ns = time.Nanosecond us = time.Microsecond ms = time.Millisecond ss = time.Second ) // 时间戳 Unix UnixNano 纳秒 获取随机数字 fmt.Printf(\u0026#34;unix时间戳=%v unixnano时间戳=%v\\n\u0026#34;, now.Unix(), now.UnixNano()) // 时间戳转日期 needConvertUnix := now.Unix() needConvertUnixTime := time.Unix(needConvertUnix, 0).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;时间戳:%v is %v\\n\u0026#34;, needConvertUnix, needConvertUnixTime) // 时间加减 // 一天前的时间 oneDayBefore, _ := time.ParseDuration(\u0026#34;-24h\u0026#34;) d1 := now.Add(oneDayBefore).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;昨天的日期：%v \\n\u0026#34;, d1) d2 := now.AddDate(0, -1, 0).Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) fmt.Printf(\u0026#34;上月的日期：%v\u0026#34;, d2) } os标准库 // 遍历所有环境遍历 envs := os.Environ() for _, value := range envs { cache := strings.Split(value, \u0026#34;=\u0026#34;) fmt.Println(cache) } // 获取指定名称的环境变量 golandEnv := os.Getenv(\u0026#34;GoLand\u0026#34;) fmt.Printf(\u0026#34;golandEnv is %s\\n\u0026#34;, golandEnv) // 打印主机名 fmt.Println(os.Hostname()) // 获取当前路径 filePath, _ := os.Getwd() fmt.Println(filePath) bytes标准库 func TestConvertAb(t *testing.T) { var b = []byte(\u0026#34;abcaBSADASD12-2131\u0026#34;) // 转大写 upper := bytes.ToUpper(b) // 转小写 lower := bytes.ToLower(b) fmt.Println(string(b), string(upper), string(lower)) var c = []byte(\u0026#34;ABC\u0026#34;) var d = []byte(\u0026#34;ABc\u0026#34;) // 忽略大小写比较 if bytes.EqualFold(c, d) { fmt.Printf(\u0026#34;c:%v,d:%v c=d成立\\n\u0026#34;, c, d) } } ","permalink":"https://wandong1.github.io/post/golang/","summary":"推荐博客 https://www.liwenzhou.com/posts/Go/golang-menu/\nLinux 安装go语言环境 # 下载地址 https://golang.google.cn/dl/ tar -xvzf go1.17.11.linux-386.tar.gz -C /usr/local yum install glibc.i686 -y mkdir -p /root/workspace cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile export PATH=$PATH:/usr/local/go/bin export GOPATH=\u0026#34;$HOME/workspace\u0026#34; EOF source /etc/profile #设置国内代理 go env -w GOPROXY=https://goproxy.cn,direct #查看go env go env Linux下创建一个go项目 #查看GOPATH go env | grep -i gopath #GOPATH=\u0026#34;/root/workspace\u0026#34; cd /root/workspace;mkdir {src,bin,pkg} #进入src目录创建项目 cd src \u0026amp;\u0026amp; mkdir GoRedis #随后编写main.go文件 构建多平台运行代码 go env -w GOOS=linux go env -w GOARCH=amd64 go build go env -w GOOS=windwos go env -w GOARCH=amd64 go build main.","title":"Golang学习笔记"},{"content":"golang定时任务系统 项目地址： https://github.com/ouqiang/gocron/releases 部署方法 https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-node-v1.5.3-linux-amd64.tar.gz\nhttps://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-v1.5.3-linux-amd64.tar.gz\n项目分为两个包 gocron-node和gocron，gocron-node为任务节点，实际执行任务，gocron为web端\n创建gocron用户 useradd gocron 解压软件包并运行 tar -xvzf gocron-node-v1.5.3-linux-amd64.tar.gz -C /home/gocron tar -xvzf gocron-v1.5.3-linux-amd64.tar.gz -C /home/gocron # 修改权限 chown gocron:gocron -R /home/gocron/ # 运行gocron web端 前台运行，监听5920端口 cd /home/gocron/gocron-linux-amd64 \u0026amp;\u0026amp; su gocron \u0026amp;\u0026amp; ./gocron web # 新起窗口运行gocron node任务节点 前台运行 cd /home/gocron/gocron-node-linux-amd64 su gocron \u0026amp;\u0026amp; ./gocron-node 1、登录web页面，访问 http://localhost:5920\n2、初始化数据库，并创建登录用户，注意数据库要单独使用新库，不能有其他表\n3、然后登录，添加任务节点，添加完成后测试连接\n![image-20220721155135950](D:\\typora Note\\assets\\image-20220721155135950.png)\n4、进入系统管理配置通知配置\n使用钉钉webhook进行通知\n![image-20220721155256643](D:\\typora Note\\assets\\image-20220721155256643.png)\n模板文件写法：\n{ \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;任务ID: {{.TaskId}} 任务名称: {{.TaskName}} 状态: {{.Status}} 执行结果: \\n{{.Result}}\u0026#34; }, \u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34; } 5、新增定时任务\n![image-20220721155354483](D:\\typora Note\\assets\\image-20220721155354483.png)\nCrontab时间表达式 Linux-crontab时间表达式语法, 支持秒级任务定义 格式: 秒 分 时 天 月 周 示例： 1 * * * * * 每分钟第一秒运行 */20 * * * * * 每隔20秒运行一次 0 30 21 * * * 每天晚上21:30:00运行一次 0 0 23 * * 6 每周六晚上23:00:00 运行一次 快捷语法: @yearly 每年运行一次 @monthly 每月运行一次 @weekly 每周运行一次 @daily 每天运行一次 @midnight 每天午夜运行一次 @hourly 每小时运行一次 @every 30s 每隔30秒运行一次 @every 1m20s 每隔1分钟20秒运行一次 @every 3h5m10s 每隔3小时5分钟10秒运行一次 执行方式 shell: 在远程主机上执行shell命令 HTTP: 执行HTTP-GET请求 任务超时时间 任务执行超时，强制结束, 默认0，不限制 shell任务执行时间不能超过86400秒 HTTP任务执行时间不能超过300秒.\n任务执行失败重试次数 无法连接远程主机，shell返回值非0, HTTP响应码非200等异常返回, 可再次执行任务, 每次重试间隔时间 = 重试次数 * 1分钟 按1分钟、2分钟、3分钟\u0026hellip;..的间隔进行重试 取值范围1-10 例: 重试次数为2 任务执行失败, 休眠1分钟, 再次执行任务 再次执行失败, 休眠2分钟, 再次执行任务 默认0，不重试.\n开启安全 TLS双向认证（重要） 为了确保数据传输及任务节点gocron-node安全, 强烈建议开启\n下载证书制作工具 wget https://github.com/square/certstrap/releases/download/v1.1.1/certstrap-v1.1.1-linux-amd64 \u0026amp;\u0026amp; mv certstrap-v1.1.1-linux-amd64 /usr/bin/certstrap \u0026amp;\u0026amp; chmod +x /usr/bin/certstrap 生成证书 # 生成CA证书 ./certstrap init --common-name \u0026#34;Root CA\u0026#34; # 生成服务端(gocron-node)证书和私钥 ./certstrap request-cert --ip 10.43.152.50 ./certstrap sign --CA \u0026#34;Root CA\u0026#34; --years 20 10.43.152.50 # 生成客户端(gocron)证书和私钥 ./certstrap request-cert --ip 127.0.0.1 ./certstrap sign --CA \u0026#34;Root CA\u0026#34; --years 20 127.0.0.1 # 生成证书会到当前目录out文件下 # 修改私钥*.key权限,只能被运行gocron-node的用户读取 cd out/ chmod 600 10.43.152.50.key chmod 600 127.0.0.1.key # 拷贝密钥到对应文件夹 mkdir /home/gocron/gocron-linux-amd64/cert/ cp Root_CA.crt 10.43.152.50.crt 10.43.152.50.key /home/gocron/gocron-node-linux-amd64/ cp Root_CA.crt 127.0.0.1.crt 127.0.0.1.key /home/gocron/gocron-linux-amd64/cert/ 启动服务 # 启动node节点（使用非root用户） cd /home/gocron/gocron-node-linux-amd64/ ./gocron-node -enable-tls -ca-file Root_CA.crt -cert-file 10.43.152.50.crt -key-file 10.43.152.50.key # 先修改gocron web端 conf/app.ini配置文件 enable_tls = true ca_file = /home/gocron/gocron-linux-amd64/cert/Root_CA.crt cert_file = /home/gocron/gocron-linux-amd64/cert/127.0.0.1.crt key_file = /home/gocron/gocron-linux-amd64/cert/127.0.0.1.key # 启动gocron web端（使用非root用户） cd /home/gocron/gocron-linux-amd64/ \u0026amp;\u0026amp; ./gocron web 常见报错 docker容器内部发送消息失败\n![image-20220721234104982](D:\\typora Note\\assets\\image-20220721234104982.png)\nx509: certificate signed by unknown authority\n我们在构建 docker 镜像时一般使用的是 linux(centos或者ubuntu等待) 系统，默认是不带 ca-certificates 根证书的，导致无法识别外部 https 携带的数字证书。那么，在访问的时候就会抛出 x509: certificate signed by unknown authority 的错误，导致 docker 容器的接口服务返回 500。\nUbuntu构建镜像时安装ca-certificates包\napt-get -qq install -y --no-install-recommends ca-certificates curl ","permalink":"https://wandong1.github.io/post/golang%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%B3%BB%E7%BB%9F/","summary":"golang定时任务系统 项目地址： https://github.com/ouqiang/gocron/releases 部署方法 https://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-node-v1.5.3-linux-amd64.tar.gz\nhttps://github.com/ouqiang/gocron/releases/download/v1.5.3/gocron-v1.5.3-linux-amd64.tar.gz\n项目分为两个包 gocron-node和gocron，gocron-node为任务节点，实际执行任务，gocron为web端\n创建gocron用户 useradd gocron 解压软件包并运行 tar -xvzf gocron-node-v1.5.3-linux-amd64.tar.gz -C /home/gocron tar -xvzf gocron-v1.5.3-linux-amd64.tar.gz -C /home/gocron # 修改权限 chown gocron:gocron -R /home/gocron/ # 运行gocron web端 前台运行，监听5920端口 cd /home/gocron/gocron-linux-amd64 \u0026amp;\u0026amp; su gocron \u0026amp;\u0026amp; ./gocron web # 新起窗口运行gocron node任务节点 前台运行 cd /home/gocron/gocron-node-linux-amd64 su gocron \u0026amp;\u0026amp; ./gocron-node 1、登录web页面，访问 http://localhost:5920\n2、初始化数据库，并创建登录用户，注意数据库要单独使用新库，不能有其他表\n3、然后登录，添加任务节点，添加完成后测试连接\n![image-20220721155135950](D:\\typora Note\\assets\\image-20220721155135950.png)\n4、进入系统管理配置通知配置\n使用钉钉webhook进行通知\n![image-20220721155256643](D:\\typora Note\\assets\\image-20220721155256643.png)\n模板文件写法：\n{ \u0026#34;at\u0026#34;: { \u0026#34;atMobiles\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;atUserIds\u0026#34;: [ \u0026#34;user123\u0026#34; ], \u0026#34;isAtAll\u0026#34;: \u0026#34;false\u0026#34; }, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;任务ID: {{.","title":"golang定时任务系统"},{"content":"golang微服务 1、RPC 简介 ⚫ 远程过程调用（Remote Procedure Call，RPC）是一个计算机通信协议\n⚫ 该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额 外地为这个交互作用编程\n⚫ 如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方 法调用\n2、golang中如何实现RPC ⚫ golang 中实现 RPC 非常简单，官方提供了封装好的库，还有一些第三方的库\n⚫ golang 官方的 net/rpc 库使用 encoding/gob 进行编解码，支持 tcp 和 http 数据传输方 式，由于其他语言不支持 gob 编解码方式，所以 golang 的 RPC 只支持 golang 开发 的服务器与客户端之间的交互\n⚫ 官方还提供了 net/rpc/jsonrpc 库实现 RPC 方法，jsonrpc 采用 JSON 进行数据编解码， 因而支持跨语言调用，目前 jsonrpc 库是基于 tcp 协议实现的，暂不支持 http 传输 方式\n⚫ golang 的 RPC 必须符合 4 个条件才可以\n​\t◼ 结构体字段首字母要大写，要跨域访问，所以大写\n​\t◼ 函数名必须首字母大写（可以序列号导出的）\n​\t◼ 函数第一个参数是接收参数，第二个参数是返回给客户端参数，必须是指针类 型\n​\t◼ 函数必须有一个返回值 error\n⚫ 另外，net/rpc/jsonrpc 库通过 json 格式编解码，支持跨语言调用\n服务端代码： package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/rpc\u0026#34; ) // 服务端，求矩形面积和周长 // Rect 声明矩形对象 type Rect struct { } // Params 生命参数结构体，字段首字母大写 type Params struct { Width, Height int } // Area 求矩形面积的方法 func (r *Rect) Area(p Params, ret *int) error { *ret = p.Width * p.Height return nil } // Perimeter 求矩形面积的方法 func (r *Rect) Perimeter(p Params, ret *int) error { *ret = (p.Width + p.Height) * 2 return nil } func main() { // 1、注册服务 rect := new(Rect) rpc.Register(rect) // 2、把服务处理绑定到http协议上 rpc.HandleHTTP() // 3、监听服务，等待客户端调用 err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) if err != nil { log.Fatal(err) } } 服务端代码（jsonrpc调用）： func main() { // 1、注册服务 rpc.Register(new(Rect)) // 2、把服务处理绑定到http协议上 lis,err := net.Listen(\u0026#34;tcp\u0026#34;,\u0026#34;127.0.0.1:8081\u0026#34;) if err != nil { log.Fatal(err) } //循环监听服务 for { conn, err := lis.Accept() if err != nil { continue } // 起协程 go func(conn net.Conn) { fmt.Println(\u0026#34;new a client\u0026#34;) jsonrpc.ServeConn(conn) }(conn) } } 客户端代码： package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/rpc\u0026#34; ) // Params 定义参数结构体 type Params struct { Width, Height int } // 调用服务 func main() { // 1、连接远程rpc服务 //http调用 rp, err := rpc.DialHTTP(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8080\u0026#34;) // json rpc 调用 // rp, err := jsonrpc.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8081\u0026#34;) if err != nil { log.Fatal(err) } // 2、调用远程方法 // 定义接收服务端传回来的计算结果的变量 var ret int // ret := 0 pArgs := Params{Width: 100, Height: 50} // 求面积 err2 := rp.Call(\u0026#34;Rect.Area\u0026#34;, pArgs, \u0026amp;ret) if err2 != nil { log.Fatal(err2) } fmt.Printf(\u0026#34;width: %d,height: %d \u0026#39;s area is %d\\n\u0026#34;, pArgs.Width, pArgs.Height, ret) // 求周长 err3 := rp.Call(\u0026#34;Rect.Perimeter\u0026#34;, pArgs, \u0026amp;ret) if err3 != nil { log.Fatal(err3) } fmt.Printf(\u0026#34;width: %d,height: %d \u0026#39;s perimeter is %d\\n\u0026#34;, pArgs.Width, pArgs.Height, ret) } ","permalink":"https://wandong1.github.io/post/golang%E5%BE%AE%E6%9C%8D%E5%8A%A1/","summary":"golang微服务 1、RPC 简介 ⚫ 远程过程调用（Remote Procedure Call，RPC）是一个计算机通信协议\n⚫ 该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额 外地为这个交互作用编程\n⚫ 如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方 法调用\n2、golang中如何实现RPC ⚫ golang 中实现 RPC 非常简单，官方提供了封装好的库，还有一些第三方的库\n⚫ golang 官方的 net/rpc 库使用 encoding/gob 进行编解码，支持 tcp 和 http 数据传输方 式，由于其他语言不支持 gob 编解码方式，所以 golang 的 RPC 只支持 golang 开发 的服务器与客户端之间的交互\n⚫ 官方还提供了 net/rpc/jsonrpc 库实现 RPC 方法，jsonrpc 采用 JSON 进行数据编解码， 因而支持跨语言调用，目前 jsonrpc 库是基于 tcp 协议实现的，暂不支持 http 传输 方式\n⚫ golang 的 RPC 必须符合 4 个条件才可以\n​\t◼ 结构体字段首字母要大写，要跨域访问，所以大写\n​\t◼ 函数名必须首字母大写（可以序列号导出的）","title":"golang微服务"},{"content":"golang日志框架zap package main import ( \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; \u0026#34;os\u0026#34; ) var logger *zap.Logger var sugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger = zap.New(core) sugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { file, _ := os.Create(\u0026#34;./test.log\u0026#34;) return zapcore.AddSync(file) } func main() { InitLogger() defer logger.Sync() defer sugarLogger.Sync() logger.Info(\u0026#34;日志记录成功\u0026#34;, zap.String(\u0026#34;service\u0026#34;, \u0026#34;logger service\u0026#34;)) logger.Error(\u0026#34;日志记录失败\u0026#34;, zap.String(\u0026#34;service\u0026#34;, \u0026#34;logger service\u0026#34;)) sugarLogger.Infof(\u0026#34;日志记录成功 服务：%s\u0026#34;, \u0026#34;logger service\u0026#34;) sugarLogger.Error(\u0026#34;日志记录失败\u0026#34;, \u0026#34;logger service\u0026#34;) } ","permalink":"https://wandong1.github.io/post/golang%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6zap/","summary":"golang日志框架zap package main import ( \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;go.uber.org/zap/zapcore\u0026#34; \u0026#34;os\u0026#34; ) var logger *zap.Logger var sugarLogger *zap.SugaredLogger func InitLogger() { writeSyncer := getLogWriter() encoder := getEncoder() core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger = zap.New(core) sugarLogger = logger.Sugar() } func getEncoder() zapcore.Encoder { encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder return zapcore.NewJSONEncoder(encoderConfig) } func getLogWriter() zapcore.WriteSyncer { file, _ := os.Create(\u0026#34;./test.log\u0026#34;) return zapcore.AddSync(file) } func main() { InitLogger() defer logger.Sync() defer sugarLogger.","title":"golang日志框架zap"},{"content":"jdk安装 dockerfile文件\n下载地址：https://www.oracle.com/java/technologies/downloads/#java8 jdk-8u341-linux-x64.tar.gz文件\nFROM ubuntu:latest ADD jdk-8u341-linux-x64.tar.gz /usr/local/ RUN mv /usr/local/jdk1.8.0_341 /usr/local/jdk1.8 ENV JAVA_HOME=/usr/local/jdk1.8 ENV JRE_HOME=${JAVA_HOME}/jre ENV CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH ENV JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin ENV PATH=$PATH:${JAVA_PATH} RUN apt-get update \u0026amp;\u0026amp; apt-get install curl tree iputils-ping net-tools iproute2 vim -y CMD [\u0026#34;java\u0026#34;,\u0026#34;-version\u0026#34;] ","permalink":"https://wandong1.github.io/post/jdk%E5%AE%89%E8%A3%85/","summary":"jdk安装 dockerfile文件\n下载地址：https://www.oracle.com/java/technologies/downloads/#java8 jdk-8u341-linux-x64.tar.gz文件\nFROM ubuntu:latest ADD jdk-8u341-linux-x64.tar.gz /usr/local/ RUN mv /usr/local/jdk1.8.0_341 /usr/local/jdk1.8 ENV JAVA_HOME=/usr/local/jdk1.8 ENV JRE_HOME=${JAVA_HOME}/jre ENV CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:$CLASSPATH ENV JAVA_PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin ENV PATH=$PATH:${JAVA_PATH} RUN apt-get update \u0026amp;\u0026amp; apt-get install curl tree iputils-ping net-tools iproute2 vim -y CMD [\u0026#34;java\u0026#34;,\u0026#34;-version\u0026#34;] ","title":"jdk安装教程（容器）"},{"content":"jenkins构建go项目 一、配置jenkins 1、全局工具配置 将go安装包解压后，拷贝至以上的安装目录\n自由风格构建，选go的构建环境，然后就可以在shell中执行go命令了\n","permalink":"https://wandong1.github.io/post/jenkins%E6%9E%84%E5%BB%BAgo%E9%A1%B9%E7%9B%AE/","summary":"jenkins构建go项目 一、配置jenkins 1、全局工具配置 将go安装包解压后，拷贝至以上的安装目录\n自由风格构建，选go的构建环境，然后就可以在shell中执行go命令了","title":"jenkins构建go项目"},{"content":"项目源码介绍 使用nfs-subdir-external-provisioner github地址：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 详细介绍：https://artifacthub.io/packages/helm/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner 镜像地址：https://hub.docker.com/r/eipwork/nfs-subdir-external-provisioner/tags\nNFS服务端安装 # 安装nfs服务端 yum install nfs-utils -y vim /etc/exports /opt/nfsdata 192.168.0.0/24(rw,no_root_squash,no_all_squash,sync) # 刷新并验证 exportfs -rv # 启动nfs服务，共两个服务 systemctl enable rpcbind --now systemctl enable nfs --now 所有客户端也需要安装nfs-utils，安装完成即可，无需启动服务\nyum install nfs-utils -y storageClass插件安装 # 添加helm仓库地址 helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 安装第一个 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata \\ --set image.repository=eipwork/nfs-subdir-external-provisioner # 安装第二个(可选) helm install second-nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata2 \\ --set image.repository=eipwork/nfs-subdir-external-provisioner \\ --set storageClass.name=nfs-client-2 \\ --set storageClass.provisionerName=k8s-sigs.io/second-nfs-subdir-external-provisioner ","permalink":"https://wandong1.github.io/post/%E4%B8%BAk8s%E9%9B%86%E7%BE%A4%E6%B7%BB%E5%8A%A0nfs%E7%B1%BB%E5%9E%8B%E7%9A%84sotrageclass/","summary":"项目源码介绍 使用nfs-subdir-external-provisioner github地址：https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner 详细介绍：https://artifacthub.io/packages/helm/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner 镜像地址：https://hub.docker.com/r/eipwork/nfs-subdir-external-provisioner/tags\nNFS服务端安装 # 安装nfs服务端 yum install nfs-utils -y vim /etc/exports /opt/nfsdata 192.168.0.0/24(rw,no_root_squash,no_all_squash,sync) # 刷新并验证 exportfs -rv # 启动nfs服务，共两个服务 systemctl enable rpcbind --now systemctl enable nfs --now 所有客户端也需要安装nfs-utils，安装完成即可，无需启动服务\nyum install nfs-utils -y storageClass插件安装 # 添加helm仓库地址 helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ # 安装第一个 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata \\ --set image.repository=eipwork/nfs-subdir-external-provisioner # 安装第二个(可选) helm install second-nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=192.168.0.13 \\ --set nfs.path=/opt/nfsdata2 \\ --set image.","title":"为K8S集群添加nfs类型的sotrageClass"},{"content":"Centos7\nwget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo wget -O /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo ","permalink":"https://wandong1.github.io/post/centos%E6%9B%B4%E6%96%B0%E5%9B%BD%E5%86%85yum%E6%BA%90/","summary":"Centos7\nwget -O /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo wget -O /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/CentOS7-Base-163.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo ","title":"Centos更新国内yum源"},{"content":"ElasticSearch快速入门实战 主讲老师：Fox\nES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\n​ 文档：1.ElasticSearch快速入门实战.note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引） 文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）\n假如，我们有一个站内搜索的功能，通过某个关键词来搜索相关的文章，那么这个关键词可能出现在标题中，也可能出现在文章内容中，那我们将会在创建或修改文章的时候，建立一个关键词与文章的对应关系表，这种，我们可以称之为倒排索引。\nlike %java设计模式% java 设计模式\n关键词 文章ID JAVA 1,2 设计模式 1,2 多线程 2 简单理解，正向索引是通过key找value，反向索引则是通过value找key。ES底层在检索时底层使用的就是倒排索引。\nElasticSearch简介 ElasticSearch是什么 ElasticSearch（简称ES）是一个分布式、RESTful 风格的搜索和数据分析引擎，是用Java开发并且是当前最流行的开源的企业级搜索引擎，能够达到近实时搜索，稳定，可靠，快速，安装使用方便。\n客户端支持Java、.NET（C#）、PHP、Python、Ruby等多种语言。\n官方网站: https://www.elastic.co/\n**下载地址：**https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n搜索引擎排名：\n​ 参考网站：https://db-engines.com/en/ranking/search+engine\n起源——Lucene\n基于Java语言开发的搜索引擎库类\n创建于1999年，2005年成为Apache 顶级开源项目\nLucene具有高性能、易扩展的优点\nLucene的局限性︰\n只能基于Java语言开发 类库的接口学习曲线陡峭 原生并不支持水平扩展 Elasticsearch的诞生\nElasticsearch是构建在Apache Lucene之上的开源分布式搜索引擎。\n2004年 Shay Banon 基于Lucene开发了Compass\n2010年 Shay Banon重写了Compass，取名Elasticsearch\n支持分布式，可水平扩展 降低全文检索的学习曲线，可以被任何编程语言调用 ​ Elasticsearch 与 Lucene 核心库竞争的优势在于：\n完美封装了 Lucene 核心库，设计了友好的 Restful-API，开发者无需过多关注底层机制，直接开箱即用。 分片与副本机制，直接解决了集群下性能与高可用问题。 ES Server进程 3节点 raft (奇数节点)\n数据分片 -》lucene实例 分片和副本数 1个ES节点可以有多个lucene实例。也可以指定一个索引的多个分片\n​ ElasticSearch版本特性 5.x新特性\nLucene 6.x， 性能提升，默认打分机制从TF-IDF改为BM 25\n支持Ingest节点/ Painless Scripting / Completion suggested支持/原生的Java REST客户端\nType标记成deprecated， 支持了Keyword的类型\n性能优化\n内部引擎移除了避免同一文档并发更新的竞争锁，带来15% - 20%的性能提升 Instant aggregation,支持分片，上聚合的缓存 新增了Profile API 6.x新特性\nLucene 7.x\n新功能\n跨集群复制(CCR) 索引生命周期管理 SQL的支持 更友好的的升级及数据迁移\n在主要版本之间的迁移更为简化，体验升级 全新的基于操作的数据复制框架，可加快恢复数据 性能优化\n有效存储稀疏字段的新方法，降低了存储成本 在索引时进行排序，可加快排序的查询性能 7.x新特性\nLucene 8.0\n重大改进-正式废除单个索引下多Type的支持\n7.1开始，Security 功能免费使用\nECK - Elasticseach Operator on Kubernetes\n新功能\nNew Cluster coordination Feature——Complete High Level REST Client Script Score Query 性能优化\n默认的Primary Shard数从5改为1,避免Over Sharding 性能优化， 更快的Top K 8.x新特性\nRest API相比较7.x而言做了比较大的改动（比如彻底删除_type） 默认开启安全配置 存储空间优化：对倒排文件使用新的编码集，对于keyword、match_only_text、text类型字段有效，有3.5%的空间优化提升，对于新建索引和segment自动生效。 优化geo_point，geo_shape类型的索引（写入）效率：15%的提升。 技术预览版KNN API发布，（K邻近算法），跟推荐系统、自然语言排名相关。 https://www.elastic.co/guide/en/elastic-stack/current/elasticsearch-breaking-changes.html ElasticSearch vs Solr\nSolr 是第一个基于 Lucene 核心库功能完备的搜索引擎产品，诞生远早于 Elasticsearch。\n当单纯的对已有数据进行搜索时，Solr更快。当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。\n​ ​ 大型互联网公司，实际生产环境测试，将搜索引擎从Solr转到 Elasticsearch以后的平均查询速度有了50倍的提升。\n​ 总结：\nSolr 利用 Zookeeper 进行分布式管理，而Elasticsearch 自身带有分布式协调管理功能。 Solr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。 Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。 Solr 是传统搜索应用的有力解决方案，但 Elasticsearch更适用于新兴的实时搜索应用。 Elastic Stack介绍 在Elastic Stack之前我们听说过ELK，ELK分别是Elasticsearch，Logstash，Kibana这三款软件在一起的简称，在发展的过程中又有新的成员Beats的加入，就形成了Elastic Stack。\n​ Elastic Stack生态圈\n在Elastic Stack生态圈中Elasticsearch作为数据存储和搜索，是生态圈的基石，Kibana在上层提供用户一个可视化及操作的界面，Logstash和Beat可以对数据进行收集。在上图的右侧X-Pack部分则是Elastic公司提供的商业项目。\n指标分析/日志分析：\n​ ElasticSearch应用场景 站内搜索 日志管理与分析 大数据分析 应用性能监控 机器学习 国内现在有大量的公司都在使用 Elasticsearch，包括携程、滴滴、今日头条、饿了么、360安全、小米、vivo等诸多知名公司。除了搜索之外，结合Kibana、Logstash、Beats，Elastic Stack还被广泛运用在大数据近实时分析领域，包括日志分析、指标监控、信息安全等多个领域。它可以帮助你探索海量结构化、非结构化数据，按需创建可视化报表，对监控数据设置报警阈值，甚至通过使用机器学习技术，自动识别异常状况。\n通用数据处理流程：\n​ ElasticSearch快速开始 ElasticSearch安装运行 环境准备\n运行Elasticsearch，需安装并配置JDK\n设置$JAVA_HOME 各个版本对Java的依赖 https://www.elastic.co/support/matrix#matrix_jvm\nElasticsearch 5需要Java 8以上的版本 Elasticsearch 从6.5开始支持Java 11 7.0开始，内置了Java环境 ES比较耗内存，建议虚拟机4G或以上内存，jvm1g以上的内存分配\n可以参考es的环境文件elasticsearch-env.bat\n​ ES的jdk环境生效的优先级配置ES_JAVA_HOME\u0026gt;JAVA_HOME\u0026gt;ES_HOME\n下载并解压ElasticSearch\n下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n选择版本：7.17.3\n​ ElasticSearch文件目录结构\n目录 描述 bin 脚本文件，包括启动elasticsearch，安装插件，运行统计数据等 config 配置文件目录，如elasticsearch配置、角色配置、jvm配置等。 jdk java运行环境 data 默认的数据存放目录，包含节点、分片、索引、文档的所有数据，生产环境需要修改。 lib elasticsearch依赖的Java类库 logs 默认的日志文件存储路径，生产环境需要修改。 modules 包含所有的Elasticsearch模块，如Cluster、Discovery、Indices等。 plugins 已安装插件目录 主配置文件elasticsearch.yml\ncluster.name 当前节点所属集群名称，多个节点如果要组成同一个集群，那么集群名称一定要配置成相同。默认值elasticsearch，生产环境建议根据ES集群的使用目的修改成合适的名字。\nnode.name 当前节点名称，默认值当前节点部署所在机器的主机名，所以如果一台机器上要起多个ES节点的话，需要通过配置该属性明确指定不同的节点名称。\npath.data 配置数据存储目录，比如索引数据等，默认值 $ES_HOME/data，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\npath.logs 配置日志存储目录，比如运行日志和集群健康信息等，默认值 $ES_HOME/logs，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\nbootstrap.memory_lock 配置ES启动时是否进行内存锁定检查，默认值true。\nES对于内存的需求比较大，一般生产环境建议配置大内存，如果内存不足，容易导致内存交换到磁盘，严重影响ES的性能。所以默认启动时进行相应大小内存的锁定，如果无法锁定则会启动失败。\n非生产环境可能机器内存本身就很小，能够供给ES使用的就更小，如果该参数配置为true的话很可能导致无法锁定内存以致ES无法成功启动，此时可以修改为false。\nnetwork.host 配置能够访问当前节点的主机，默认值为当前节点所在机器的本机回环地址127.0.0.1 和[::1]，这就导致默认情况下只能通过当前节点所在主机访问当前节点。可以配置为 0.0.0.0 ，表示所有主机均可访问。\nhttp.port 配置当前ES节点对外提供服务的http端口，默认值 9200\ndiscovery.seed_hosts 配置参与集群节点发现过程的主机列表，说白一点就是集群中所有节点所在的主机列表，可以是具体的IP地址，也可以是可解析的域名。\ncluster.initial_master_nodes 配置ES集群初始化时参与master选举的节点名称列表，必须与node.name配置的一致。ES集群首次构建完成后，应该将集群中所有节点的配置文件中的cluster.initial_master_nodes配置项移除，重启集群或者将新节点加入某个已存在的集群时切记不要设置该配置项。\n​ #ES开启远程访问 network.host: 0.0.0.0\n修改JVM配置\n修改config/jvm.options配置文件，调整jvm堆内存大小\n​ vim jvm.options -Xms4g -Xmx4g\n配置的建议\nXms和Xms设置成—样 Xmx不要超过机器内存的50% 不要超过30GB - https://www.elastic.co/cn/blog/a-heap-of-trouble 启动ElasticSearch服务 Windows\n直接运行elasticsearch.bat\nLinux（centos7）\nES不允许使用root账号启动服务，如果你当前账号是root，则需要创建一个专有账户\n​ #非root用户 bin/elasticsearch # -d 后台启动 bin/elasticsearch -d\n​ 注意：es默认不能用root用户启动，生产环境建议为elasticsearch创建用户。\n​ #为elaticsearch创建用户并赋予相应权限 adduser es passwd es chown -R es:es elasticsearch-17.3\n运行http://localhost:9200/\n​ 如果ES服务启动异常，会有提示：\n​ 启动ES服务常见错误解决方案 [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\nES因为需要大量的创建索引文件，需要大量的打开系统的文件，所以我们需要解除linux系统当中打开文件最大数目的限制，不然ES启动就会抛错\n​ #切换到root用户 vim /etc/security/limits.conf 末尾添加如下配置： *\tsoft nofile 65536 * hard nofile 65536 * soft nproc 4096 *\thard nproc 4096\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n无法创建本地线程问题,用户最大可创建线程数太小\n​ vim /etc/security/limits.d/20-nproc.conf 改为如下配置： * soft nproc 4096\n[3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n最大虚拟内存太小,调大系统的虚拟内存\n​ vim /etc/sysctl.conf 追加以下内容： vm.max_map_count=262144 保存退出之后执行如下命令： sysctl -p\n[4]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n缺少默认配置，至少需要配置discovery.seed_hosts/discovery.seed_providers/cluster.initial_master_nodes中的一个参数.\ndiscovery.seed_hosts: 集群主机列表 discovery.seed_providers: 基于配置文件配置集群主机列表 cluster.initial_master_nodes: 启动时初始化的参与选主的node，生产环境必填 ​ vim config/elasticsearch.yml #添加配置 discovery.seed_hosts: [\u0026ldquo;127.0.0.1\u0026rdquo;] cluster.initial_master_nodes: [\u0026ldquo;node-1\u0026rdquo;] #或者 单节点（集群单节点） discovery.type: single-node\n客户端Kibana安装 Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。\n1）下载并解压缩Kibana\n下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana\n选择版本：7.17.3\n​ 2）修改Kibana.yml\n​ vim config/kibana.yml server.port: 5601 server.host: \u0026ldquo;localhost\u0026rdquo; #服务器ip elasticsearch.hosts: [\u0026ldquo;http://localhost:9200\u0026rdquo;] #elasticsearch的访问地址 i18n.locale: \u0026ldquo;zh-CN\u0026rdquo; #Kibana汉化\n3）运行Kibana\n注意：kibana也需要非root用户启动\n​ bin/kibana #后台启动 nohup bin/kibana \u0026amp;\n访问Kibana: http://localhost:5601/\n​ cat API\n​ /_cat/allocation #查看单节点的shard分配整体情况 /_cat/shards #查看各shard的详细情况 /_cat/shards/{index} #查看指定分片的详细情况 /_cat/master #查看master节点信息 /_cat/nodes #查看所有节点信息 /_cat/indices #查看集群中所有index的详细信息 /_cat/indices/{index} #查看集群中指定index的详细信息 /_cat/segments #查看各index的segment详细信息,包括segment名, 所属shard, 内存(磁盘)占用大小, 是否刷盘 /_cat/segments/{index}#查看指定index的segment详细信息 /_cat/count #查看当前集群的doc数量 /_cat/count/{index} #查看指定索引的doc数量 /_cat/recovery #查看集群内每个shard的recovery过程.调整replica。 /_cat/recovery/{index}#查看指定索引shard的recovery过程 /_cat/health #查看集群当前状态：红、黄、绿 /_cat/pending_tasks #查看当前集群的pending task /_cat/aliases #查看集群中所有alias信息,路由配置等 /_cat/aliases/{alias} #查看指定索引的alias信息 /_cat/thread_pool #查看集群各节点内部不同类型的threadpool的统计信息, /_cat/plugins #查看集群各个节点上的plugin信息 /_cat/fielddata #查看当前集群各个节点的fielddata内存使用情况 /_cat/fielddata/{fields} #查看指定field的内存使用情况,里面传field属性对应的值 /_cat/nodeattrs #查看单节点的自定义属性 /_cat/repositories #输出集群中注册快照存储库 /_cat/templates #输出当前正在存在的模板信息\nElasticsearch安装分词插件 Elasticsearch提供插件机制对系统进行扩展\n以安装analysis-icu这个分词插件为例\n在线安装\n​ #查看已安装插件 bin/elasticsearch-plugin list #安装插件 bin/elasticsearch-plugin install analysis-icu #删除插件 bin/elasticsearch-plugin remove analysis-icu\n注意：安装和删除完插件后，需要重启ES服务才能生效。\n测试分词效果\n​ POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;icu_analyzer\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n​ 离线安装\n本地下载相应的插件，解压，然后手动上传到elasticsearch的plugins目录，然后重启ES实例就可以了。\n比如ik中文分词插件：https://github.com/medcl/elasticsearch-analysis-ik\n测试分词效果\n​ #ES的默认分词设置是standard，会单字拆分 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;standard\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; } #ik_smart:会做最粗粒度的拆 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;: \u0026ldquo;ik_smart\u0026rdquo;, \u0026ldquo;text\u0026rdquo;: \u0026ldquo;中华人民共和国\u0026rdquo; } #ik_max_word:会将文本做最细粒度的拆分 POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;ik_max_word\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n创建索引时可以指定IK分词器作为默认分词器\n​ PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;analysis.analyzer.default.type\u0026rdquo;: \u0026ldquo;ik_max_word\u0026rdquo; } } }\n​ ElasticSearch基本概念 关系型数据库 VS ElasticSearch 在7.0之前，一个 Index可以设置多个Types\n目前Type已经被Deprecated，7.0开始，一个索引只能创建一个Type - “_doc”\n传统关系型数据库和Elasticsearch的区别:\nElasticsearch- Schemaless /相关性/高性能全文检索 RDMS —事务性/ Join ​ 索引（Index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。\n一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。\n​ 文档（Document） Elasticsearch是面向文档的，文档是所有可搜索数据的最小单位。\n日志文件中的日志项 一本电影的具体信息/一张唱片的详细信息 MP3播放器里的一首歌/一篇PDF文档中的具体内容 文档会被序列化成JSON格式，保存在Elasticsearch中\nJSON对象由字段组成 每个字段都有对应的字段类型(字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID\n可以自己指定ID或者通过Elasticsearch自动生成 一篇文档包含了一系列字段，类似数据库表中的一条记录\nJSON文档，格式灵活，不需要预先定义格式\n字段的类型可以指定或者通过Elasticsearch自动推算 支持数组/支持嵌套 文档元数据\n​ 元数据，用于标注文档的相关信息：\n_index：文档所属的索引名 _type：文档所属的类型名 _id：文档唯—ld _source: 文档的原始Json数据 _version: 文档的版本号，修改删除操作_version都会自增1 _seq_no: 和_version一样，一旦数据发生更改，数据也一直是累计的。Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。 _primary_term: _primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。 ElasticSearch索引操作 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index.html\n创建索引\n索引命名必须小写，不能以下划线开头\n格式: PUT /索引名称\n​ #创建索引 PUT /es_db #创建索引时可以设置分片数和副本数 PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;number_of_shards\u0026rdquo; : 3, \u0026ldquo;number_of_replicas\u0026rdquo; : 2 } } #修改索引配置 PUT /es_db/_settings { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;number_of_replicas\u0026rdquo; : 1 } }\n​ 查询索引\n格式: GET /索引名称\n​ #查询索引 GET /es_db #es_db是否存在 HEAD /es_db\n​ ​\n删除索引\n格式: DELETE /索引名称\n​ DELETE /es_db\nElasticSearch文档操作 示例数据\n​ PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;analysis.analyzer.default.type\u0026rdquo;: \u0026ldquo;ik_max_word\u0026rdquo; } } } PUT /es_db/_doc/1 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; } PUT /es_db/_doc/2 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;李四\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 28, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州荔湾大厦\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java assistant\u0026rdquo; } PUT /es_db/_doc/3 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;王五\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 26, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州白云山公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;php developer\u0026rdquo; } PUT /es_db/_doc/4 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;赵六\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 22, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙橘子洲\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;python assistant\u0026rdquo; } PUT /es_db/_doc/5 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张龙\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 0, \u0026ldquo;age\u0026rdquo;: 19, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙麓谷企业广场\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java architect assistant\u0026rdquo; }\tPUT /es_db/_doc/6 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;赵虎\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 32, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;长沙麓谷兴工国际产业园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java architect\u0026rdquo; }\n添加（索引）文档\n格式: [PUT | POST] /索引名称/[_doc | _create ]/id ​ # 创建文档,指定id # 如果id不存在，创建新的文档，否则先删除现有文档，再创建新的文档，版本会增加 PUT /es_db/_doc/1 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; }\t#创建文档，ES生成id POST /es_db/_doc { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25, \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州天河公园\u0026rdquo;, \u0026ldquo;remark\u0026rdquo;: \u0026ldquo;java developer\u0026rdquo; }\n​ 注意:POST和PUT都能起到创建/更新的作用，PUT需要对一个具体的资源进行操作也就是要确定id才能进行更新/创建，而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建/更新\n​ Create -如果ID已经存在，会失败\n​ 修改文档\n全量更新，整个json都会替换，格式: [PUT | POST] /索引名称/_doc/id 如果文档存在，现有文档会被删除，新的文档会被索引\n​ # 全量更新，替换整个json PUT /es_db/_doc/1/ { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;张三\u0026rdquo;, \u0026ldquo;sex\u0026rdquo;: 1, \u0026ldquo;age\u0026rdquo;: 25 } #查询文档 GET /es_db/_doc/1\n​ 使用_update部分更新，格式: POST /索引名称/_update/id update不会删除原来的文档，而是实现真正的数据更新\n​ # 部分更新：在原有文档上更新 # Update -文档必须已经存在，更新只会对相应字段做增量修改 POST /es_db/_update/1 { \u0026ldquo;doc\u0026rdquo;: { \u0026ldquo;age\u0026rdquo;: 28 } } #查询文档 GET /es_db/_doc/1\n​ 使用 _update_by_query 更新文档 ​ POST /es_db/_update_by_query { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;match\u0026rdquo;: { \u0026ldquo;_id\u0026rdquo;: 1 } }, \u0026ldquo;script\u0026rdquo;: { \u0026ldquo;source\u0026rdquo;: \u0026ldquo;ctx._source.age = 30\u0026rdquo; } }\n​ 并发场景下修改文档\n_seq_no和_primary_term是对_version的优化，7.X版本的ES默认使用这种方式控制版本，所以当在高并发环境下使用乐观锁机制修改文档时，要带上当前文档的_seq_no和_primary_term进行更新：\n​ POST /es_db/_doc/2?if_seq_no=21\u0026amp;if_primary_term=6 { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;李四xxx\u0026rdquo; }\n如果版本号不对，会抛出版本冲突异常，如下图：\n​ 查询文档\n根据id查询文档，格式: GET /索引名称/_doc/id ​ GET /es_db/_doc/1\n条件查询 _search，格式： /索引名称/_doc/_search ​ # 查询前10条文档 GET /es_db/_doc/_search\nES Search API提供了两种条件查询搜索方式：\nREST风格的请求URI，直接将参数带过去 封装到request body中，这种方式可以定义更加易读的JSON格式 ​ #通过URI搜索，使用“q”指定查询字符串，“query string syntax” KV键值对 #条件查询, 如要查询age等于28岁的 _search?q=:** GET /es_db/_doc/_search?q=age:28 #范围查询, 如要查询age在25至26岁之间的 _search?q=[ TO **] 注意: TO 必须为大写 GET /es_db/_doc/_search?q=age[25 TO 26] #查询年龄小于等于28岁的 :\u0026lt;= GET /es_db/_doc/_search?q=age:\u0026lt;=28 #查询年龄大于28前的 :\u0026gt; GET /es_db/_doc/_search?q=age:\u0026gt;28 #分页查询 from=\u0026amp;size=* GET /es_db/_doc/_search?q=age[25 TO 26]\u0026amp;from=0\u0026amp;size=1 #对查询结果只输出某些字段 _source=字段,字段 GET /es_db/_doc/_search?_source=name,age #对查询结果排序 sort=字段:desc/asc GET /es_db/_doc/_search?sort=age:desc\n通过请求体的搜索方式会在后面课程详细讲解（DSL）\n​ GET /es_db/_search { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;match\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;广州白云\u0026rdquo; } } }\n删除文档\n格式: DELETE /索引名称/_doc/id\n​ DELETE /es_db/_doc/1\nElasticSearch文档批量操作\n批量操作可以减少网络连接所产生的开销，提升性能\n支持在一次API调用中，对不同的索引进行操作 可以在URI中指定Index，也可以在请求的Payload中进行 操作中单条操作失败，并不会影响其他操作 返回结果包括了每一条操作执行的结果 批量写入\n批量对文档进行写操作是通过_bulk的API来实现的\n请求方式：POST\n请求地址：_bulk\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n第一行参数为指定操作的类型及操作的对象(index,type和id) 第二行参数才是操作的数据 参数类似于：\n​ {\u0026ldquo;actionName\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;indexName\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026ldquo;typeName\u0026rdquo;,\u0026quot;_id\u0026quot;:\u0026ldquo;id\u0026rdquo;}} {\u0026ldquo;field1\u0026rdquo;:\u0026ldquo;value1\u0026rdquo;, \u0026ldquo;field2\u0026rdquo;:\u0026ldquo;value2\u0026rdquo;}\nactionName：表示操作类型，主要有create,index,delete和update 批量创建文档create\n​ POST _bulk {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;fox老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;fox老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;id\u0026rdquo;:4,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;mark老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;mark老师NB\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530}\n普通创建或全量替换index\n​ POST _bulk {\u0026ldquo;index\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;图灵徐庶老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;图灵学院徐庶老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;index\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;id\u0026rdquo;:4,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;图灵诸葛老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;图灵学院诸葛老师NB\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530}\n如果原文档不存在，则是创建 如果原文档存在，则是替换(全量修改原文档) 批量删除delete\n​ POST _bulk {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}}\n批量修改update\n​ POST _bulk {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;title\u0026rdquo;:\u0026ldquo;ES大法必修内功\u0026rdquo;}} {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;create_time\u0026rdquo;:1554018421008}}\n组合应用\n​ POST _bulk {\u0026ldquo;create\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;id\u0026rdquo;:3,\u0026ldquo;title\u0026rdquo;:\u0026ldquo;fox老师\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;fox老师666\u0026rdquo;,\u0026ldquo;tags\u0026rdquo;:[\u0026ldquo;java\u0026rdquo;, \u0026ldquo;面向对象\u0026rdquo;],\u0026ldquo;create_time\u0026rdquo;:1554015482530} {\u0026ldquo;delete\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:3}} {\u0026ldquo;update\u0026rdquo;:{\u0026quot;_index\u0026quot;:\u0026ldquo;article\u0026rdquo;, \u0026ldquo;_type\u0026rdquo;:\u0026quot;_doc\u0026quot;, \u0026ldquo;_id\u0026rdquo;:4}} {\u0026ldquo;doc\u0026rdquo;:{\u0026ldquo;create_time\u0026rdquo;:1554018421008}}\n批量读取\nes的批量查询可以使用mget和msearch两种。其中mget是需要我们知道它的id，可以指定不同的index，也可以指定返回值source。msearch可以通过字段查询来进行一个批量的查找。\n_mget\n​ #可以通过ID批量获取不同index和type的数据 GET _mget { \u0026ldquo;docs\u0026rdquo;: [ { \u0026ldquo;_index\u0026rdquo;: \u0026ldquo;es_db\u0026rdquo;, \u0026ldquo;_id\u0026rdquo;: 1 }, { \u0026ldquo;_index\u0026rdquo;: \u0026ldquo;article\u0026rdquo;, \u0026ldquo;_id\u0026rdquo;: 4 } ] } #可以通过ID批量获取es_db的数据 GET /es_db/_mget { \u0026ldquo;docs\u0026rdquo;: [ { \u0026ldquo;_id\u0026rdquo;: 1 }, { \u0026ldquo;_id\u0026rdquo;: 4 } ] } #简化后 GET /es_db/_mget { \u0026ldquo;ids\u0026rdquo;:[\u0026ldquo;1\u0026rdquo;,\u0026ldquo;2\u0026rdquo;] }\n​ _msearch\n在_msearch中，请求格式和bulk类似。查询一条数据需要两个对象，第一个设置index和type，第二个设置查询语句。查询语句和search相同。如果只是查询一个index，我们可以在url中带上index，这样，如果查该index可以直接用空对象表示。\n​ GET /es_db/_msearch {} {\u0026ldquo;query\u0026rdquo; : {\u0026ldquo;match_all\u0026rdquo; : {}}, \u0026ldquo;from\u0026rdquo; : 0, \u0026ldquo;size\u0026rdquo; : 2} {\u0026ldquo;index\u0026rdquo; : \u0026ldquo;article\u0026rdquo;} {\u0026ldquo;query\u0026rdquo; : {\u0026ldquo;match_all\u0026rdquo; : {}}}\n​ Logstash与FileBeat详解以及ELK整合 ​ 文档：6. Logstash与FileBeat详解以及ELK整合\u0026hellip; 链接：http://note.youdao.com/noteshare?id=cd88d72a1c76d18efcf7fe767e8c2d20\u0026amp;sub=D7819084A43243FFA52E8A8741795414\n注意：本节课的命令和配置文件不要再pdf文件中复制，为存在格式问题，保存到有道云笔记后再操作\n​ 背景\n​ ELK架构\n​ 经典的ELK\n​ 整合消息队列+Nginx架构\n​ 什么是Logstash\n​ Logstash核心概念\n​ Logstash数据传输原理\n​ Logstash配置文件结构\n​ Logstash Queue\n​ Logstash导入数据到ES\n​ 同步数据库数据到Elasticsearch\n​ 什么是Beats\n​ FileBeat简介\n​ FileBeat的工作原理\n​ logstash vs FileBeat\n​ Filebeat安装\n​ ELK整合实战\n​ 案例：采集tomcat服务器日志\n​ 使用FileBeats将日志发送到Logstash\n​ 配置Logstash接收FileBeat收集的数据并打印\n​ Logstash输出数据到Elasticsearch\n​ 利用Logstash过滤器解析日志\n​ 输出到Elasticsearch指定索引\n背景 日志管理的挑战：\n关注点很多，任何一个点都有可能引起问题 日志分散在很多机器，出了问题时，才发现日志被删了 很多运维人员是消防员，哪里有问题去哪里 ​ 集中化日志管理思路：\n日志收集 ——》格式化分析 ——》检索和可视化 ——》 风险告警\nELK架构\nELK架构分为两种，一种是经典的ELK，另外一种是加上消息队列（Redis或Kafka或RabbitMQ）和Nginx结构。\n经典的ELK\n经典的ELK主要是由Filebeat + Logstash + Elasticsearch + Kibana组成，如下图：（早期的ELK只有Logstash + Elasticsearch + Kibana）\n​ 此架构主要适用于数据量小的开发环境，存在数据丢失的危险。\n整合消息队列+Nginx架构 这种架构，主要加上了Redis或Kafka或RabbitMQ做消息队列，保证了消息的不丢失。\n​ 此种架构，主要用在生产环境，可以处理大数据量，并且不会丢失数据。\n什么是Logstash\nLogstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的存储库中。\nhttps://www.elastic.co/cn/logstash/\n应用：ETL工具 / 数据采集处理引擎\n​ Logstash核心概念 Pipeline\n包含了input—filter-output三个阶段的处理流程 插件生命周期管理 队列管理 Logstash Event\n数据在内部流转时的具体表现形式。数据在input 阶段被转换为Event，在 output被转化成目标格式数据 Event 其实是一个Java Object，在配置文件中，对Event 的属性进行增删改查 Codec (Code / Decode)\n将原始数据decode成Event;将Event encode成目标数据\n​ Logstash数据传输原理 数据采集与输入：Logstash支持各种输入选择，能够以连续的流式传输方式，轻松地从日志、指标、Web应用以及数据存储中采集数据。 实时解析和数据转换：通过Logstash过滤器解析各个事件，识别已命名的字段来构建结构，并将它们转换成通用格式，最终将数据从源端传输到存储库中。 存储与数据导出：Logstash提供多种输出选择，可以将数据发送到指定的地方。 Logstash通过管道完成数据的采集与处理，管道配置中包含input、output和filter（可选）插件，input和output用来配置输入和输出数据源、filter用来对数据进行过滤或预处理。\n​ Logstash配置文件结构 参考：https://www.elastic.co/guide/en/logstash/7.17/configuration.html\nLogstash的管道配置文件对每种类型的插件都提供了一个单独的配置部分，用于处理管道事件。\n​ input { stdin { } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{COMBINEDAPACHELOG}\u0026rdquo; } } date { match =\u0026gt; [ \u0026ldquo;timestamp\u0026rdquo; , \u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026ldquo;localhost:9200\u0026rdquo;]} stdout { codec =\u0026gt; rubydebug } }\n每个配置部分可以包含一个或多个插件。例如，指定多个filter插件，Logstash会按照它们在配置文件中出现的顺序进行处理。\n​ #运行 bin/logstash -f logstash-demo.conf\nInput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/input-plugins.html\n一个 Pipeline可以有多个input插件\nStdin / File\nBeats / Log4J /Elasticsearch / JDBC / Kafka /Rabbitmq /Redis\nJMX/ HTTP / Websocket / UDP / TCP\nGoogle Cloud Storage / S3\nGithub / Twitter\nOutput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/output-plugins.html\n将Event发送到特定的目的地，是 Pipeline 的最后一个阶段。\n常见 Output Plugins：\nElasticsearch Email / Pageduty Influxdb / Kafka / Mongodb / Opentsdb / Zabbix Http / TCP / Websocket Filter Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/filter-plugins.html\n处理Event\n内置的Filter Plugins:\nMutate 一操作Event的字段 Metrics — Aggregate metrics Ruby 一执行Ruby 代码 Codec Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/codec-plugins.html\n将原始数据decode成Event;将Event encode成目标数据\n内置的Codec Plugins:\nLine / Multiline JSON / Avro / Cef (ArcSight Common Event Format) Dots / Rubydebug Logstash Queue\nIn Memory Queue 进程Crash，机器宕机，都会引起数据的丢失\nPersistent Queue 机器宕机，数据也不会丢失; 数据保证会被消费; 可以替代 Kafka等消息队列缓冲区的作用\n​ queue.type: persisted (默认是memory) queue.max_bytes: 4gb\n​ Logstash安装 logstash官方文档: https://www.elastic.co/guide/en/logstash/7.17/installing-logstash.html\n1）下载并解压logstash\n下载地址： https://www.elastic.co/cn/downloads/past-releases#logstash\n选择版本：7.17.3\n​ 2）测试：运行最基本的logstash管道\n​ cd logstash-7.17.3 #linux #-e选项表示，直接把配置放在命令中，这样可以有效快速进行测试 bin/logstash -e \u0026lsquo;input { stdin { } } output { stdout {} }\u0026rsquo; #windows .\\bin\\logstash.bat -e \u0026ldquo;input { stdin { } } output { stdout {} }\u0026rdquo;\n测试结果：\n​ window版本的logstash-7.17.3的bug:\nwindows出现错误提示could not find java; set JAVA_HOME or ensure java is in PATH\n​ 修改setup.bat\n​ ​ Codec Plugin测试\n​ # single line bin/logstash -e \u0026ldquo;input{stdin{codec=\u0026gt;line}}output{stdout{codec=\u0026gt; rubydebug}}\u0026rdquo; bin/logstash -e \u0026ldquo;input{stdin{codec=\u0026gt;json}}output{stdout{codec=\u0026gt; rubydebug}}\u0026rdquo;\nCodec Plugin —— Multiline\n设置参数:\npattern: 设置行匹配的正则表达式\nwhat : 如果匹配成功，那么匹配行属于上一个事件还是下一个事件\nprevious / next negate : 是否对pattern结果取反\ntrue / false ​ # 多行数据，异常 Exception in thread \u0026ldquo;main\u0026rdquo; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) # multiline-exception.conf input { stdin { codec =\u0026gt; multiline { pattern =\u0026gt; \u0026ldquo;^\\s\u0026rdquo; what =\u0026gt; \u0026ldquo;previous\u0026rdquo; } } } filter {} output { stdout { codec =\u0026gt; rubydebug } } #执行管道 bin/logstash -f multiline-exception.conf\nInput Plugin —— File\n支持从文件中读取数据，如日志文件 文件读取需要解决的问题：只被读取一次。重启后需要从上次读取的位置继续(通过sincedb 实现) 读取到文件新内容，发现新文件 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取 Filter Plugin\nFilter Plugin可以对Logstash Event进行各种处理，例如解析，删除字段，类型转换\nDate: 日期解析 Dissect: 分割符解析 Grok: 正则匹配解析 Mutate: 处理字段。重命名，删除，替换 Ruby: 利用Ruby 代码来动态修改Event Filter Plugin - Mutate\n对字段做各种操作:\nConvert : 类型转换 Gsub : 字符串替换 Split / Join /Merge: 字符串切割，数组合并字符串，数组合并数组 Rename: 字段重命名 Update / Replace: 字段内容更新替换 Remove_field: 字段删除 Logstash导入数据到ES\n1）测试数据集下载：https://grouplens.org/datasets/movielens/\n​ 2）准备logstash-movie.conf配置文件\n​ input { file { path =\u0026gt; \u0026ldquo;/home/es/logstash-7.17.3/dataset/movies.csv\u0026rdquo; start_position =\u0026gt; \u0026ldquo;beginning\u0026rdquo; sincedb_path =\u0026gt; \u0026ldquo;/dev/null\u0026rdquo; } } filter { csv { separator =\u0026gt; \u0026ldquo;,\u0026rdquo; columns =\u0026gt; [\u0026ldquo;id\u0026rdquo;,\u0026ldquo;content\u0026rdquo;,\u0026ldquo;genre\u0026rdquo;] } mutate { split =\u0026gt; { \u0026ldquo;genre\u0026rdquo; =\u0026gt; \u0026ldquo;|\u0026rdquo; } remove_field =\u0026gt; [\u0026ldquo;path\u0026rdquo;, \u0026ldquo;host\u0026rdquo;,\u0026quot;@timestamp\u0026quot;,\u0026ldquo;message\u0026rdquo;] } mutate { split =\u0026gt; [\u0026ldquo;content\u0026rdquo;, \u0026ldquo;(\u0026rdquo;] add_field =\u0026gt; { \u0026ldquo;title\u0026rdquo; =\u0026gt; \u0026ldquo;%{[content][0]}\u0026rdquo;} add_field =\u0026gt; { \u0026ldquo;year\u0026rdquo; =\u0026gt; \u0026ldquo;%{[content][1]}\u0026rdquo;} } mutate { convert =\u0026gt; { \u0026ldquo;year\u0026rdquo; =\u0026gt; \u0026ldquo;integer\u0026rdquo; } strip =\u0026gt; [\u0026ldquo;title\u0026rdquo;] remove_field =\u0026gt; [\u0026ldquo;path\u0026rdquo;, \u0026ldquo;host\u0026rdquo;,\u0026quot;@timestamp\u0026quot;,\u0026ldquo;message\u0026rdquo;,\u0026ldquo;content\u0026rdquo;] } } output { elasticsearch { hosts =\u0026gt; \u0026ldquo;http://localhost:9200\u0026rdquo; index =\u0026gt; \u0026ldquo;movies\u0026rdquo; document_id =\u0026gt; \u0026ldquo;%{id}\u0026rdquo; user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout {} }\n3）运行logstash\n​ # linux bin/logstash -f logstash-movie.conf\n同步数据库数据到Elasticsearch 需求: 将数据库中的数据同步到ES，借助ES的全文搜索,提高搜索速度\n需要把新增用户信息同步到Elasticsearch中 用户信息Update 后，需要能被更新到Elasticsearch 支持增量更新 用户注销后，不能被ES所搜索到 实现思路\n基于canal同步数据（项目实战中讲解）\n借助JDBC Input Plugin将数据从数据库读到Logstash\n需要自己提供所需的 JDBC Driver； JDBC Input Plugin 支持定时任务 Scheduling，其语法来自 Rufus-scheduler，其扩展了 Cron，使用 Cron 的语法可以完成任务的触发； JDBC Input Plugin 支持通过 Tracking_column / sql_last_value 的方式记录 State，最终实现增量的更新； https://www.elastic.co/cn/blog/logstash-jdbc-input-plugin JDBC Input Plugin实现步骤\n1）拷贝jdbc依赖到logstash-7.17.3/drivers目录下\n2）准备mysql-demo.conf配置文件\n​ input { jdbc { jdbc_driver_library =\u0026gt; \u0026ldquo;/home/es/logstash-7.17.3/drivers/mysql-connector-java-5.1.49.jar\u0026rdquo; jdbc_driver_class =\u0026gt; \u0026ldquo;com.mysql.jdbc.Driver\u0026rdquo; jdbc_connection_string =\u0026gt; \u0026ldquo;jdbc:mysql://localhost:3306/test?useSSL=false\u0026rdquo; jdbc_user =\u0026gt; \u0026ldquo;root\u0026rdquo; jdbc_password =\u0026gt; \u0026ldquo;123456\u0026rdquo; #启用追踪，如果为true，则需要指定tracking_column use_column_value =\u0026gt; true #指定追踪的字段， tracking_column =\u0026gt; \u0026ldquo;last_updated\u0026rdquo; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =\u0026gt; \u0026ldquo;numeric\u0026rdquo; #记录最后一次运行的结果 record_last_run =\u0026gt; true #上面运行结果的保存位置 last_run_metadata_path =\u0026gt; \u0026ldquo;jdbc-position.txt\u0026rdquo; statement =\u0026gt; \u0026ldquo;SELECT * FROM user where last_updated \u0026gt;:sql_last_value;\u0026rdquo; schedule =\u0026gt; \u0026quot; * * * * * *\u0026quot; } } output { elasticsearch { document_id =\u0026gt; \u0026ldquo;%{id}\u0026rdquo; document_type =\u0026gt; \u0026ldquo;_doc\u0026rdquo; index =\u0026gt; \u0026ldquo;users\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n3）运行logstash\n​ bin/logstash -f mysql-demo.conf\n​ 测试\n​ #user表 CREATE TABLE user ( id int NOT NULL AUTO_INCREMENT, name varchar(50) DEFAULT NULL, address varchar(50) CHARACTER DEFAULT NULL, last_updated bigint DEFAULT NULL, is_deleted int DEFAULT NULL, PRIMARY KEY (id) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; #插入数据 INSERT INTO user(name,address,last_updated,is_deleted) VALUES(\u0026ldquo;张三\u0026rdquo;,\u0026ldquo;广州天河\u0026rdquo;,unix_timestamp(NOW()),0)\n​ ​ # 更新 update user set address=\u0026ldquo;广州白云山\u0026rdquo;,last_updated=unix_timestamp(NOW()) where name=\u0026ldquo;张三\u0026rdquo;\n​ ​ #删除 update user set is_deleted=1,last_updated=unix_timestamp(NOW()) where name=\u0026ldquo;张三\u0026rdquo;\n​ ​ #ES中查询 # 创建 alias，只显示没有被标记 deleted的用户 POST /_aliases { \u0026ldquo;actions\u0026rdquo;: [ { \u0026ldquo;add\u0026rdquo;: { \u0026ldquo;index\u0026rdquo;: \u0026ldquo;users\u0026rdquo;, \u0026ldquo;alias\u0026rdquo;: \u0026ldquo;view_users\u0026rdquo;, \u0026ldquo;filter\u0026rdquo; : { \u0026ldquo;term\u0026rdquo; : { \u0026ldquo;is_deleted\u0026rdquo; : 0} } } } ] } # 通过 Alias查询，查不到被标记成 deleted的用户 POST view_users/_search POST view_users/_search { \u0026ldquo;query\u0026rdquo;: { \u0026ldquo;term\u0026rdquo;: { \u0026ldquo;name.keyword\u0026rdquo;: { \u0026ldquo;value\u0026rdquo;: \u0026ldquo;张三\u0026rdquo; } } } }\n什么是Beats 轻量型数据采集器，文档地址： https://www.elastic.co/guide/en/beats/libbeat/7.17/index.html\nBeats 是一个免费且开放的平台，集合了多种单一用途的数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。\n​ FileBeat简介 FileBeat专门用于转发和收集日志数据的轻量级采集工具。它可以作为代理安装在服务器上，FileBeat监视指定路径的日志文件，收集日志数据，并将收集到的日志转发到Elasticsearch或者Logstash。\nFileBeat的工作原理 启动FileBeat时，会启动一个或者多个输入（Input），这些Input监控指定的日志数据位置。FileBeat会针对每一个文件启动一个Harvester（收割机）。Harvester读取每一个文件的日志，将新的日志发送到libbeat，libbeat将数据收集到一起，并将数据发送给输出（Output）。\n​ logstash vs FileBeat\nLogstash是在jvm上运行的，资源消耗比较大。而FileBeat是基于golang编写的，功能较少但资源消耗也比较小，更轻量级。 Logstash 和Filebeat都具有日志收集功能，Filebeat更轻量，占用资源更少 Logstash 具有Filter功能，能过滤分析日志 一般结构都是Filebeat采集日志，然后发送到消息队列、Redis、MQ中，然后Logstash去获取，利用Filter功能过滤分析，然后存储到Elasticsearch中 FileBeat和Logstash配合，实现背压机制。当将数据发送到Logstash或 Elasticsearch时，Filebeat使用背压敏感协议，以应对更多的数据量。如果Logstash正在忙于处理数据，则会告诉Filebeat 减慢读取速度。一旦拥堵得到解决，Filebeat就会恢复到原来的步伐并继续传输数据。 Filebeat安装 https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html\n1）下载并解压Filebeat\n下载地址：https://www.elastic.co/cn/downloads/past-releases#filebeat\n选择版本：7.17.3\n​ 2）编辑配置\n修改 filebeat.yml 以设置连接信息：\n​ output.elasticsearch: hosts: [\u0026ldquo;192.168.65.174:9200\u0026rdquo;,\u0026ldquo;192.168.65.192:9200\u0026rdquo;,\u0026ldquo;192.168.65.204:9200\u0026rdquo;] username: \u0026ldquo;elastic\u0026rdquo; password: \u0026ldquo;123456\u0026rdquo; setup.kibana: host: \u0026ldquo;192.168.65.174:5601\u0026rdquo;\n3) 启用和配置数据收集模块\n从安装目录中，运行：\n​ # 查看可以模块列表 ./filebeat modules list #启用nginx模块 ./filebeat modules enable nginx #如果需要更改nginx日志路径,修改modules.d/nginx.yml - module: nginx access: var.paths: [\u0026quot;/var/log/nginx/access.log*\u0026quot;] #启用 Logstash 模块 ./filebeat modules enable logstash #在 modules.d/logstash.yml 文件中修改设置 - module: logstash log: enabled: true var.paths: [\u0026quot;/home/es/logstash-7.17.3/logs/*.log\u0026quot;]\n4）启动 Filebeat\n​ # setup命令加载Kibana仪表板。 如果仪表板已经设置，则忽略此命令。 ./filebeat setup # 启动Filebeat ./filebeat -e\nELK整合实战 案例：采集tomcat服务器日志\nTomcat服务器运行过程中产生很多日志信息，通过Logstash采集并存储日志信息至ElasticSearch中\n使用FileBeats将日志发送到Logstash\n1）创建配置文件filebeat-logstash.yml，配置FileBeats将数据发送到Logstash\n​ vim filebeat-logstash.yml chmod 644 filebeat-logstash.yml #因为Tomcat的web log日志都是以IP地址开头的，所以我们需要修改下匹配字段。 # 不以ip地址开头的行追加到上一行 filebeat.inputs: - type: log enabled: true paths: - /home/es/apache-tomcat-8.5.33/logs/access.* multiline.pattern: \u0026lsquo;^\\d+\\.\\d+\\.\\d+\\.\\d+ \u0026rsquo; multiline.negate: true multiline.match: after output.logstash: enabled: true hosts: [\u0026ldquo;192.168.65.204:5044\u0026rdquo;]\npattern：正则表达式 negate：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行 match：after 或 before，合并到上一行的末尾或开头 2）启动FileBeat，并指定使用指定的配置文件\n​ ./filebeat -e -c filebeat-logstash.yml\n可能出现的异常：\n异常1：Exiting: error loading config file: config file (\u0026ldquo;filebeat-logstash.yml\u0026rdquo;) can only be writable by the owner but the permissions are \u0026ldquo;-rw-rw-r\u0026ndash;\u0026rdquo; (to fix the permissions use: \u0026lsquo;chmod go-w /home/es/filebeat-7.17.3-linux-x86_64/filebeat-logstash.yml\u0026rsquo;)\n因为安全原因不要其他用户写的权限，去掉写的权限就可以了\n​ chmod 644 filebeat-logstash.yml\n异常2：Failed to connect to backoff(async(tcp://192.168.65.204:5044)): dial tcp 192.168.65.204:5044: connect: connection refused\nFileBeat将尝试建立与Logstash监听的IP和端口号进行连接。但此时，我们并没有开启并配置Logstash，所以FileBeat是无法连接到Logstash的。\n配置Logstash接收FileBeat收集的数据并打印\n​ vim config/filebeat-console.conf # 配置从FileBeat接收数据 input { beats { port =\u0026gt; 5044 } } output { stdout { codec =\u0026gt; rubydebug } }\n测试logstash配置是否正确\n​ bin/logstash -f config/filebeat-console.conf \u0026ndash;config.test_and_exit\n启动logstash\n​ # reload.automatic：修改配置文件时自动重新加载 bin/logstash -f config/filebeat-console.conf \u0026ndash;config.reload.automatic\n测试访问tomcat，logstash是否接收到了Filebeat传过来的tomcat日志\nLogstash输出数据到Elasticsearch\n如果我们需要将数据输出值ES而不是控制台的话，我们修改Logstash的output配置。\n​ vim config/filebeat-elasticSearch.conf input { beats { port =\u0026gt; 5044 } } output { elasticsearch { hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n启动logstash\n​ bin/logstash -f config/filebeat-elasticSearch.conf \u0026ndash;config.reload.automatic\nES中会生成一个以logstash开头的索引，测试日志是否保存到了ES。\n思考：日志信息都保证在message字段中，是否可以把日志进行解析一个个的字段？例如：IP字段、时间、请求方式、请求URL、响应结果。\n利用Logstash过滤器解析日志\n从日志文件中收集到的数据包含了很多有效信息，比如IP、时间等，在Logstash中可以配置过滤器Filter对采集到的数据进行过滤处理，Logstash中有大量的插件可以供我们使用。\n​ 查看Logstash已经安装的插件 bin/logstash-plugin list\nGrok插件\nGrok是一种将非结构化日志解析为结构化的插件。这个工具非常适合用来解析系统日志、Web服务器日志、MySQL或者是任意其他的日志格式。\nhttps://www.elastic.co/guide/en/logstash/7.17/plugins-filters-grok.html\nGrok语法\nGrok是通过模式匹配的方式来识别日志中的数据,可以把Grok插件简单理解为升级版本的正则表达式。它拥有更多的模式，默认Logstash拥有120个模式。如果这些模式不满足我们解析日志的需求，我们可以直接使用正则表达式来进行匹配。\ngrok模式的语法是：\n​ %{SYNTAX:SEMANTIC}\nSYNTAX（语法）指的是Grok模式名称，SEMANTIC（语义）是给模式匹配到的文本字段名。例如：\n​ %{NUMBER:duration} %{IP:client} duration表示：匹配一个数字，client表示匹配一个IP地址。\n默认在Grok中，所有匹配到的的数据类型都是字符串，如果要转换成int类型（目前只支持int和float），可以这样：%{NUMBER:duration:int} %{IP:client}\n常用的Grok模式\nhttps://help.aliyun.com/document_detail/129387.html?scm=20140722.184.2.173\n用法\n​ filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026rdquo; } } }\n比如，tomacat日志\n​ 192.168.65.103 - - [23/Jun/2022:22:37:23 +0800] \u0026ldquo;GET /docs/images/docs-stylesheet.css HTTP/1.1\u0026rdquo; 200 5780\n解析后的字段\n字段名 说明 client IP 浏览器端IP timestamp 请求的时间戳 method 请求方式（GET/POST） uri 请求的链接地址 status 服务器端响应状态 length 响应的数据长度 grok模式\n​ %{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status} %{INT:length}\n为了方便测试，我们可以使用Kibana来进行Grok开发：\n​ 修改Logstash配置文件\n​ vim config/filebeat-console.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status:int} %{INT:length:int}\u0026rdquo; } } } output { stdout { codec =\u0026gt; rubydebug } }\n启动logstash测试\n​ bin/logstash -f config/filebeat-console.conf \u0026ndash;config.reload.automatic\n使用mutate插件过滤掉不需要的字段\n​ mutate { enable_metric =\u0026gt; \u0026ldquo;false\u0026rdquo; remove_field =\u0026gt; [\u0026ldquo;message\u0026rdquo;, \u0026ldquo;log\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;input\u0026rdquo;, \u0026ldquo;agent\u0026rdquo;, \u0026ldquo;host\u0026rdquo;, \u0026ldquo;ecs\u0026rdquo;, \u0026ldquo;@version\u0026rdquo;] }\n要将日期格式进行转换，我们可以使用Date插件来实现。该插件专门用来解析字段中的日期，官方说明文档：https://www.elastic.co/guide/en/logstash/7.17/plugins-filters-date.html\n用法如下：\n​ 将date字段转换为「年月日 时分秒」格式。默认字段经过date插件处理后，会输出到@timestamp字段，所以，我们可以通过修改target属性来重新定义输出字段。\n​ date { match =\u0026gt; [\u0026ldquo;date\u0026rdquo;,\u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo;,\u0026ldquo;yyyy-MM-dd HH:mm:ss\u0026rdquo;] target =\u0026gt; \u0026ldquo;date\u0026rdquo; }\n输出到Elasticsearch指定索引\nindex来指定索引名称，默认输出的index名称为：logstash-%{+yyyy.MM.dd}。但注意，要在index中使用时间格式化，filter的输出必须包含 @timestamp字段，否则将无法解析日期。\n​ output { elasticsearch { index =\u0026gt; \u0026ldquo;tomcat_web_log_%{+YYYY-MM}\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } stdout{ codec =\u0026gt; rubydebug } }\n注意：index名称中，不能出现大写字符\n完整的Logstash配置文件\n​ vim config/filebeat-filter-es.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;%{IP:ip} - - [%{HTTPDATE:date}] \u0026quot;%{WORD:method} %{PATH:uri} %{DATA:protocol}\u0026quot; %{INT:status:int} %{INT:length:int}\u0026rdquo; } } mutate { enable_metric =\u0026gt; \u0026ldquo;false\u0026rdquo; remove_field =\u0026gt; [\u0026ldquo;message\u0026rdquo;, \u0026ldquo;log\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;input\u0026rdquo;, \u0026ldquo;agent\u0026rdquo;, \u0026ldquo;host\u0026rdquo;, \u0026ldquo;ecs\u0026rdquo;, \u0026ldquo;@version\u0026rdquo;] } date { match =\u0026gt; [\u0026ldquo;date\u0026rdquo;,\u0026ldquo;dd/MMM/yyyy:HH:mm:ss Z\u0026rdquo;,\u0026ldquo;yyyy-MM-dd HH:mm:ss\u0026rdquo;] target =\u0026gt; \u0026ldquo;date\u0026rdquo; } } output { stdout { codec =\u0026gt; rubydebug } elasticsearch { index =\u0026gt; \u0026ldquo;tomcat_web_log_%{+YYYY-MM}\u0026rdquo; hosts =\u0026gt; [\u0026ldquo;http://localhost:9200\u0026rdquo;] user =\u0026gt; \u0026ldquo;elastic\u0026rdquo; password =\u0026gt; \u0026ldquo;123456\u0026rdquo; } }\n启动logstash\n​ bin/logstash -f config/filebeat-filter-es.conf \u0026ndash;config.reload.automatic\n","permalink":"https://wandong1.github.io/post/elasticsearch/","summary":"ElasticSearch快速入门实战 主讲老师：Fox\nES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\n​ 文档：1.ElasticSearch快速入门实战.note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:","title":"ElasticSearch快速入门实战"},{"content":"Git Git 是一个开源的分布式版本控制软件,用以有效、高速的处理从很小到非常大的项目版本管理。 Git 最初是由Linus Torvalds设计开发的，用于管理Linux内核开发。Git 是根据GNU通用公共许可证版本2的条款分发的自由/免费软件，安装参见：http://git-scm.com/\n打开git bash，初始化配置 git config --global user.name \u0026#34;wandong\u0026#34; git config --global user.email \u0026#34;993696910@qq.com\u0026#34; # 对已存在的目录进行git的初始化 git init # 添加远程仓库地址 git remote add origin http://git.cqzwymgmt.com/root/gin-project-orm.git # git add . git commit -m \u0026#34;Initial commit\u0026#34; # 推送到远程仓库 master分支 git push -u origin master 在新的环境拉取代码，进行开发 git clone http://git.cqzwymgmt.com/root/gin-project-orm.git # 创建新的分支继续开发 git branch dev # 列出所有分支 git branch # 切换分支 git checkout dev # 可以开始开发新功能了，尽量开发新的文件，避免合并的时候出现冲突进而解决冲突。 git add . git commit -m \u0026#34;change log function\u0026#34; # 推送到远程仓库 dev分支 git push -u origin dev 更新本地代码 # 拉取最新的dev分支代码，如果本地没有该分支，先创建 git branch dev # 使用pull命令更新分支代码的时候，要先处于该分支，不然会被合并 git branch dev git checkout dev git pull origin dev # 查看dev分支代码和master代码区别 将dev分支合并到master分支 git merge dev # 或者 git rebase dev 删除本地和远程仓库的分支 # 删除分支前先切换其他分支 git branch -d dev git push origin --delete dev 将你的仓库和你的gitee合并了，用填充的方法，即： git pull --rebase origin master ","permalink":"https://wandong1.github.io/post/git/","summary":"Git Git 是一个开源的分布式版本控制软件,用以有效、高速的处理从很小到非常大的项目版本管理。 Git 最初是由Linus Torvalds设计开发的，用于管理Linux内核开发。Git 是根据GNU通用公共许可证版本2的条款分发的自由/免费软件，安装参见：http://git-scm.com/\n打开git bash，初始化配置 git config --global user.name \u0026#34;wandong\u0026#34; git config --global user.email \u0026#34;993696910@qq.com\u0026#34; # 对已存在的目录进行git的初始化 git init # 添加远程仓库地址 git remote add origin http://git.cqzwymgmt.com/root/gin-project-orm.git # git add . git commit -m \u0026#34;Initial commit\u0026#34; # 推送到远程仓库 master分支 git push -u origin master 在新的环境拉取代码，进行开发 git clone http://git.cqzwymgmt.com/root/gin-project-orm.git # 创建新的分支继续开发 git branch dev # 列出所有分支 git branch # 切换分支 git checkout dev # 可以开始开发新功能了，尽量开发新的文件，避免合并的时候出现冲突进而解决冲突。 git add . git commit -m \u0026#34;change log function\u0026#34; # 推送到远程仓库 dev分支 git push -u origin dev 更新本地代码 # 拉取最新的dev分支代码，如果本地没有该分支，先创建 git branch dev # 使用pull命令更新分支代码的时候，要先处于该分支，不然会被合并 git branch dev git checkout dev git pull origin dev # 查看dev分支代码和master代码区别 将dev分支合并到master分支 git merge dev # 或者 git rebase dev 删除本地和远程仓库的分支 # 删除分支前先切换其他分支 git branch -d dev git push origin --delete dev 将你的仓库和你的gitee合并了，用填充的方法，即： git pull --rebase origin master ","title":"git的使用方法"},{"content":"ES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\nElasticSearch快速入门实战 note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引）\n文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）\n假如，我们有一个站内搜索的功能，通过某个关键词来搜索相关的文章，那么这个关键词可能出现在标题中，也可能出现在文章内容中，那我们将会在创建或修改文章的时候，建立一个关键词与文章的对应关系表，这种，我们可以称之为倒排索引。\nlike %java设计模式% java 设计模式\n关键词 文章ID JAVA 1,2 设计模式 1,2 多线程 2 简单理解，正向索引是通过key找value，反向索引则是通过value找key。ES底层在检索时底层使用的就是倒排索引。\nElasticSearch简介 ElasticSearch是什么 ElasticSearch（简称ES）是一个分布式、RESTful 风格的搜索和数据分析引擎，是用Java开发并且是当前最流行的开源的企业级搜索引擎，能够达到近实时搜索，稳定，可靠，快速，安装使用方便。\n客户端支持Java、.NET（C#）、PHP、Python、Ruby等多种语言。\n官方网站: https://www.elastic.co/\n**下载地址：**https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n搜索引擎排名：\n​ 参考网站：https://db-engines.com/en/ranking/search+engine\n起源——Lucene 基于Java语言开发的搜索引擎库类\n创建于1999年，2005年成为Apache 顶级开源项目\nLucene具有高性能、易扩展的优点\nLucene的局限性︰\n只能基于Java语言开发 类库的接口学习曲线陡峭 原生并不支持水平扩展 Elasticsearch的诞生 Elasticsearch是构建在Apache Lucene之上的开源分布式搜索引擎。\n2004年 Shay Banon 基于Lucene开发了Compass\n2010年 Shay Banon重写了Compass，取名Elasticsearch\n支持分布式，可水平扩展 降低全文检索的学习曲线，可以被任何编程语言调用 ​ Elasticsearch 与 Lucene 核心库竞争的优势在于：\n完美封装了 Lucene 核心库，设计了友好的 Restful-API，开发者无需过多关注底层机制，直接开箱即用。 分片与副本机制，直接解决了集群下性能与高可用问题。 ES Server进程 3节点 raft (奇数节点)\n数据分片 -》lucene实例 分片和副本数 1个ES节点可以有多个lucene实例。也可以指定一个索引的多个分片\n​ ElasticSearch版本特性 5.x新特性\nLucene 6.x， 性能提升，默认打分机制从TF-IDF改为BM 25\n支持Ingest节点/ Painless Scripting / Completion suggested支持/原生的Java REST客户端\nType标记成deprecated， 支持了Keyword的类型\n性能优化\n内部引擎移除了避免同一文档并发更新的竞争锁，带来15% - 20%的性能提升 Instant aggregation,支持分片，上聚合的缓存 新增了Profile API 6.x新特性\nLucene 7.x\n新功能\n跨集群复制(CCR) 索引生命周期管理 SQL的支持 更友好的的升级及数据迁移\n在主要版本之间的迁移更为简化，体验升级 全新的基于操作的数据复制框架，可加快恢复数据 性能优化\n有效存储稀疏字段的新方法，降低了存储成本 在索引时进行排序，可加快排序的查询性能 7.x新特性\nLucene 8.0\n重大改进-正式废除单个索引下多Type的支持\n7.1开始，Security 功能免费使用\nECK - Elasticseach Operator on Kubernetes\n新功能\nNew Cluster coordination Feature——Complete High Level REST Client Script Score Query 性能优化\n默认的Primary Shard数从5改为1,避免Over Sharding 性能优化， 更快的Top K 8.x新特性\nRest API相比较7.x而言做了比较大的改动（比如彻底删除_type） 默认开启安全配置 存储空间优化：对倒排文件使用新的编码集，对于keyword、match_only_text、text类型字段有效，有3.5%的空间优化提升，对于新建索引和segment自动生效。 优化geo_point，geo_shape类型的索引（写入）效率：15%的提升。 技术预览版KNN API发布，（K邻近算法），跟推荐系统、自然语言排名相关。 https://www.elastic.co/guide/en/elastic-stack/current/elasticsearch-breaking-changes.html ElasticSearch vs Solr Solr 是第一个基于 Lucene 核心库功能完备的搜索引擎产品，诞生远早于 Elasticsearch。\n当单纯的对已有数据进行搜索时，Solr更快。当实时建立索引时, Solr会产生io阻塞，查询性能较差, Elasticsearch具有明显的优势。\n​ ​ 大型互联网公司，实际生产环境测试，将搜索引擎从Solr转到 Elasticsearch以后的平均查询速度有了50倍的提升。\n​ 总结：\nSolr 利用 Zookeeper 进行分布式管理，而Elasticsearch 自身带有分布式协调管理功能。 Solr 支持更多格式的数据，比如JSON、XML、CSV，而 Elasticsearch 仅支持json文件格式。 Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。 Solr 是传统搜索应用的有力解决方案，但 Elasticsearch更适用于新兴的实时搜索应用。 Elastic Stack介绍 在Elastic Stack之前我们听说过ELK，ELK分别是Elasticsearch，Logstash，Kibana这三款软件在一起的简称，在发展的过程中又有新的成员Beats的加入，就形成了Elastic Stack。\n​ Elastic Stack生态圈\n在Elastic Stack生态圈中Elasticsearch作为数据存储和搜索，是生态圈的基石，Kibana在上层提供用户一个可视化及操作的界面，Logstash和Beat可以对数据进行收集。在上图的右侧X-Pack部分则是Elastic公司提供的商业项目。\n指标分析/日志分析：\n​ ElasticSearch应用场景 站内搜索 日志管理与分析 大数据分析 应用性能监控 机器学习 国内现在有大量的公司都在使用 Elasticsearch，包括携程、滴滴、今日头条、饿了么、360安全、小米、vivo等诸多知名公司。除了搜索之外，结合Kibana、Logstash、Beats，Elastic Stack还被广泛运用在大数据近实时分析领域，包括日志分析、指标监控、信息安全等多个领域。它可以帮助你探索海量结构化、非结构化数据，按需创建可视化报表，对监控数据设置报警阈值，甚至通过使用机器学习技术，自动识别异常状况。\n通用数据处理流程：\n​ ElasticSearch快速开始 安装JDK\n1、yum install -y java-1.8.0-openjdk* # 或者 mkdir /opt/jdk;tar -xvzf jdk-8u333-linux-x64.tar.gz -C /opt/jdk/; mv /opt/jdk/jdk1.8.0_333/ /opt/jdk/jdk1.8 ; cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/profile JAVA_HOME=/opt/jdk/jdk1.8 CLASSPATH=$JAVA_HOME/lib/ PATH=$PATH:$JAVA_HOME/bin export PATH JAVA_HOME CLASSPATH EOF source /etc/profile ElasticSearch安装运行 环境准备 运行Elasticsearch，需安装并配置JDK\n设置$JAVA_HOME 各个版本对Java的依赖 https://www.elastic.co/support/matrix#matrix_jvm\nElasticsearch 5需要Java 8以上的版本 Elasticsearch 从6.5开始支持Java 11 7.0开始，内置了Java环境 ES比较耗内存，建议虚拟机4G或以上内存，jvm1g以上的内存分配\n可以参考es的环境文件elasticsearch-env.bat\n​ ES的jdk环境生效的优先级配置ES_JAVA_HOME\u0026gt;JAVA_HOME\u0026gt;ES_HOME\n下载并解压ElasticSearch 下载地址： https://www.elastic.co/cn/downloads/past-releases#elasticsearch\n选择版本：7.17.3\n​ ElasticSearch文件目录结构\n目录 描述 bin 脚本文件，包括启动elasticsearch，安装插件，运行统计数据等 config 配置文件目录，如elasticsearch配置、角色配置、jvm配置等。 jdk java运行环境 data 默认的数据存放目录，包含节点、分片、索引、文档的所有数据，生产环境需要修改。 lib elasticsearch依赖的Java类库 logs 默认的日志文件存储路径，生产环境需要修改。 modules 包含所有的Elasticsearch模块，如Cluster、Discovery、Indices等。 plugins 已安装插件目录 主配置文件elasticsearch.yml\ncluster.name 当前节点所属集群名称，多个节点如果要组成同一个集群，那么集群名称一定要配置成相同。默认值elasticsearch，生产环境建议根据ES集群的使用目的修改成合适的名字。\nnode.name 当前节点名称，默认值当前节点部署所在机器的主机名，所以如果一台机器上要起多个ES节点的话，需要通过配置该属性明确指定不同的节点名称。\npath.data 配置数据存储目录，比如索引数据等，默认值 $ES_HOME/data，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\npath.logs 配置日志存储目录，比如运行日志和集群健康信息等，默认值 $ES_HOME/logs，生产环境下强烈建议部署到另外的安全目录，防止ES升级导致数据被误删除。\nbootstrap.memory_lock 配置ES启动时是否进行内存锁定检查，默认值true。\nES对于内存的需求比较大，一般生产环境建议配置大内存，如果内存不足，容易导致内存交换到磁盘，严重影响ES的性能。所以默认启动时进行相应大小内存的锁定，如果无法锁定则会启动失败。\n非生产环境可能机器内存本身就很小，能够供给ES使用的就更小，如果该参数配置为true的话很可能导致无法锁定内存以致ES无法成功启动，此时可以修改为false。\nnetwork.host 配置能够访问当前节点的主机，默认值为当前节点所在机器的本机回环地址127.0.0.1 和[::1]，这就导致默认情况下只能通过当前节点所在主机访问当前节点。可以配置为 0.0.0.0 ，表示所有主机均可访问。\nhttp.port 配置当前ES节点对外提供服务的http端口，默认值 9200\ndiscovery.seed_hosts 配置参与集群节点发现过程的主机列表，说白一点就是集群中所有节点所在的主机列表，可以是具体的IP地址，也可以是可解析的域名。\ncluster.initial_master_nodes 配置ES集群初始化时参与master选举的节点名称列表，必须与node.name配置的一致。ES集群首次构建完成后，应该将集群中所有节点的配置文件中的cluster.initial_master_nodes配置项移除，重启集群或者将新节点加入某个已存在的集群时切记不要设置该配置项。\n​ #ES开启远程访问 network.host: 0.0.0.0\n修改JVM配置 修改config/jvm.options配置文件，调整jvm堆内存大小\n​ vim jvm.options -Xms4g -Xmx4g\n配置的建议\nXms和Xms设置成—样 Xmx不要超过机器内存的50% 不要超过30GB - https://www.elastic.co/cn/blog/a-heap-of-trouble 启动ElasticSearch服务 Windows\n直接运行elasticsearch.bat\nLinux（centos7）\nES不允许使用root账号启动服务，如果你当前账号是root，则需要创建一个专有账户\n​ #非root用户 bin/elasticsearch # -d 后台启动 bin/elasticsearch -d\n​ 注意：es默认不能用root用户启动，生产环境建议为elasticsearch创建用户。\n​ #为elaticsearch创建用户并赋予相应权限 adduser es passwd es chown -R es:es elasticsearch-17.3\n运行http://localhost:9200/\n​ 如果ES服务启动异常，会有提示：\n​ 启动ES服务常见错误解决方案\n[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\nES因为需要大量的创建索引文件，需要大量的打开系统的文件，所以我们需要解除linux系统当中打开文件最大数目的限制，不然ES启动就会抛错\n#切换到root用户 vim /etc/security/limits.conf 末尾添加如下配置： *\tsoft nofile 65536 * hard nofile 65536 * soft nproc 4096 *\thard nproc 4096 ​\n[2]: max number of threads [1024] for user [es] is too low, increase to at least [4096]\n无法创建本地线程问题,用户最大可创建线程数太小\nvim /etc/security/limits.d/20-nproc.conf 改为如下配置： * soft nproc 4096 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n最大虚拟内存太小,调大系统的虚拟内存\n​\nvim /etc/sysctl.conf 追加以下内容： vm.max_map_count=262144 保存退出之后执行如下命令： sysctl -p [4]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured\n缺少默认配置，至少需要配置discovery.seed_hosts/discovery.seed_providers/cluster.initial_master_nodes中的一个参数.\ndiscovery.seed_hosts: 集群主机列表 discovery.seed_providers: 基于配置文件配置集群主机列表 cluster.initial_master_nodes: 启动时初始化的参与选主的node，生产环境必填 ​\nvim config/elasticsearch.yml #添加配置 discovery.seed_hosts: [\u0026#34;127.0.0.1\u0026#34;] cluster.initial_master_nodes: [\u0026#34;node-1\u0026#34;] #或者 单节点（集群单节点） discovery.type: single-node 客户端Kibana安装 Kibana是一个开源分析和可视化平台，旨在与Elasticsearch协同工作。\n1）下载并解压缩Kibana\n下载地址：https://www.elastic.co/cn/downloads/past-releases#kibana\n选择版本：7.17.3\n​ 2）修改Kibana.yml\nvim config/kibana.yml server.port: 5601 server.host: \u0026#34;0.0.0.0\u0026#34; #服务器ip elasticsearch.hosts: [\u0026#34;http://localhost:9200\u0026#34;] #elasticsearch的访问地址 i18n.locale: \u0026#34;zh-CN\u0026#34; #Kibana汉化 3）运行Kibana\n# 注意：kibana也需要非root用户启动 bin/kibana # 后台启动 nohup bin/kibana \u0026amp; # 或者 sudo -H -u es /bin/bash -c \u0026#34;nohup bin/kibana \u0026amp;\u0026#34; ​\n访问Kibana: http://localhost:5601/\n​ cat API\n/_cat/allocation #查看单节点的shard分配整体情况 /_cat/shards #查看各shard的详细情况 /_cat/shards/{index} #查看指定分片的详细情况 /_cat/master #查看master节点信息 /_cat/nodes #查看所有节点信息 /_cat/indices #查看集群中所有index的详细信息 /_cat/indices/{index} #查看集群中指定index的详细信息 /_cat/segments #查看各index的segment详细信息,包括segment名, 所属shard, 内存(磁盘)占用大小, 是否刷盘 /_cat/segments/{index}#查看指定index的segment详细信息 /_cat/count #查看当前集群的doc数量 /_cat/count/{index} #查看指定索引的doc数量 /_cat/recovery #查看集群内每个shard的recovery过程.调整replica。 /_cat/recovery/{index}#查看指定索引shard的recovery过程 /_cat/health #查看集群当前状态：红、黄、绿 /_cat/pending_tasks #查看当前集群的pending task /_cat/aliases #查看集群中所有alias信息,路由配置等 /_cat/aliases/{alias} #查看指定索引的alias信息 /_cat/thread_pool #查看集群各节点内部不同类型的threadpool的统计信息, /_cat/plugins #查看集群各个节点上的plugin信息 /_cat/fielddata #查看当前集群各个节点的fielddata内存使用情况 /_cat/fielddata/{fields} #查看指定field的内存使用情况,里面传field属性对应的值 /_cat/nodeattrs #查看单节点的自定义属性 /_cat/repositories #输出集群中注册快照存储库 /_cat/templates #输出当前正在存在的模板信息 Elasticsearch安装分词插件 Elasticsearch提供插件机制对系统进行扩展\n以安装analysis-icu这个分词插件为例\n在线安装\n​\n#查看已安装插件 bin/elasticsearch-plugin list #安装插件 bin/elasticsearch-plugin install analysis-icu #删除插件 bin/elasticsearch-plugin remove analysis-icu 注意：安装和删除完插件后，需要重启ES服务才能生效。\n测试分词效果\n​ POST _analyze { \u0026ldquo;analyzer\u0026rdquo;:\u0026ldquo;icu_analyzer\u0026rdquo;, \u0026ldquo;text\u0026rdquo;:\u0026ldquo;中华人民共和国\u0026rdquo; }\n​ 离线安装\n本地下载相应的插件，解压，然后手动上传到elasticsearch的plugins目录，然后重启ES实例就可以了。\n比如ik中文分词插件：https://github.com/medcl/elasticsearch-analysis-ik\nelasticsearch-analysis-ik-7.15.2.zip\n必须对应es版本\n测试分词效果\n#ES的默认分词设置是standard，会单字拆分 POST _analyze { \u0026#34;analyzer\u0026#34;:\u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;中华人民共和国\u0026#34; } #ik_smart:会做最粗粒度的拆 POST _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;中华人民共和国\u0026#34; } #ik_max_word:会将文本做最细粒度的拆分 POST _analyze { \u0026#34;analyzer\u0026#34;:\u0026#34;ik_max_word\u0026#34;, \u0026#34;text\u0026#34;:\u0026#34;中华人民共和国\u0026#34; } ​\n创建索引时可以指定IK分词器作为默认分词器\nPUT /es_db { \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;analysis.analyzer.default.type\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } ​ ElasticSearch基本概念 关系型数据库 VS ElasticSearch 在7.0之前，一个 Index可以设置多个Types\n目前Type已经被Deprecated，7.0开始，一个索引只能创建一个Type - “_doc”\n传统关系型数据库和Elasticsearch的区别:\nElasticsearch- Schemaless /相关性/高性能全文检索 RDMS —事务性/ Join ​ 索引（Index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。\n一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。\n​ 文档（Document） Elasticsearch是面向文档的，文档是所有可搜索数据的最小单位。\n日志文件中的日志项 一本电影的具体信息/一张唱片的详细信息 MP3播放器里的一首歌/一篇PDF文档中的具体内容 文档会被序列化成JSON格式，保存在Elasticsearch中\nJSON对象由字段组成 每个字段都有对应的字段类型(字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID\n可以自己指定ID或者通过Elasticsearch自动生成 一篇文档包含了一系列字段，类似数据库表中的一条记录\nJSON文档，格式灵活，不需要预先定义格式\n字段的类型可以指定或者通过Elasticsearch自动推算 支持数组/支持嵌套 文档元数据\n​ 元数据，用于标注文档的相关信息：\n_index：文档所属的索引名 _type：文档所属的类型名 _id：文档唯—ld _source: 文档的原始Json数据 _version: 文档的版本号，修改删除操作_version都会自增1 _seq_no: 和_version一样，一旦数据发生更改，数据也一直是累计的。Shard级别严格递增，保证后写入的Doc的_seq_no大于先写入的Doc的_seq_no。 _primary_term: _primary_term主要是用来恢复数据时处理当多个文档的_seq_no一样时的冲突，避免Primary Shard上的写入被覆盖。每当Primary Shard发生重新分配时，比如重启，Primary选举等，_primary_term会递增1。 ElasticSearch索引操作 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index.html\n创建索引 索引命名必须小写，不能以下划线开头\n格式: PUT /索引名称\n​ #创建索引 PUT /es_db #创建索引时可以设置分片数和副本数 PUT /es_db { \u0026ldquo;settings\u0026rdquo; : { \u0026ldquo;number_of_shards\u0026rdquo; : 3, \u0026ldquo;number_of_replicas\u0026rdquo; : 2 } } #修改索引配置 PUT /es_db/_settings { \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;number_of_replicas\u0026rdquo; : 1 } }\n​ 查询索引 格式: GET /索引名称\n​ #查询索引 GET /es_db #es_db是否存在 HEAD /es_db\n​ ​\n删除索引 格式: DELETE /索引名称\n​ DELETE /es_db\nElasticSearch文档操作 示例数据\nPUT /es_db { \u0026#34;settings\u0026#34; : { \u0026#34;index\u0026#34; : { \u0026#34;analysis.analyzer.default.type\u0026#34;: \u0026#34;ik_max_word\u0026#34; } } } PUT /es_db/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; } PUT /es_db/_doc/2 { \u0026#34;name\u0026#34;: \u0026#34;李四\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 28, \u0026#34;address\u0026#34;: \u0026#34;广州荔湾大厦\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java assistant\u0026#34; } PUT /es_db/_doc/3 { \u0026#34;name\u0026#34;: \u0026#34;王五\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 26, \u0026#34;address\u0026#34;: \u0026#34;广州白云山公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;php developer\u0026#34; } PUT /es_db/_doc/4 { \u0026#34;name\u0026#34;: \u0026#34;赵六\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 22, \u0026#34;address\u0026#34;: \u0026#34;长沙橘子洲\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;python assistant\u0026#34; } PUT /es_db/_doc/5 { \u0026#34;name\u0026#34;: \u0026#34;张龙\u0026#34;, \u0026#34;sex\u0026#34;: 0, \u0026#34;age\u0026#34;: 19, \u0026#34;address\u0026#34;: \u0026#34;长沙麓谷企业广场\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java architect assistant\u0026#34; }\tPUT /es_db/_doc/6 { \u0026#34;name\u0026#34;: \u0026#34;赵虎\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 32, \u0026#34;address\u0026#34;: \u0026#34;长沙麓谷兴工国际产业园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java architect\u0026#34; }\t​\n添加（索引）文档 格式: [PUT | POST] /索引名称/[_doc | _create ]/id ​\n# 创建文档,指定id # 如果id不存在，创建新的文档，否则先删除现有文档，再创建新的文档，版本会增加 PUT /es_db/_doc/1 { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; }\t#创建文档，ES生成id POST /es_db/_doc { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25, \u0026#34;address\u0026#34;: \u0026#34;广州天河公园\u0026#34;, \u0026#34;remark\u0026#34;: \u0026#34;java developer\u0026#34; } ​\n​ 注意:POST和PUT都能起到创建/更新的作用，PUT需要对一个具体的资源进行操作也就是要确定id才能进行更新/创建，而POST是可以针对整个资源集合进行操作的，如果不写id就由ES生成一个唯一id进行创建新文档，如果填了id那就针对这个id的文档进行创建/更新\n​ Create -如果ID已经存在，会失败\n​ 修改文档 全量更新，整个json都会替换，格式: [PUT | POST] /索引名称/_doc/id 如果文档存在，现有文档会被删除，新的文档会被索引\n​\n# 全量更新，替换整个json PUT /es_db/_doc/1/ { \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;sex\u0026#34;: 1, \u0026#34;age\u0026#34;: 25 } #查询文档 GET /es_db/_doc/1 ​\n​ 使用_update部分更新，格式: POST /索引名称/_update/id update不会删除原来的文档，而是实现真正的数据更新\n​\n# 部分更新：在原有文档上更新 # Update -文档必须已经存在，更新只会对相应字段做增量修改 POST /es_db/_update/1 { \u0026#34;doc\u0026#34;: { \u0026#34;age\u0026#34;: 28 } } #查询文档 GET /es_db/_doc/1 ​\n​ 使用 _update_by_query 更新文档 ​\nPOST /es_db/_update_by_query { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;_id\u0026#34;: 1 } }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;ctx._source.age = 30\u0026#34; } } ​\n​ 并发场景下修改文档 _seq_no和_primary_term是对_version的优化，7.X版本的ES默认使用这种方式控制版本，所以当在高并发环境下使用乐观锁机制修改文档时，要带上当前文档的_seq_no和_primary_term进行更新：\n​\nPOST /es_db/_doc/2?if_seq_no=21\u0026amp;if_primary_term=6 { \u0026#34;name\u0026#34;: \u0026#34;李四xxx\u0026#34; } 如果版本号不对，会抛出版本冲突异常，如下图：\n​ 查询文档 根据id查询文档，格式: GET /索引名称/_doc/id GET /es_db/_doc/1 条件查询 _search，格式： /索引名称/_doc/_search # 查询前10条文档 GET /es_db/_doc/_search ​\nES Search API提供了两种条件查询搜索方式：\nREST风格的请求URI，直接将参数带过去 封装到request body中，这种方式可以定义更加易读的JSON格式 ​\n#通过URI搜索，使用“q”指定查询字符串，“query string syntax” KV键值对 #条件查询, 如要查询age等于28岁的 _search?q=*:*** GET /es_db/_doc/_search?q=age:28 #范围查询, 如要查询age在25至26岁之间的 _search?q=***[** TO **] 注意: TO 必须为大写 GET /es_db/_doc/_search?q=age[25 TO 26] #查询年龄小于等于28岁的 :\u0026lt;= GET /es_db/_doc/_search?q=age:\u0026lt;=28 #查询年龄大于28前的 :\u0026gt; GET /es_db/_doc/_search?q=age:\u0026gt;28 #分页查询 from=*\u0026amp;size=* GET /es_db/_doc/_search?q=age[25 TO 26]\u0026amp;from=0\u0026amp;size=1 #对查询结果只输出某些字段 _source=字段,字段 GET /es_db/_doc/_search?_source=name,age #对查询结果排序 sort=字段:desc/asc GET /es_db/_doc/_search?sort=age:desc 通过请求体的搜索方式会在后面课程详细讲解（DSL）\n​\nGET /es_db/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;广州白云\u0026#34; } } } 删除文档 格式: DELETE /索引名称/_doc/id\nDELETE /es_db/_doc/1 ElasticSearch文档批量操作 批量操作可以减少网络连接所产生的开销，提升性能\n支持在一次API调用中，对不同的索引进行操作 可以在URI中指定Index，也可以在请求的Payload中进行 操作中单条操作失败，并不会影响其他操作 返回结果包括了每一条操作执行的结果 批量写入 批量对文档进行写操作是通过_bulk的API来实现的\n请求方式：POST\n请求地址：_bulk\n请求参数：通过_bulk操作文档，一般至少有两行参数(或偶数行参数)\n第一行参数为指定操作的类型及操作的对象(index,type和id) 第二行参数才是操作的数据 参数类似于：\n{\u0026#34;actionName\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;indexName\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;typeName\u0026#34;,\u0026#34;_id\u0026#34;:\u0026#34;id\u0026#34;}} {\u0026#34;field1\u0026#34;:\u0026#34;value1\u0026#34;, \u0026#34;field2\u0026#34;:\u0026#34;value2\u0026#34;} actionName：表示操作类型，主要有create,index,delete和update 批量创建文档create\nPOST _bulk {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;fox老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;fox老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;id\u0026#34;:4,\u0026#34;title\u0026#34;:\u0026#34;mark老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;mark老师NB\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} 普通创建或全量替换index\nPOST _bulk {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;图灵徐庶老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;图灵学院徐庶老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;index\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;id\u0026#34;:4,\u0026#34;title\u0026#34;:\u0026#34;图灵诸葛老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;图灵学院诸葛老师NB\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} 如果原文档不存在，则是创建 如果原文档存在，则是替换(全量修改原文档) 批量删除delete\n​\nPOST _bulk {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} 批量修改update\n​\nPOST _bulk {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;doc\u0026#34;:{\u0026#34;title\u0026#34;:\u0026#34;ES大法必修内功\u0026#34;}} {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;doc\u0026#34;:{\u0026#34;create_time\u0026#34;:1554018421008}} 组合应用\n​\nPOST _bulk {\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;id\u0026#34;:3,\u0026#34;title\u0026#34;:\u0026#34;fox老师\u0026#34;,\u0026#34;content\u0026#34;:\u0026#34;fox老师666\u0026#34;,\u0026#34;tags\u0026#34;:[\u0026#34;java\u0026#34;, \u0026#34;面向对象\u0026#34;],\u0026#34;create_time\u0026#34;:1554015482530} {\u0026#34;delete\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:3}} {\u0026#34;update\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;article\u0026#34;, \u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;:4}} {\u0026#34;doc\u0026#34;:{\u0026#34;create_time\u0026#34;:1554018421008}} 批量读取 es的批量查询可以使用mget和msearch两种。其中mget是需要我们知道它的id，可以指定不同的index，也可以指定返回值source。msearch可以通过字段查询来进行一个批量的查找。\n_mget\n​\n#可以通过ID批量获取不同index和type的数据 GET _mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;es_db\u0026#34;, \u0026#34;_id\u0026#34;: 1 }, { \u0026#34;_index\u0026#34;: \u0026#34;article\u0026#34;, \u0026#34;_id\u0026#34;: 4 } ] } #可以通过ID批量获取es_db的数据 GET /es_db/_mget { \u0026#34;docs\u0026#34;: [ { \u0026#34;_id\u0026#34;: 1 }, { \u0026#34;_id\u0026#34;: 4 } ] } #简化后 GET /es_db/_mget { \u0026#34;ids\u0026#34;:[\u0026#34;1\u0026#34;,\u0026#34;2\u0026#34;] } ​\n​ _msearch\n在_msearch中，请求格式和bulk类似。查询一条数据需要两个对象，第一个设置index和type，第二个设置查询语句。查询语句和search相同。如果只是查询一个index，我们可以在url中带上index，这样，如果查该index可以直接用空对象表示。\n​\nGET /es_db/_msearch {} {\u0026#34;query\u0026#34; : {\u0026#34;match_all\u0026#34; : {}}, \u0026#34;from\u0026#34; : 0, \u0026#34;size\u0026#34; : 2} {\u0026#34;index\u0026#34; : \u0026#34;article\u0026#34;} {\u0026#34;query\u0026#34; : {\u0026#34;match_all\u0026#34; : {}}} ​ Logstash与FileBeat详解以及ELK整合 链接：http://note.youdao.com/noteshare?id=cd88d72a1c76d18efcf7fe767e8c2d20\u0026amp;sub=D7819084A43243FFA52E8A8741795414\n背景 日志管理的挑战：\n关注点很多，任何一个点都有可能引起问题 日志分散在很多机器，出了问题时，才发现日志被删了 很多运维人员是消防员，哪里有问题去哪里 ​ 集中化日志管理思路：\n日志收集 ——》格式化分析 ——》检索和可视化 ——》 风险告警\nELK架构 ELK架构分为两种，一种是经典的ELK，另外一种是加上消息队列（Redis或Kafka或RabbitMQ）和Nginx结构。\n经典的ELK 经典的ELK主要是由Filebeat + Logstash + Elasticsearch + Kibana组成，如下图：（早期的ELK只有Logstash + Elasticsearch + Kibana）\n​ 此架构主要适用于数据量小的开发环境，存在数据丢失的危险。\n整合消息队列+Nginx架构 这种架构，主要加上了Redis或Kafka或RabbitMQ做消息队列，保证了消息的不丢失。\n​ 此种架构，主要用在生产环境，可以处理大数据量，并且不会丢失数据。\n什么是Logstash Logstash 是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的存储库中。\nhttps://www.elastic.co/cn/logstash/\n应用：ETL工具 / 数据采集处理引擎\n​ Logstash核心概念 Pipeline\n包含了input—filter-output三个阶段的处理流程 插件生命周期管理 队列管理 Logstash Event\n数据在内部流转时的具体表现形式。数据在input 阶段被转换为Event，在 output被转化成目标格式数据 Event 其实是一个Java Object，在配置文件中，对Event 的属性进行增删改查 Codec (Code / Decode)\n将原始数据decode成Event;将Event encode成目标数据\n​ Logstash数据传输原理 数据采集与输入：Logstash支持各种输入选择，能够以连续的流式传输方式，轻松地从日志、指标、Web应用以及数据存储中采集数据。 实时解析和数据转换：通过Logstash过滤器解析各个事件，识别已命名的字段来构建结构，并将它们转换成通用格式，最终将数据从源端传输到存储库中。 存储与数据导出：Logstash提供多种输出选择，可以将数据发送到指定的地方。 Logstash通过管道完成数据的采集与处理，管道配置中包含input、output和filter（可选）插件，input和output用来配置输入和输出数据源、filter用来对数据进行过滤或预处理。\n​ Logstash配置文件结构 参考：https://www.elastic.co/guide/en/logstash/7.17/configuration.html\nLogstash的管道配置文件对每种类型的插件都提供了一个单独的配置部分，用于处理管道事件。\ninput { stdin { } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{COMBINEDAPACHELOG}\u0026#34; } } date { match =\u0026gt; [ \u0026#34;timestamp\u0026#34; , \u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34; ] } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;localhost:9200\u0026#34;]} stdout { codec =\u0026gt; rubydebug } } 每个配置部分可以包含一个或多个插件。例如，指定多个filter插件，Logstash会按照它们在配置文件中出现的顺序进行处理。\n#运行 bin/logstash -f logstash-demo.conf Input Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/input-plugins.html\n一个 Pipeline可以有多个input插件\nStdin / File\nBeats / Log4J /Elasticsearch / JDBC / Kafka /Rabbitmq /Redis\nJMX/ HTTP / Websocket / UDP / TCP\nGoogle Cloud Storage / S3\nGithub / Twitter\nOutput Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/output-plugins.html\n将Event发送到特定的目的地，是 Pipeline 的最后一个阶段。\n常见 Output Plugins：\nElasticsearch Email / Pageduty Influxdb / Kafka / Mongodb / Opentsdb / Zabbix Http / TCP / Websocket Filter Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/filter-plugins.html\n处理Event\n内置的Filter Plugins:\nMutate 一操作Event的字段 Metrics — Aggregate metrics Ruby 一执行Ruby 代码 Codec Plugins\nhttps://www.elastic.co/guide/en/logstash/7.17/codec-plugins.html\n将原始数据decode成Event;将Event encode成目标数据\n内置的Codec Plugins:\nLine / Multiline JSON / Avro / Cef (ArcSight Common Event Format) Dots / Rubydebug Logstash Queue In Memory Queue 进程Crash，机器宕机，都会引起数据的丢失\nPersistent Queue 机器宕机，数据也不会丢失; 数据保证会被消费; 可以替代 Kafka等消息队列缓冲区的作用\nqueue.type: persisted (默认是memory) queue.max_bytes: 4gb ​ Logstash安装 logstash官方文档: https://www.elastic.co/guide/en/logstash/7.17/installing-logstash.html\n1）下载并解压logstash 下载地址： https://www.elastic.co/cn/downloads/past-releases#logstash\n选择版本：7.17.3\n​ 2）测试：运行最基本的logstash管道 ​\ncd logstash-7.17.3 #linux #-e选项表示，直接把配置放在命令中，这样可以有效快速进行测试 bin/logstash -e \u0026#39;input { stdin { } } output { stdout {} }\u0026#39; #windows .\\bin\\logstash.bat -e \u0026#34;input { stdin { } } output { stdout {} }\u0026#34; ​\n测试结果：\n​ window版本的logstash-7.17.3的bug:\nwindows出现错误提示could not find java; set JAVA_HOME or ensure java is in PATH\n​ 修改setup.bat\n​ ​ Codec Plugin测试\n# single line bin/logstash -e \u0026#34;input{stdin{codec=\u0026gt;line}}output{stdout{codec=\u0026gt; rubydebug}}\u0026#34; bin/logstash -e \u0026#34;input{stdin{codec=\u0026gt;json}}output{stdout{codec=\u0026gt; rubydebug}}\u0026#34; ​\nCodec Plugin —— Multiline\n设置参数:\npattern: 设置行匹配的正则表达式\nwhat : 如果匹配成功，那么匹配行属于上一个事件还是下一个事件\nprevious / next negate : 是否对pattern结果取反\ntrue / false ​\n# 多行数据，异常 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at com.example.myproject.Book.getTitle(Book.java:16) at com.example.myproject.Author.getBookTitles(Author.java:25) at com.example.myproject.Bootstrap.main(Bootstrap.java:14) # multiline-exception.conf input { stdin { codec =\u0026gt; multiline { pattern =\u0026gt; \u0026#34;^\\s\u0026#34; what =\u0026gt; \u0026#34;previous\u0026#34; } } } filter {} output { stdout { codec =\u0026gt; rubydebug } } #执行管道 bin/logstash -f multiline-exception.conf Input Plugin —— File\n支持从文件中读取数据，如日志文件 文件读取需要解决的问题：只被读取一次。重启后需要从上次读取的位置继续(通过sincedb 实现) 读取到文件新内容，发现新文件 文件发生归档操作(文档位置发生变化，日志rotation)，不能影响当前的内容读取 Filter Plugin\nFilter Plugin可以对Logstash Event进行各种处理，例如解析，删除字段，类型转换\nDate: 日期解析 Dissect: 分割符解析 Grok: 正则匹配解析 Mutate: 处理字段。重命名，删除，替换 Ruby: 利用Ruby 代码来动态修改Event Filter Plugin - Mutate\n对字段做各种操作:\nConvert : 类型转换 Gsub : 字符串替换 Split / Join /Merge: 字符串切割，数组合并字符串，数组合并数组 Rename: 字段重命名 Update / Replace: 字段内容更新替换 Remove_field: 字段删除 Logstash导入数据到ES 1）测试数据集下载：https://grouplens.org/datasets/movielens/\n​ 2）准备logstash-movie.conf配置文件\ninput { file { path =\u0026gt; \u0026#34;/home/es/logstash-7.17.3/dataset/movies.csv\u0026#34; start_position =\u0026gt; \u0026#34;beginning\u0026#34; sincedb_path =\u0026gt; \u0026#34;/dev/null\u0026#34; } } filter { csv { separator =\u0026gt; \u0026#34;,\u0026#34; columns =\u0026gt; [\u0026#34;id\u0026#34;,\u0026#34;content\u0026#34;,\u0026#34;genre\u0026#34;] } mutate { split =\u0026gt; { \u0026#34;genre\u0026#34; =\u0026gt; \u0026#34;|\u0026#34; } remove_field =\u0026gt; [\u0026#34;path\u0026#34;, \u0026#34;host\u0026#34;,\u0026#34;@timestamp\u0026#34;,\u0026#34;message\u0026#34;] } mutate { split =\u0026gt; [\u0026#34;content\u0026#34;, \u0026#34;(\u0026#34;] add_field =\u0026gt; { \u0026#34;title\u0026#34; =\u0026gt; \u0026#34;%{[content][0]}\u0026#34;} add_field =\u0026gt; { \u0026#34;year\u0026#34; =\u0026gt; \u0026#34;%{[content][1]}\u0026#34;} } mutate { convert =\u0026gt; { \u0026#34;year\u0026#34; =\u0026gt; \u0026#34;integer\u0026#34; } strip =\u0026gt; [\u0026#34;title\u0026#34;] remove_field =\u0026gt; [\u0026#34;path\u0026#34;, \u0026#34;host\u0026#34;,\u0026#34;@timestamp\u0026#34;,\u0026#34;message\u0026#34;,\u0026#34;content\u0026#34;] } } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://localhost:9200\u0026#34; index =\u0026gt; \u0026#34;movies\u0026#34; document_id =\u0026gt; \u0026#34;%{id}\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout {} } ​\n3）运行logstash\n​\n# linux bin/logstash -f logstash-movie.conf ​\n同步数据库数据到Elasticsearch 需求: 将数据库中的数据同步到ES，借助ES的全文搜索,提高搜索速度\n需要把新增用户信息同步到Elasticsearch中 用户信息Update 后，需要能被更新到Elasticsearch 支持增量更新 用户注销后，不能被ES所搜索到 实现思路\n基于canal同步数据（项目实战中讲解）\n借助JDBC Input Plugin将数据从数据库读到Logstash\n需要自己提供所需的 JDBC Driver； JDBC Input Plugin 支持定时任务 Scheduling，其语法来自 Rufus-scheduler，其扩展了 Cron，使用 Cron 的语法可以完成任务的触发； JDBC Input Plugin 支持通过 Tracking_column / sql_last_value 的方式记录 State，最终实现增量的更新； https://www.elastic.co/cn/blog/logstash-jdbc-input-plugin JDBC Input Plugin实现步骤\n1）拷贝jdbc依赖到logstash-7.17.3/drivers目录下\n2）准备mysql-demo.conf配置文件\ninput { jdbc { jdbc_driver_library =\u0026gt; \u0026#34;/home/es/logstash-7.17.3/drivers/mysql-connector-java-5.1.49.jar\u0026#34; jdbc_driver_class =\u0026gt; \u0026#34;com.mysql.jdbc.Driver\u0026#34; jdbc_connection_string =\u0026gt; \u0026#34;jdbc:mysql://localhost:3306/test?useSSL=false\u0026#34; jdbc_user =\u0026gt; \u0026#34;root\u0026#34; jdbc_password =\u0026gt; \u0026#34;123456\u0026#34; #启用追踪，如果为true，则需要指定tracking_column use_column_value =\u0026gt; true #指定追踪的字段， tracking_column =\u0026gt; \u0026#34;last_updated\u0026#34; #追踪字段的类型，目前只有数字(numeric)和时间类型(timestamp)，默认是数字类型 tracking_column_type =\u0026gt; \u0026#34;numeric\u0026#34; #记录最后一次运行的结果 record_last_run =\u0026gt; true #上面运行结果的保存位置 last_run_metadata_path =\u0026gt; \u0026#34;jdbc-position.txt\u0026#34; statement =\u0026gt; \u0026#34;SELECT * FROM user where last_updated \u0026gt;:sql_last_value;\u0026#34; schedule =\u0026gt; \u0026#34; * * * * * *\u0026#34; } } output { elasticsearch { document_id =\u0026gt; \u0026#34;%{id}\u0026#34; document_type =\u0026gt; \u0026#34;_doc\u0026#34; index =\u0026gt; \u0026#34;users\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 3）运行logstash\nbin/logstash -f mysql-demo.conf 测试\n#user表 CREATE TABLE `user` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(50) DEFAULT NULL, `address` varchar(50) CHARACTER DEFAULT NULL, `last_updated` bigint DEFAULT NULL, `is_deleted` int DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; #插入数据 INSERT INTO user(name,address,last_updated,is_deleted) VALUES(\u0026#34;张三\u0026#34;,\u0026#34;广州天河\u0026#34;,unix_timestamp(NOW()),0) ​ # 更新 update user set address=\u0026#34;广州白云山\u0026#34;,last_updated=unix_timestamp(NOW()) where name=\u0026#34;张三\u0026#34; ​\n​ #删除 update user set is_deleted=1,last_updated=unix_timestamp(NOW()) where name=\u0026#34;张三\u0026#34; ​ #ES中查询 # 创建 alias，只显示没有被标记 deleted的用户 POST /_aliases { \u0026#34;actions\u0026#34;: [ { \u0026#34;add\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;view_users\u0026#34;, \u0026#34;filter\u0026#34; : { \u0026#34;term\u0026#34; : { \u0026#34;is_deleted\u0026#34; : 0} } } } ] } # 通过 Alias查询，查不到被标记成 deleted的用户 POST view_users/_search POST view_users/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;name.keyword\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;张三\u0026#34; } } } } ​\n什么是Beats 轻量型数据采集器，文档地址： https://www.elastic.co/guide/en/beats/libbeat/7.17/index.html\nBeats 是一个免费且开放的平台，集合了多种单一用途的数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。\n​ FileBeat简介 FileBeat专门用于转发和收集日志数据的轻量级采集工具。它可以作为代理安装在服务器上，FileBeat监视指定路径的日志文件，收集日志数据，并将收集到的日志转发到Elasticsearch或者Logstash。\nFileBeat的工作原理 启动FileBeat时，会启动一个或者多个输入（Input），这些Input监控指定的日志数据位置。FileBeat会针对每一个文件启动一个Harvester（收割机）。Harvester读取每一个文件的日志，将新的日志发送到libbeat，libbeat将数据收集到一起，并将数据发送给输出（Output）。\n​ logstash vs FileBeat Logstash是在jvm上运行的，资源消耗比较大。而FileBeat是基于golang编写的，功能较少但资源消耗也比较小，更轻量级。 Logstash 和Filebeat都具有日志收集功能，Filebeat更轻量，占用资源更少 Logstash 具有Filter功能，能过滤分析日志 一般结构都是Filebeat采集日志，然后发送到消息队列、Redis、MQ中，然后Logstash去获取，利用Filter功能过滤分析，然后存储到Elasticsearch中 FileBeat和Logstash配合，实现背压机制。当将数据发送到Logstash或 Elasticsearch时，Filebeat使用背压敏感协议，以应对更多的数据量。如果Logstash正在忙于处理数据，则会告诉Filebeat 减慢读取速度。一旦拥堵得到解决，Filebeat就会恢复到原来的步伐并继续传输数据。 Filebeat安装 https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html\n1）下载并解压Filebeat\n下载地址：https://www.elastic.co/cn/downloads/past-releases#filebeat\n选择版本：7.17.3\n​ 2）编辑配置\n修改 filebeat.yml 以设置连接信息：\n​\noutput.elasticsearch: hosts: [\u0026#34;192.168.65.174:9200\u0026#34;,\u0026#34;192.168.65.192:9200\u0026#34;,\u0026#34;192.168.65.204:9200\u0026#34;] username: \u0026#34;elastic\u0026#34; password: \u0026#34;123456\u0026#34; setup.kibana: host: \u0026#34;192.168.65.174:5601\u0026#34; ​\n3) 启用和配置数据收集模块\n从安装目录中，运行：\n# 查看可以模块列表 ./filebeat modules list #启用nginx模块 ./filebeat modules enable nginx #如果需要更改nginx日志路径,修改modules.d/nginx.yml - module: nginx access: var.paths: [\u0026#34;/var/log/nginx/access.log*\u0026#34;] #启用 Logstash 模块 ./filebeat modules enable logstash #在 modules.d/logstash.yml 文件中修改设置 - module: logstash log: enabled: true var.paths: [\u0026#34;/home/es/logstash-7.17.3/logs/*.log\u0026#34;] 4）启动 Filebeat\n# setup命令加载Kibana仪表板。 如果仪表板已经设置，则忽略此命令。 ./filebeat setup # 启动Filebeat ./filebeat -e ELK整合实战 案例：采集tomcat服务器日志 Tomcat服务器运行过程中产生很多日志信息，通过Logstash采集并存储日志信息至ElasticSearch中\n使用FileBeats将日志发送到Logstash 1）创建配置文件filebeat-logstash.yml，配置FileBeats将数据发送到Logstash\nvim filebeat-logstash.yml chmod 644 filebeat-logstash.yml #因为Tomcat的web log日志都是以IP地址开头的，所以我们需要修改下匹配字段。 # 不以ip地址开头的行追加到上一行 filebeat.inputs: - type: log enabled: true paths: - /home/es/apache-tomcat-8.5.33/logs/*access*.* multiline.pattern: \u0026#39;^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+ \u0026#39; multiline.negate: true multiline.match: after output.logstash: enabled: true hosts: [\u0026#34;192.168.65.204:5044\u0026#34;] ​\npattern：正则表达式 negate：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行 match：after 或 before，合并到上一行的末尾或开头 2）启动FileBeat，并指定使用指定的配置文件\n./filebeat -e -c filebeat-logstash.yml 可能出现的异常：\n异常1：Exiting: error loading config file: config file (\u0026ldquo;filebeat-logstash.yml\u0026rdquo;) can only be writable by the owner but the permissions are \u0026ldquo;-rw-rw-r\u0026ndash;\u0026rdquo; (to fix the permissions use: \u0026lsquo;chmod go-w /home/es/filebeat-7.17.3-linux-x86_64/filebeat-logstash.yml\u0026rsquo;)\n因为安全原因不要其他用户写的权限，去掉写的权限就可以了\n​ chmod 644 filebeat-logstash.yml\n异常2：Failed to connect to backoff(async(tcp://192.168.65.204:5044)): dial tcp 192.168.65.204:5044: connect: connection refused\nFileBeat将尝试建立与Logstash监听的IP和端口号进行连接。但此时，我们并没有开启并配置Logstash，所以FileBeat是无法连接到Logstash的。\n配置Logstash接收FileBeat收集的数据并打印 vim config/filebeat-console.conf # 配置从FileBeat接收数据 input { beats { port =\u0026gt; 5044 } } output { stdout { codec =\u0026gt; rubydebug } } 测试logstash配置是否正确\nbin/logstash -f config/filebeat-console.conf --config.test_and_exit 启动logstash\n# reload.automatic：修改配置文件时自动重新加载 bin/logstash -f config/filebeat-console.conf --config.reload.automatic ​\n测试访问tomcat，logstash是否接收到了Filebeat传过来的tomcat日志\nLogstash输出数据到Elasticsearch 如果我们需要将数据输出值ES而不是控制台的话，我们修改Logstash的output配置。\nvim config/filebeat-elasticSearch.conf input { beats { port =\u0026gt; 5044 } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 启动logstash\nbin/logstash -f config/filebeat-elasticSearch.conf --config.reload.automatic ​\nES中会生成一个以logstash开头的索引，测试日志是否保存到了ES。\n思考：日志信息都保证在message字段中，是否可以把日志进行解析一个个的字段？例如：IP字段、时间、请求方式、请求URL、响应结果。\n利用Logstash过滤器解析日志 从日志文件中收集到的数据包含了很多有效信息，比如IP、时间等，在Logstash中可以配置过滤器Filter对采集到的数据进行过滤处理，Logstash中有大量的插件可以供我们使用。\n#查看Logstash已经安装的插件 bin/logstash-plugin list Grok插件\nGrok是一种将非结构化日志解析为结构化的插件。这个工具非常适合用来解析系统日志、Web服务器日志、MySQL或者是任意其他的日志格式。\nhttps://www.elastic.co/guide/en/logstash/7.17/plugins-filters-grok.html\nGrok语法\nGrok是通过模式匹配的方式来识别日志中的数据,可以把Grok插件简单理解为升级版本的正则表达式。它拥有更多的模式，默认Logstash拥有120个模式。如果这些模式不满足我们解析日志的需求，我们可以直接使用正则表达式来进行匹配。\ngrok模式的语法是：\n%{SYNTAX:SEMANTIC} SYNTAX（语法）指的是Grok模式名称，SEMANTIC（语义）是给模式匹配到的文本字段名。例如：\n%{NUMBER:duration} %{IP:client} duration表示：匹配一个数字，client表示匹配一个IP地址。 默认在Grok中，所有匹配到的的数据类型都是字符串，如果要转换成int类型（目前只支持int和float），可以这样：%{NUMBER:duration:int} %{IP:client}\n常用的Grok模式\nhttps://help.aliyun.com/document_detail/129387.html?scm=20140722.184.2.173\n用法\nfilter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026#34; } } } 比如，tomacat日志\n192.168.65.103 - - [23/Jun/2022:22:37:23 +0800] \u0026#34;GET /docs/images/docs-stylesheet.css HTTP/1.1\u0026#34; 200 5780 解析后的字段\n字段名 说明 client IP 浏览器端IP timestamp 请求的时间戳 method 请求方式（GET/POST） uri 请求的链接地址 status 服务器端响应状态 length 响应的数据长度 grok模式\n​\n%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status} %{INT:length} ​\n为了方便测试，我们可以使用Kibana来进行Grok开发：\n​ 修改Logstash配置文件\nvim config/filebeat-console.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status:int} %{INT:length:int}\u0026#34; } } } output { stdout { codec =\u0026gt; rubydebug } } 启动logstash测试\nbin/logstash -f config/filebeat-console.conf --config.reload.automatic 使用mutate插件过滤掉不需要的字段\nmutate { enable_metric =\u0026gt; \u0026#34;false\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;, \u0026#34;log\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;input\u0026#34;, \u0026#34;agent\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;@version\u0026#34;] } 要将日期格式进行转换，我们可以使用Date插件来实现。该插件专门用来解析字段中的日期，官方说明文档：https://www.elastic.co/guide/en/logstash/7.17/plugins-filters-date.html\n用法如下：\n​ 将date字段转换为「年月日 时分秒」格式。默认字段经过date插件处理后，会输出到@timestamp字段，所以，我们可以通过修改target属性来重新定义输出字段。\ndate { match =\u0026gt; [\u0026#34;date\u0026#34;,\u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] target =\u0026gt; \u0026#34;date\u0026#34; } ​\n输出到Elasticsearch指定索引 index来指定索引名称，默认输出的index名称为：logstash-%{+yyyy.MM.dd}。但注意，要在index中使用时间格式化，filter的输出必须包含 @timestamp字段，否则将无法解析日期。\noutput { elasticsearch { index =\u0026gt; \u0026#34;tomcat_web_log_%{+YYYY-MM}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } stdout{ codec =\u0026gt; rubydebug } } 注意：index名称中，不能出现大写字符\n完整的Logstash配置文件\nvim config/filebeat-filter-es.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IP:ip} - - \\[%{HTTPDATE:date}\\] \\\u0026#34;%{WORD:method} %{PATH:uri} %{DATA:protocol}\\\u0026#34; %{INT:status:int} %{INT:length:int}\u0026#34; } } mutate { enable_metric =\u0026gt; \u0026#34;false\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;, \u0026#34;log\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;input\u0026#34;, \u0026#34;agent\u0026#34;, \u0026#34;host\u0026#34;, \u0026#34;ecs\u0026#34;, \u0026#34;@version\u0026#34;] } date { match =\u0026gt; [\u0026#34;date\u0026#34;,\u0026#34;dd/MMM/yyyy:HH:mm:ss Z\u0026#34;,\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;] target =\u0026gt; \u0026#34;date\u0026#34; } } output { stdout { codec =\u0026gt; rubydebug } elasticsearch { index =\u0026gt; \u0026#34;tomcat_web_log_%{+YYYY-MM}\u0026#34; hosts =\u0026gt; [\u0026#34;http://localhost:9200\u0026#34;] user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;123456\u0026#34; } } 启动logstash\nbin/logstash -f config/filebeat-filter-es.conf --config.reload.automatic input { redis { host=\u0026gt; \u0026#34;localhost\u0026#34; port =\u0026gt; \u0026#34;6379\u0026#34; password =\u0026gt; \u0026#34;9ed99d6b\u0026#34; key =\u0026gt; \u0026#34;filebeat\u0026#34; type =\u0026gt; \u0026#34;redis-input\u0026#34; data_type =\u0026gt; \u0026#34;list\u0026#34; threads =\u0026gt;4 batch_count =\u0026gt; 10 db =\u0026gt; 0 } } filter { } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#34; index =\u0026gt; \u0026#34;netlog\u0026#34; user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; } } logstash 问题处理\nhttps://blog.csdn.net/King_weng/article/details/106506996\nELFK整合实战2 filebeat =\u0026gt; redis =\u0026gt; logstash =\u0026gt; elasticsearch =\u0026gt; kinbana\nfilebeat的配置 filebeat.yml\nfilebeat.inputs: - type: filestream paths: - /var/log/data/10.42.76.202/*.log - /var/log/data/10.42.76.201/*.log - /var/log/data/10.42.76.204/*.log encoding: gbk # 对非utf-8的数据进行转码 ignore_older: 5m #只采集5分钟内更新的文件 - type: filestream paths: - /var/log/data/10.42.76.206/*.log - /var/log/data/10.42.76.207/*.log ignore_older: 5m output.redis: hosts: [\u0026#34;10.43.152.65:6379\u0026#34;] password: \u0026#34;9ed99d6b\u0026#34; key: \u0026#34;filebeat\u0026#34; db: 0 timeout: 5 filebeat 启动命令\nnohup ./filebeat -e -c filebeat.yml \u0026gt;/dev/null \u0026amp; logstash的配置 logstash_run.yml\ninput { redis { host=\u0026gt; \u0026#34;localhost\u0026#34; port =\u0026gt; \u0026#34;6379\u0026#34; password =\u0026gt; \u0026#34;9ed99d6b\u0026#34; key =\u0026gt; \u0026#34;filebeat\u0026#34; # 对应redis中的key名称 type =\u0026gt; \u0026#34;redis-input\u0026#34; data_type =\u0026gt; \u0026#34;list\u0026#34; # key的类型 threads =\u0026gt;4 batch_count =\u0026gt; 10 db =\u0026gt; 0 } } filter { } output { elasticsearch { hosts =\u0026gt; \u0026#34;http://cqzwy-mgmt-log-platform-grc055ce-0.cqzwy-mgmt-log-platform-grc055ce.013497775a1b4580924a00009a20c887.svc.cluster.local:9200\u0026#34; index =\u0026gt; \u0026#34;netlog\u0026#34; # 在es中的索引名称 user =\u0026gt; \u0026#34;elastic\u0026#34; password =\u0026gt; \u0026#34;kuGmFNENeZyYuGkYZ4BU\u0026#34; } } 通过jvm.options文件修改jvm参数\n制作logstash镜像(可选) start.sh启动脚本\n#!/bin/bash # -f 后指定的配置文件需是新的文件 bin/logstash -f config/logstash_run.yml --config.reload.automatic Dockerfile文件\nFROM centos:7 MAINTAINER wandong RUN yum install -y wget java-1.8.0-openjdk curl unzip iproute net-tools \u0026amp;\u0026amp; \\ yum clean all \u0026amp;\u0026amp; \\ rm -rf /var/cache/yum/* ADD logstash-7.15.2-linux-x86_64.tar.gz /opt/ WORKDIR /opt/logstash-7.15.2 COPY start.sh /opt/logstash-7.15.2 EXPOSE 5044 9600 CMD [\u0026#34;sh\u0026#34;,\u0026#34;start.sh\u0026#34;] log.file.path : 10.42.76.201 and message : 39.144.219.132\n","permalink":"https://wandong1.github.io/post/elasticsearch%E5%85%A5%E9%97%A8/","summary":"ES版本： v7.17.3\nES环境搭建视频：https://pan.baidu.com/s/1PsTNbpDy\u0026ndash;M-pvFWb3aehQ?pwd=nwxl\nElasticSearch快速入门实战 note 链接：http://note.youdao.com/noteshare?id=d5d5718ae542f274ba0fda4284a53231\u0026amp;sub=68E590656C7A48858C7F6997D4A1511A\n全文检索 数据分类：\n结构化数据： 固定格式，有限长度 比如mysql存的数据 非结构化数据：不定长，无固定格式 比如邮件，word文档，日志 半结构化数据： 前两者结合 比如xml，html 搜索分类：\n结构化数据搜索： 使用关系型数据库\n非结构化数据搜索\n顺序扫描 全文检索 设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗\nname content author 静夜思 床前明月光,疑是地上霜。举头望明月，低头思故乡。 李白 望庐山瀑布 日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺,疑是银河落九天。 李白 \u0026hellip; \u0026hellip; \u0026hellip; 思考：用传统关系型数据库和ES 实现会有什么差别？\n如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询\n​ select name from poems where content like \u0026ldquo;%前%\u0026rdquo;\n这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD\u0026quot;这样的关键词时，通常还希望看到\u0026quot;A\u0026quot;,\u0026ldquo;AB\u0026rdquo;,\u0026ldquo;CD\u0026rdquo;,“ABC”的搜索结果。\n什么是全文检索 全文检索是指：\n通过一个程序扫描文本中的每一个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现的次数 用户查询时，通过之前建立好的索引来查询，将索引中单词对应的文本位置、出现的次数返回给用户，因为有了具体文本的位置，所以就可以将具体内容读取出来了 ​ 搜索原理简单概括的话可以分为这么几步：\n内容爬取，停顿词过滤比如一些无用的像\u0026quot;的\u0026quot;，“了”之类的语气词/连接词 内容分词，提取关键词 根据关键词建立倒排索引 用户输入关键词进行搜索 倒排索引 索引就类似于目录，平时我们使用的都是索引，都是通过主键定位到某条数据，那么倒排索引呢，刚好相反，数据对应到主键。\n​ 这里以一个博客文章的内容为例:\n正排索引（正向索引）\n文章ID 文章标题 文章内容 1 浅析JAVA设计模式 JAVA设计模式是每一个JAVA程序员都应该掌握的进阶知识 2 JAVA多线程设计模式 JAVA多线程与设计模式结合 倒排索引（反向索引）","title":"离线安装docker"},{"content":"离线安装docker https://download.docker.com/linux/static/stable/x86_64/docker-20.10.14.tgz\n#解压 tar -xvzf docker-20.10.14.tgz -C /opt/ chown root:root -R /opt/docker/ cp /opt/docker/* /usr/bin cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP \\$MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target EOF chmod +x /etc/systemd/system/docker.service # 加载service配置 systemctl daemon-reload #设置开机启动 并立即启动 systemctl enable docker.service --now ","permalink":"https://wandong1.github.io/post/%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85docker/","summary":"离线安装docker https://download.docker.com/linux/static/stable/x86_64/docker-20.10.14.tgz\n#解压 tar -xvzf docker-20.10.14.tgz -C /opt/ chown root:root -R /opt/docker/ cp /opt/docker/* /usr/bin cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd ExecReload=/bin/kill -s HUP \\$MAINPID LimitNOFILE=infinity LimitNPROC=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target EOF chmod +x /etc/systemd/system/docker.service # 加载service配置 systemctl daemon-reload #设置开机启动 并立即启动 systemctl enable docker.service --now ","title":"离线安装docker"},{"content":"随笔 技术的变革，一定是思想先行，云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。\n符合云原生架构的应用程序应该是：采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。\n云原生的四要素 微服务：几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用，微服务有理论基础，那就是康威定律，指导服务怎么切分，很玄乎，凡是能称为理论定律的都简单明白不了，不然就忒没b格，大概意思是组织架构决定产品形态，不知道跟马克思的生产关系影响生产力有无关系。\n微服务架构的好处就是按function切了之后，服务解耦，内聚更强，变更更易；另一个划分服务的技巧据说是依据DDD来搞。\n容器化：Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用，是基于LXC技术搞的，容器化为微服务提供实施保障，起到应用隔离作用，K8S是容器编排系统，用于容器管理，容器间的负载均衡，谷歌搞的，Docker和K8S都采用Go编写，都是好东西。\nDevOps：这是个组合词，Dev+Ops，就是开发和运维合体，不像开发和产品，经常刀刃相见，实际上DevOps应该还包括测试，DevOps是一个敏捷思维，是一个沟通文化，也是组织形式，为云原生提供持续交付能力。\n持续交付：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。\n如何云原生？ 首先，云原生借了云计算的东风，没有云计算，自然没有云原生，云计算是云原生的基础。\n随着虚拟化技术的成熟和分布式框架的普及，在容器技术、可持续交付、编排系统等开源社区的推动下，以及微服务等开发理念的带动下，应用上云已经是不可逆转的趋势。\n云计算的3层划分，即基础设施即服务(IaaS)、平台即服务(PaaS)、软件即服务(SaaS)为云原生提供了技术基础和方向指引，真正的云化不仅仅是基础设施和平台的变化，应用也需要做出改变，摈弃传统的土方法，在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点，重新设计，从而建设全新的云化的应用，即云原生应用。\n1.本地部署的传统应用往往采用c/c++、企业级java编写，而云原生应用则需要用以网络为中心的go、node.js等新兴语言编写。\n2.本地部署的传统应用可能需要停机更新，而云原生应用应该始终是最新的，需要支持频繁变更，持续交付，蓝绿部署。\n3.本地部署的传统应用无法动态扩展，往往需要冗余资源以抵抗流量高峰，而云原生应用利用云的弹性自动伸缩，通过共享降本增效。\n4.本地部署的传统应用对网络资源，比如ip、端口等有依赖，甚至是硬编码，而云原生应用对网络和存储都没有这种限制。\n5.本地部署的传统应用通常人肉部署手工运维，而云原生应用这一切都是自动化的。\n6.本地部署的传统应用通常依赖系统环境，而云原生应用不会硬连接到任何系统环境，而是依赖抽象的基础架构，从而获得良好移植性。\n7.本地部署的传统应用有些是单体(巨石)应用，或者强依赖，而基于微服务架构的云原生应用，纵向划分服务，模块化更合理。\n可见，要转向云原生应用需要以新的云原生方法开展工作，云原生包括很多方面：基础架构服务、虚拟化、容器化、容器编排、微服务。幸运的是，开源社区在云原生应用方面做出了大量卓有成效的工作，很多开源的框架和设施可以通过拿来主义直接用，2013年Docker推出并很快成为容器事实标准，随后围绕容器编排的混战中，2017年诞生的k8s很快脱颖而出，而这些技术极大的降低了开发云原生应用的技术门槛。\n","permalink":"https://wandong1.github.io/post/%E9%9A%8F%E7%AC%94/","summary":"随笔 技术的变革，一定是思想先行，云原生是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。\n符合云原生架构的应用程序应该是：采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。\n云原生的四要素 微服务：几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用，微服务有理论基础，那就是康威定律，指导服务怎么切分，很玄乎，凡是能称为理论定律的都简单明白不了，不然就忒没b格，大概意思是组织架构决定产品形态，不知道跟马克思的生产关系影响生产力有无关系。\n微服务架构的好处就是按function切了之后，服务解耦，内聚更强，变更更易；另一个划分服务的技巧据说是依据DDD来搞。\n容器化：Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用，是基于LXC技术搞的，容器化为微服务提供实施保障，起到应用隔离作用，K8S是容器编排系统，用于容器管理，容器间的负载均衡，谷歌搞的，Docker和K8S都采用Go编写，都是好东西。\nDevOps：这是个组合词，Dev+Ops，就是开发和运维合体，不像开发和产品，经常刀刃相见，实际上DevOps应该还包括测试，DevOps是一个敏捷思维，是一个沟通文化，也是组织形式，为云原生提供持续交付能力。\n持续交付：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。\n如何云原生？ 首先，云原生借了云计算的东风，没有云计算，自然没有云原生，云计算是云原生的基础。\n随着虚拟化技术的成熟和分布式框架的普及，在容器技术、可持续交付、编排系统等开源社区的推动下，以及微服务等开发理念的带动下，应用上云已经是不可逆转的趋势。\n云计算的3层划分，即基础设施即服务(IaaS)、平台即服务(PaaS)、软件即服务(SaaS)为云原生提供了技术基础和方向指引，真正的云化不仅仅是基础设施和平台的变化，应用也需要做出改变，摈弃传统的土方法，在架构设计、开发方式、部署维护等各个阶段和方面都基于云的特点，重新设计，从而建设全新的云化的应用，即云原生应用。\n1.本地部署的传统应用往往采用c/c++、企业级java编写，而云原生应用则需要用以网络为中心的go、node.js等新兴语言编写。\n2.本地部署的传统应用可能需要停机更新，而云原生应用应该始终是最新的，需要支持频繁变更，持续交付，蓝绿部署。\n3.本地部署的传统应用无法动态扩展，往往需要冗余资源以抵抗流量高峰，而云原生应用利用云的弹性自动伸缩，通过共享降本增效。\n4.本地部署的传统应用对网络资源，比如ip、端口等有依赖，甚至是硬编码，而云原生应用对网络和存储都没有这种限制。\n5.本地部署的传统应用通常人肉部署手工运维，而云原生应用这一切都是自动化的。\n6.本地部署的传统应用通常依赖系统环境，而云原生应用不会硬连接到任何系统环境，而是依赖抽象的基础架构，从而获得良好移植性。\n7.本地部署的传统应用有些是单体(巨石)应用，或者强依赖，而基于微服务架构的云原生应用，纵向划分服务，模块化更合理。\n可见，要转向云原生应用需要以新的云原生方法开展工作，云原生包括很多方面：基础架构服务、虚拟化、容器化、容器编排、微服务。幸运的是，开源社区在云原生应用方面做出了大量卓有成效的工作，很多开源的框架和设施可以通过拿来主义直接用，2013年Docker推出并很快成为容器事实标准，随后围绕容器编排的混战中，2017年诞生的k8s很快脱颖而出，而这些技术极大的降低了开发云原生应用的技术门槛。","title":"随笔"},{"content":"基于Docker和Kubernetes的企业级DevOps实践训练营 课程准备 离线镜像包\n百度：https://pan.baidu.com/s/1N1AYGCYftYGn6L0QPMWIMw 提取码：ev2h\n天翼云：https://cloud.189.cn/t/ENjUbmRR7FNz\nCentOS7.4版本以上 虚拟机3台（4C+8G+50G），内网互通，可连外网\n课件文档\n《训练营课件》 《安装手册》 git仓库\nhttps://gitee.com/agagin/python-demo.git python demo项目\nhttps://gitee.com/agagin/demo-resources.git demo项目演示需要的资源文件\n关于本人 李永信\n2012-2017，云平台开发工程师，先后对接过Vmware、OpenStack、Docker平台\n2017-2019， 运维开发工程师，Docker+Kubernetes的Paas平台运维开发\n2019至今，DevOps工程师\n8年多的时间，积攒了一定的开发和运维经验，跟大家分享。\n课程安排 2020.4.11 Docker + kubernetes\n2020.4.18 DevOps平台实践\n2天的时间，节奏会相对快一些\n小调研：\nA : 只听过docker，几乎没有docker的使用经验 B：有一定的docker实践经验，不熟悉或者几乎没用过k8s C：对于docker和k8s都有一定的实践经验，想更多了解如何基于docker+k8s构建devops平台 D：其他 课程介绍 最近的三年多时间，关注容器圈的话应该会知道这么几个事情：\n容器技术持续火爆\nKubernetes(k8s)成为容器编排管理的标准\n国内外厂商均已开始了全面拥抱Kubernetes的转型， 无数中小型企业已经落地 Kubernetes，或正走落地的道路上 。基于目前的发展趋势可以预见，未来几年以kubernetes平台为核心的容器运维管理、DevOps等将迎来全面的发展。\n本着实践为核心的思想，本课程使用企业常见的基于Django + uwsgi + Nginx架构的Python Demo项目，分别讲述三个事情：\n项目的容器化\n教大家如何把公司的项目做成容器，并且运行在docker环境中\n使用Kubernetes集群来管理容器化的项目\n带大家一步一步部署k8s集群，并把容器化后的demo项目使用k8s来管理起来\n使用Jenkins和Kubernetes集成，实现demo项目的持续集成/持续交付(CI/CD)\n会使用k8s管理应用生命周期后，还差最后的环节，就是如何把开发、测试、部署的流程使用自动化工具整合起来，最后一部分呢，课程会教会大家如何优雅的使用gitlab+Jenkins+k8s构建企业级的DevOps平台\n流程示意 你将学到哪些 Docker相关\n如何使用Dockerfile快速构建镜像 Docker镜像、容器、仓库的常用操作 Docker容器的网络（Bridge下的SNAT、DNAT） Kubernetes相关\n集群的快速搭建 kubernetes的架构及工作流程 使用Pod控制器管理业务应用的生命周期 使用CoreDNS、Service和Ingress实现服务发现、负载均衡及四层、七层网络的访问 Kubernetes的认证授权体系 使用EFK构建集群业务应用的日志收集系统\n基于Gitlab+Jenkins+k8s构建DevOps平台\nJenkins介绍及流水线的使用 Jenkinsfile及多分支流水线的实际应用 Jenkins集成sonarQube、Docker、Kubernetes 使用groovy编写sharedLibrary，实现CI/CD流程的优化 第一章 走进Docker的世界 介绍docker的前世今生，了解docker的实现原理，以Django项目为例，带大家如何编写最佳的Dockerfile构建镜像。通过本章的学习，大家会知道docker的概念及基本操作，并学会构建自己的业务镜像，并通过抓包的方式掌握Docker最常用的bridge网络模式的通信。\n认识docker 怎么出现的 轻量、高效的虚拟化\nDocker 公司位于旧金山,原名dotCloud，底层利用了Linux容器技术（在操作系统中实现资源隔离与限制）。为了方便创建和管理这些容器，dotCloud 开发了一套内部工具，之后被命名为“Docker”。Docker就是这样诞生的。\n（思考为啥要用Linux容器技术？）\nHypervisor： 一种运行在基础物理服务器和操作系统之间的中间软件层，可允许多个操作系统和应用共享硬件 。常见的VMware的 Workstation 、ESXi、微软的Hyper-V或者思杰的XenServer。\nContainer Runtime：通过Linux内核虚拟化能力管理多个容器，多个容器共享一套操作系统内核。因此摘掉了内核占用的空间及运行所需要的耗时，使得容器极其轻量与快速。\n软件交付过程中的环境依赖\n几个知识点 可以把应用程序代码及运行依赖环境打包成镜像，作为交付介质，在各环境部署\n可以将镜像（image）启动成为容器(container)，并且提供多容器的生命周期进行管理（启、停、删）\ncontainer容器之间相互隔离，且每个容器可以设置资源限额\n提供轻量级虚拟化功能，容器就是在宿主机中的一个个的虚拟的空间，彼此相互隔离，完全独立\nCS架构的软件产品\n版本管理 Docker 引擎主要有两个版本：企业版（EE）和社区版（CE） 每个季度(1-3,4-6,7-9,10-12)，企业版和社区版都会发布一个稳定版本(Stable)。社区版本会提供 4 个月的支持，而企业版本会提供 12 个月的支持 每个月社区版还会通过 Edge 方式发布月度版 从 2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年 6 月第一次发布的社区版本为 18.06.0-ce 发展史 13年成立，15年开始，迎来了飞速发展。\nDocker 1.8之前，使用LXC，Docker在上层做了封装， 把LXC复杂的容器创建与使用方式简化为自己的一套命令体系。\n之后，为了实现跨平台等复杂的场景，Docker抽出了libcontainer项目，把对namespace、cgroup的操作封装在libcontainer项目里，支持不同的平台类型。\n2015年6月，Docker牵头成立了 OCI（Open Container Initiative开放容器计划）组织，这个组织的目的是建立起一个围绕容器的通用标准 。 容器格式标准是一种不受上层结构绑定的协议，即不限于某种特定操作系统、硬件、CPU架构、公有云等 ， 允许任何人在遵循该标准的情况下开发应用容器技术，这使得容器技术有了一个更广阔的发展空间。\nOCI成立后，libcontainer 交给OCI组织来维护，但是libcontainer中只包含了与kernel交互的库，因此基于libcontainer项目，后面又加入了一个CLI工具，并且项目改名为runC (https://github.com/opencontainers/runc )， 目前runC已经成为一个功能强大的runtime工具。\nDocker也做了架构调整。将容器运行时相关的程序从docker daemon剥离出来，形成了containerd。containerd向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化，确保原有接口向下兼容。向下通过containerd-shim结合runC，使得引擎可以独立升级，避免之前Docker Daemon升级会导致所有容器不可用的问题。\n也就是说\nrunC（libcontainer）是符合OCI标准的一个实现，与底层系统交互 containerd是实现了OCI之上的容器的高级功能，比如镜像管理、容器执行的调用等 Dockerd目前是最上层与CLI交互的进程，接收cli的请求并与containerd协作 小结 为了解决软件交付过程中的环境依赖，同时提供一种更加轻量的虚拟化技术，Docker出现了 Docker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理 docker-ce，每季度发布stable版本。18.06，18.09，19.03 发展至今，docker已经通过制定OCI标准对最初的项目做了拆分，其中runC和containerd是docker的核心项目，理解docker整个请求的流程，对我们深入理解docker有很大的帮助 安装 配置宿主机网卡转发 ## 配置网卡转发,看值是否为1 $ sysctl -a |grep -w net.ipv4.ip_forward net.ipv4.ip_forward = 1 ## 若未配置，需要执行如下 $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/docker.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 EOF $ sysctl -p /etc/sysctl.d/docker.conf Yum安装配置docker ## 下载阿里源repo文件 $ curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo $ curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo $ yum clean all \u0026amp;\u0026amp; yum makecache ## yum安装 $ yum install -y docker-ce ## 查看源中可用版本 $ yum list docker-ce --showduplicates | sort -r ## 安装指定版本 ##yum install -y docker-ce-18.09.9 ## 配置源加速 ## https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors mkdir -p /etc/docker vi /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34; : [ \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://dockerhub.azk8s.cn\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://ot2k4d59.mirror.aliyuncs.com/\u0026#34; ] } ## 设置开机自启 systemctl enable docker systemctl daemon-reload ## 启动docker systemctl start docker ## 查看docker信息 docker info ## docker-client which docker ## docker daemon ps aux |grep docker 核心要素及常用操作详解 三大核心要素：镜像(Image)、容器(Container)、仓库(Registry)\n（先整体看下流程，再逐个演示）\n镜像（Image） 打包了业务代码及运行环境的包，是静态的文件，不能直接对外提供服务。\n容器（Container） 镜像的运行时，可以对外提供服务。本质上讲是利用namespace和cgroup等技术在宿主机中创建的独立的虚拟空间。\n仓库（Registry） 公有仓库，Docker Hub，阿里，网易\u0026hellip; 私有仓库，企业内部搭建 Docker Registry，Docker官方提供的镜像仓库存储服务 Harbor, 是Docker Registry的更高级封装，它除了提供友好的Web UI界面，角色和用户权限管理，用户操作审计等功能 镜像访问地址形式 registry.devops.com/demo/hello:latest,若没有前面的url地址，则默认寻找Docker Hub中的镜像，若没有tag标签，则使用latest作为标签 公有的仓库中，一般存在这么几类镜像 操作系统基础镜像（centos，ubuntu，suse，alpine） 中间件（nginx，redis，mysql，tomcat） 语言编译环境（python，java，golang） 业务镜像（django-demo\u0026hellip;） 操作演示 解压离线包\n为了保证镜像下载的速度，因此提前在一台节点下载了离线镜像包，做解压：\n$ tar zxf registry.tar.gz -C /opt $ ll /opt/registry-data total 25732 drwxr-xr-x 3 root root 4096 Apr 9 20:11 registry -rw------- 1 root root 26344448 Apr 9 22:15 registry-v2.tar 查看所有镜像：\n$ docker images 拉取镜像: $ docker pull nginx:alpine 如何唯一确定镜像: image_id repository:tag $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx alpine 377c0837328f 2 weeks ago 19.7MB 导出镜像到文件中\n$ docker save -o nginx-alpine.tar nginx:alpine\n5. 从文件中加载镜像\r```powershell\r$ docker load -i nginx-alpine.tar 部署镜像仓库\nhttps://docs.docker.com/registry/\n## 使用docker镜像启动镜像仓库服务 $ docker run -d -p 5000:5000 --restart always -v /opt/registry-data/registry:/var/lib/registry --name registry registry:2 ## 默认仓库不带认证，若需要认证，参考https://docs.docker.com/registry/deploying/#restricting-access 假设启动镜像仓库服务的主机地址为172.21.32.6，该目录中已存在的镜像列表：\n现镜像仓库地址 原镜像仓库地址 172.21.32.6:5000/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 172.21.32.6:5000/mysql:5.7 mysql:5.7 172.21.32.6:5000/nginx:alpine nginx:alpine 172.21.32.6:5000/centos:centos7.5.1804 centos:centos7.5.1804 172.21.32.6:5000/elasticsearch/elasticsearch:7.4.2 docker.elastic.co/elasticsearch/elasticsearch:7.4.2 172.21.32.6:5000/fluentd-es-root:v1.6.2-1.0 gcr.io/google_containers/fluentd-elasticsearch:v2.4.0 172.21.32.6:5000/kibana/kibana:7.4.2 docker.elastic.co/kibana/kibana:7.4.2 172.21.32.6:5000/kubernetesui/dashboard:v2.0.0-beta5 kubernetesui/dashboard:v2.0.0-beta5 172.21.32.6:5000/kubernetesui/metrics-scraper:v1.0.1 kubernetesui/metrics-scraper:v1.0.1 172.21.32.6:5000/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 推送本地镜像到镜像仓库中\n$ docker tag nginx:alpine localhost:5000/nginx:alpine $ docker push localhost:5000/nginx:alpine ## 我的镜像仓库给外部访问，不能通过localhost，尝试使用内网地址172.21.16.3:5000/nginx:alpine $ docker tag nginx:alpine 172.21.16.3:5000/nginx:alpine $ docker push 172.21.16.3:5000/nginx:alpine The push refers to repository [172.21.16.3:5000/nginx] Get https://172.21.16.3:5000/v2/: http: server gave HTTP response to HTTPS client ## docker默认不允许向http的仓库地址推送，如何做成https的，参考：https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry ## 我们没有可信证书机构颁发的证书和域名，自签名证书需要在每个节点中拷贝证书文件，比较麻烦，因此我们通过配置daemon的方式，来跳过证书的验证： $ cat /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34; ], \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;172.21.16.3:5000\u0026#34; ] } $ systemctl restart docker $ docker push 172.21.16.3:5000/nginx:alpine $ docker images\t# IMAGE ID相同，等于起别名或者加快捷方式 REPOSITORY TAG IMAGE ID CREATED SIZE 172.21.16.3:5000/nginx alpine 377c0837328f 4 weeks ago nginx alpine 377c0837328f 4 weeks ago localhost:5000/nginx alpine 377c0837328f 4 weeks ago registry 2 708bc6af7e5e 2 months ago 删除镜像\ndocker rmi nginx:alpine 查看容器列表\n## 查看运行状态的容器列表 $ docker ps ## 查看全部状态的容器列表 $ docker ps -a 启动容器\n## 后台启动 $ docker run --name nginx -d nginx:alpine ##查看run流程# ##查看容器进程 ## 等同于在虚拟机中开辟了一块隔离的独立的虚拟空间 ## 启动容器的同时进入容器，-ti与/bin/sh或者/bin/bash配套使用，意思未分配一个tty终端 $ docker run --name nginx -ti nginx:alpine /bin/sh （注意：退出容器后，该容器会变成退出状态，因为容器内部的1号进程退出） ## 实际上，在运行容器的时候，镜像地址后面跟的命令等于是覆盖了原有的容器的CMD命令，因此，执行的这些命令在容器内部就是1号进程，若该进程不存在了，那么容器就会处于退出的状态，比如，宿主机中执行 1. echo 1,执行完后，该命令立马就结束了 2. ping www.baidu.com,执行完后，命令的进程会持续运行 $ docker run -d --name test_echo nginx:alpine echo 1,容器会立马退出 $ docker run -d --name test_ping nginx:alpine ping www.baidu.com,容器不会退出，但是因为没有加-d参数，因此一直在前台运行，若ctrl+C终止，则容器退出，因为1号进程被终止了 ## 映射端口,把容器的端口映射到宿主机中,-p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; $ docker run --name nginx -d -p 8080:80 nginx:alpine ## 资源限制,-cpuset-cpus用于设置容器可以使用的 vCPU 核。-c,--cpu-shares用于设置多个容器竞争 CPU 时，各个容器相对能分配到的 CPU 时间比例。假设有三个正在运行的容器，这三个容器中的任务都是 CPU 密集型的。第一个容器的 cpu 共享权值是 1024，其它两个容器的 cpu 共享权值是 512。第一个容器将得到 50% 的 CPU 时间，而其它两个容器就只能各得到 25% 的 CPU 时间了。如果再添加第四个 cpu 共享值为 1024 的容器，每个容器得到的 CPU 时间将重新计算。第一个容器的CPU 时间变为 33%，其它容器分得的 CPU 时间分别为 16.5%、16.5%、33%。必须注意的是，这个比例只有在 CPU 密集型的任务执行时才有用。在四核的系统上，假设有四个单进程的容器，它们都能各自使用一个核的 100% CPU 时间，不管它们的 cpu 共享权值是多少。 $ docker run --cpuset-cpus=\u0026#34;0-3\u0026#34; --cpu-shares=512 --memory=500m nginx:alpine 容器数据持久化\n## 挂载主机目录 $ docker run --name nginx -d -v /opt:/opt -v /var/log:/var/log nginx:alpine $ docker run --name mysql -e MYSQL_ROOT_PASSWORD=123456 -d -v /opt/mysql/:/var/lib/mysql mysql:5.7 ## 使用volumes卷 $ docker volume ls $ docker volume create my-vol $ docker run --name nginx -d -v my-vol:/opt/my-vol nginx:alpine $ docker exec -ti nginx touch /opt/my-vol/a.txt ## 验证数据共享 $ docker run --name nginx2 -d -v my-vol:/opt/hh nginx:alpine $ docker exec -ti nginx2 ls /opt/hh/ a.txt 进入容器或者执行容器内的命令\n$ docker exec -ti \u0026lt;container_id_or_name\u0026gt; /bin/sh $ docker exec -ti \u0026lt;container_id_or_name\u0026gt; hostname 主机与容器之间拷贝数据\n## 主机拷贝到容器 $ echo \u0026#39;123\u0026#39;\u0026gt;/tmp/test.txt $ docker cp /tmp/test.txt nginx:/tmp $ docker exec -ti nginx cat /tmp/test.txt 123 ## 容器拷贝到主机 $ docker cp nginx:/tmp/test.txt ./ 查看容器日志\n## 查看全部日志 $ docker logs nginx ## 实时查看最新日志 $ docker logs -f nginx ## 从最新的100条开始查看 $ docker logs --tail=100 -f nginx 停止或者删除容器\n## 停止运行中的容器 $ docker stop nginx ## 启动退出容器 $ docker start nginx ## 删除退出容器 $ docker rm nginx ## 删除运行中的容器 $ docker rm -f nginx 查看容器或者镜像的明细\n## 查看容器详细信息，包括容器IP地址等 $ docker inspect nginx ## 查看镜像的明细信息 $ docker inspect nginx:alpine Django应用容器化实践 django项目介绍 项目地址：https://gitee.com/agagin/python-demo.git\npython3 + uwsgi + nginx + mysql\n内部服务端口8002\n构建命令 $ docker build . -t ImageName:ImageTag -f Dockerfile 如何理解构建镜像的过程？\nDockerfile是一堆指令，在docker build的时候，按照该指令进行操作，最终生成我们期望的镜像\nFROM 指定基础镜像，必须为第一个命令\n格式：\rFROM \u0026lt;image\u0026gt;\rFROM \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt;\r示例：\rFROM mysql:5.7\r注意：\rtag是可选的，如果不使用tag时，会使用latest版本的基础镜像 MAINTAINER 镜像维护者的信息\n格式：\rMAINTAINER \u0026lt;name\u0026gt;\r示例：\rMAINTAINER Yongxin Li\rMAINTAINER inspur_lyx@hotmail.com\rMAINTAINER Yongxin Li \u0026lt;inspur_lyx@hotmail.com\u0026gt; COPY|ADD 添加本地文件到镜像中\n格式：\rCOPY \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt;\r示例：\rADD hom* /mydir/ # 添加所有以\u0026#34;hom\u0026#34;开头的文件\rADD test relativeDir/ # 添加 \u0026#34;test\u0026#34; 到 `WORKDIR`/relativeDir/\rADD test /absoluteDir/ # 添加 \u0026#34;test\u0026#34; 到 /absoluteDir/ WORKDIR 工作目录\n格式：\rWORKDIR /path/to/workdir\r示例：\rWORKDIR /a (这时工作目录为/a)\r注意：\r通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行 RUN 构建镜像过程中执行命令\n格式：\rRUN \u0026lt;command\u0026gt;\r示例：\rRUN yum install nginx\rRUN pip install django\rRUN mkdir test \u0026amp;\u0026amp; rm -rf /var/lib/unusedfiles\r注意：\rRUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache CMD 构建容器后调用，也就是在容器启动时才进行调用\n格式：\rCMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (执行可执行文件，优先)\rCMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数)\rCMD command param1 param2 (执行shell内部命令)\r示例：\rCMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]\rCMD ping www.baidu.com\r注意：\rCMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 ENTRYPOINT 设置容器初始化命令，使其可执行化\n格式：\rENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] (可执行文件, 优先)\rENTRYPOINT command param1 param2 (shell内部命令)\r示例：\rENTRYPOINT [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]\r注意：\rENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令 ENV\n格式：\rENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt;\rENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;\r示例：\rENV myName John\rENV myCat=fluffy EXPOSE\n格式：\rEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...]\r示例：\rEXPOSE 80 443\rEXPOSE 8080\rEXPOSE 11211/tcp 11211/udp\r注意：\rEXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口 Dockerfile dockerfiles/myblog/Dockerfile\n# This my first django Dockerfile # Version 1.0 # Base images 基础镜像 FROM centos:centos7.5.1804 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #ENV 设置环境变量 ENV LANG en_US.UTF-8 ENV LC_ALL en_US.UTF-8 #RUN 执行以下命令 RUN curl -so /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo RUN yum install -y python36 python3-devel gcc pcre-devel zlib-devel make net-tools #工作目录 WORKDIR /opt/myblog #拷贝文件至工作目录 COPY . . #安装nginx RUN tar -zxf nginx-1.13.7.tar.gz -C /opt \u0026amp;\u0026amp; cd /opt/nginx-1.13.7 \u0026amp;\u0026amp; ./configure --prefix=/usr/local/nginx \\ \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx RUN cp myblog.conf /usr/local/nginx/conf/myblog.conf #安装依赖的插件 RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt RUN chmod +x run.sh \u0026amp;\u0026amp; rm -rf ~/.cache/pip #EXPOSE 映射端口 EXPOSE 8002 #容器启动时执行命令 CMD [\u0026#34;./run.sh\u0026#34;] 执行构建：\n$ docker build . -t myblog:v1 -f Dockerfile 定制化基础镜像 dockerfiles/myblog/Dockerfile-base\n# Base images 基础镜像 FROM centos:centos7.5.1804 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #ENV 设置环境变量 ENV LANG en_US.UTF-8 ENV LC_ALL en_US.UTF-8 #RUN 执行以下命令 RUN curl -so /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo RUN yum install -y python36 python3-devel gcc pcre-devel zlib-devel make net-tools COPY nginx-1.13.7.tar.gz /opt #安装nginx RUN tar -zxf /opt/nginx-1.13.7.tar.gz -C /opt \u0026amp;\u0026amp; cd /opt/nginx-1.13.7 \u0026amp;\u0026amp; ./configure --prefix=/usr/local/nginx \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install \u0026amp;\u0026amp; ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx ## 构建基础镜像 $ docker build . -t centos-python3-nginx:v1 -f Dockerfile-base $ docker tag centos-python3-nginx:v1 172.21.32.6:5000/base/centos-python3-nginx:v1 $ docker push 172.21.32.6:5000/base/centos-python3-nginx:v1 简化Dockerfile dockerfiles/myblog/Dockerfile-optimized\n# This my first django Dockerfile # Version 1.0 # Base images 基础镜像 FROM centos-python3-nginx:v1 #MAINTAINER 维护者信息 LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; #工作目录 WORKDIR /opt/myblog #拷贝文件至工作目录 COPY . . RUN cp myblog.conf /usr/local/nginx/conf/myblog.conf #安装依赖的插件 RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt RUN chmod +x run.sh \u0026amp;\u0026amp; rm -rf ~/.cache/pip #EXPOSE 映射端口 EXPOSE 8002 #容器启动时执行命令 CMD [\u0026#34;./run.sh\u0026#34;] $ docker build . -t myblog -f Dockerfile-optimized 运行mysql $ docker run -d -p 3306:3306 --name mysql -v /opt/mysql/mysql-data/:/var/lib/mysql -e MYSQL_DATABASE=myblog -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 ## 查看数据库 $ docker exec -ti mysql bash #/ mysql -uroot -p123456 #/ show databases; ## navicator连接 启动Django应用 ## 启动容器 $ docker run -d -p 8002:8002 --name myblog -e MYSQL_HOST=172.21.32.6 -e MYSQL_USER=root -e MYSQL_PASSWD=123456 myblog ## migrate $ docker exec -ti myblog bash #/ python3 manage.py makemigrations #/ python3 manage.py migrate #/ python3 manage.py createsuperuser ## 创建超级用户 $ docker exec -ti myblog python3 manage.py createsuperuser ## 收集静态文件 ## $ docker exec -ti myblog python3 manage.py collectstatic 访问62.234.214.206:8002/admin\n构建镜像，替换默认编码：\ndockerfiles/mysql/my.cnf\n$ cat my.cnf [mysqld] user=root character-set-server=utf8 lower_case_table_names=1 [client] default-character-set=utf8 [mysql] default-character-set=utf8 !includedir /etc/mysql/conf.d/ !includedir /etc/mysql/mysql.conf.d/ dockerfiles/mysql/Dockerfile\nFROM mysql:5.7 COPY my.cnf /etc/mysql/my.cnf ## CMD或者ENTRYPOINT默认继承 $ docker build . -t mysql:5.7-utf8 $ docker tag mysql:5.7-utf8 172.21.16.3:5000/mysql:5.7-utf8 $ docker push 172.21.16.3:5000/mysql:5.7-utf8 ## 删除旧的mysql容器，使用新镜像启动,不用再次初始化 $ docker rm -f mysql $ rm -rf /opt/mysql/mysql-data/* $ docker run -d -p 3306:3306 --name mysql -v /opt/mysql/mysql-data/:/var/lib/mysql -e MYSQL_DATABASE=myblog -e MYSQL_ROOT_PASSWORD=123456 172.21.32.6:5000/mysql:5.7-utf8 ## 重新migrate $ docker exec -ti myblog bash #/ python3 manage.py makemigrations #/ python3 manage.py migrate #/ python3 manage.py createsuperuser 实现原理 录屏！！！ 虚拟化核心需要解决的问题：资源隔离与资源限制\n虚拟机硬件虚拟化技术， 通过一个 hypervisor 层实现对资源的彻底隔离。 容器则是操作系统级别的虚拟化，利用的是内核的 Cgroup 和 Namespace 特性，此功能完全通过软件实现。 Namespace 资源隔离 命名空间是全局资源的一种抽象，将资源放到不同的命名空间中，各个命名空间中的资源是相互隔离的。 通俗来讲，就是docker在启动一个容器的时候，会调用Linux Kernel Namespace的接口，来创建一块虚拟空间，创建的时候，可以支持设置下面这几种（可以随意选择）,docker默认都设置。\npid：用于进程隔离（PID：进程ID） net：管理网络接口（NET：网络） ipc：管理对 IPC 资源的访问（IPC：进程间通信（信号量、消息队列和共享内存）） mnt：管理文件系统挂载点（MNT：挂载） uts：隔离主机名和域名 user：隔离用户和用户组（3.8以后的内核才支持） func setNamespaces(daemon *Daemon, s *specs.Spec, c *container.Container) error { // user // network // ipc // uts // pid if c.HostConfig.PidMode.IsContainer() { ns := specs.LinuxNamespace{Type: \u0026#34;pid\u0026#34;} pc, err := daemon.getPidContainer(c) if err != nil { return err } ns.Path = fmt.Sprintf(\u0026#34;/proc/%d/ns/pid\u0026#34;, pc.State.GetPID()) setNamespace(s, ns) } else if c.HostConfig.PidMode.IsHost() { oci.RemoveNamespace(s, specs.LinuxNamespaceType(\u0026#34;pid\u0026#34;)) } else { ns := specs.LinuxNamespace{Type: \u0026#34;pid\u0026#34;} setNamespace(s, ns) } return nil } CGroup 资源限制 通过namespace可以保证容器之间的隔离，但是无法控制每个容器可以占用多少资源， 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题。\nControl Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。每一个 CGroup 都是一组被相同的标准和参数限制的进程。而我们需要做的，其实就是把容器这个进程加入到指定的Cgroup中。深入理解CGroup，请点此。\nUnionFS 联合文件系统 Linux namespace和cgroup分别解决了容器的资源隔离与资源限制，那么容器是很轻量的，通常每台机器中可以运行几十上百个容器， 这些个容器是共用一个image，还是各自将这个image复制了一份，然后各自独立运行呢？ 如果每个容器之间都是全量的文件系统拷贝，那么会导致至少如下问题：\n运行容器的速度会变慢 容器和镜像对宿主机的磁盘空间的压力 怎么解决这个问题\u0026mdash;\u0026mdash;Docker的存储驱动\n镜像分层存储 UnionFS Docker 镜像是由一系列的层组成的，每层代表 Dockerfile 中的一条指令，比如下面的 Dockerfile 文件：\nFROM ubuntu:15.04 COPY . /app RUN make /app CMD python /app/app.py 这里的 Dockerfile 包含4条命令，其中每一行就创建了一层，下面显示了上述Dockerfile构建出来的镜像运行的容器层的结构：\n镜像就是由这些层一层一层堆叠起来的，镜像中的这些层都是只读的，当我们运行容器的时候，就可以在这些基础层至上添加新的可写层，也就是我们通常说的容器层，对于运行中的容器所做的所有更改（比如写入新文件、修改现有文件、删除文件）都将写入这个容器层。\n对容器层的操作，主要利用了写时复制（CoW）技术。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。 CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。\n镜像中每一层的文件都是分散在不同的目录中的，如何把这些不同目录的文件整合到一起呢？\nUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统联合到同一个挂载点的文件系统服务。 它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，整个联合的过程被称为联合挂载（Union Mount）。\n上图是AUFS的实现，AUFS是作为Docker存储驱动的一种实现，Docker 还支持了不同的存储驱动，包括 aufs、devicemapper、overlay2、zfs 和 Btrfs 等等，在最新的 Docker 中，overlay2 取代了 aufs 成为了推荐的存储驱动，但是在没有 overlay2 驱动的机器上仍然会使用 aufs 作为 Docker 的默认驱动。\nDocker网络 录屏！！！ docker容器是一块具有隔离性的虚拟系统，容器内可以有自己独立的网络空间，\n多个容器之间是如何实现通信的呢？ 容器和宿主机之间又是如何实现的通信呢？ 使用-p参数是怎么实现的端口映射? 带着我们就这些问题，我们来学习一下docker的网络模型，最后我会通过抓包的方式，给大家演示一下数据包在容器和宿主机之间的转换过程。\n网络模式 我们在使用docker run创建Docker容器时，可以用\u0026ndash;net选项指定容器的网络模式，Docker有以下4种网络模式：\nbridge模式，使用\u0026ndash;net=bridge指定，默认设置\nhost模式，使用\u0026ndash;net=host指定，容器内部网络空间共享宿主机的空间，效果类似直接在宿主机上启动一个进程，端口信息和宿主机共用。\ncontainer模式，使用\u0026ndash;net=container:NAME_or_ID指定\n指定容器与特定容器共享网络命名空间\nnone模式，使用\u0026ndash;net=none指定\n网络模式为空，即仅保留网络命名空间，但是不做任何网络相关的配置(网卡、IP、路由等)\nbridge模式 那我们之前在演示创建docker容器的时候其实是没有指定的网络模式的，如果不指定的话默认就会使用bridge模式，bridge本意是桥的意思，其实就是网桥模式，那我们怎么理解网桥，如果需要做类比的话，我们可以把网桥看成一个二层的交换机设备，我们来看下这张图：\n交换机通信简图\n网桥模式示意图\n网桥在哪，查看网桥\n$ yum install -y bridge-utils $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242b5fbe57b no veth3a496ed 有了网桥之后，那我们看下docker在启动一个容器的时候做了哪些事情才能实现容器间的互联互通\nDocker 创建一个容器的时候，会执行如下操作：\n创建一对虚拟接口/网卡，也就是veth pair； 本地主机一端桥接 到默认的 docker0 或指定网桥上，并具有一个唯一的名字，如 veth9953b75； 容器一端放到新启动的容器内部，并修改名字作为 eth0，这个网卡/接口只在容器的命名空间可见； 从网桥可用地址段中（也就是与该bridge对应的network）获取一个空闲地址分配给容器的 eth0 配置默认路由到网桥 那整个过程其实是docker自动帮我们完成的，清理掉所有容器，来验证。\n## 清掉所有容器 $ docker rm -f `docker ps -aq` $ docker ps $ brctl show # 查看网桥中的接口，目前没有 ## 创建测试容器test1 $ docker run -d --name test1 nginx:alpine $ brctl show # 查看网桥中的接口，已经把test1的veth端接入到网桥中 $ ip a |grep veth # 已在宿主机中可以查看到 $ docker exec -ti test1 sh / # ifconfig # 查看容器的eth0网卡及分配的容器ip / # route -n # 观察默认网关都指向了网桥的地址，即所有流量都转向网桥，等于是在veth pair接通了网线 Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 # 再来启动一个测试容器，测试容器间的通信 $ docker run -d --name test2 nginx:alpine $ docker exec -ti test sh / # sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories / # apk add curl / # curl 172.17.0.8:80 ## 为啥可以通信，因为两个容器是接在同一个网桥中的，通信其实是通过mac地址和端口的的记录来做转发的。test1访问test2，通过test1的eth0发送ARP广播，网桥会维护一份mac映射表，我们可以大概通过命令来看一下， $ brctl showmacs docker0 ## 这些mac地址是主机端的veth网卡对应的mac，可以查看一下 $ ip a 我们如何知道网桥上的这些虚拟网卡与容器端是如何对应？\n通过ifindex，网卡索引号\n## 查看test1容器的网卡索引 $ docker exec -ti test1 cat /sys/class/net/eth0/ifindex ## 主机中找到虚拟网卡后面这个@ifxx的值，如果是同一个值，说明这个虚拟网卡和这个容器的eth0网卡是配对的。 $ ip a |grep @if 整理脚本，快速查看对应：\nfor container in $(docker ps -q); do iflink=`docker exec -it $container sh -c \u0026#39;cat /sys/class/net/eth0/iflink\u0026#39;` iflink=`echo $iflink|tr -d \u0026#39;\\r\u0026#39;` veth=`grep -l $iflink /sys/class/net/veth*/ifindex` veth=`echo $veth|sed -e \u0026#39;s;^.*net/\\(.*\\)/ifindex$;\\1;\u0026#39;` echo $container:$veth done 上面我们讲解了容器之间的通信，那么容器与宿主机的通信是如何做的？\n添加端口映射：\n## 启动容器的时候通过-p参数添加宿主机端口与容器内部服务端口的映射 $ docker run --name test -d -p 8088:80 nginx:alpine $ curl localhost:8088 端口映射如何实现的？先来回顾iptables链表图\n访问本机的8088端口，数据包会从流入方向进入本机，因此涉及到PREROUTING和INPUT链，我们是通过做宿主机与容器之间加的端口映射，所以肯定会涉及到端口转换，那哪个表是负责存储端口转换信息的呢，就是nat表，负责维护网络地址转换信息的。因此我们来查看一下PREROUTING链的nat表：\n$ iptables -t nat -nvL PREROUTING Chain PREROUTING (policy ACCEPT 159 packets, 20790 bytes) pkts bytes target prot opt in out source destination 3 156 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL 规则利用了iptables的addrtype拓展，匹配网络类型为本地的包，如何确定哪些是匹配本地，\n$ ip route show table local type local local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 local 192.168.136.133 dev ens33 proto kernel scope host src 192.168.136.133 也就是说目标地址类型匹配到这些的，会转发到我们的TARGET中，TARGET是动作，意味着对符合要求的数据包执行什么样的操作，最常见的为ACCEPT或者DROP，此处的TARGET为DOCKER，很明显DOCKER不是标准的动作，那DOCKER是什么呢？我们通常会定义自定义的链，这样把某类对应的规则放在自定义链中，然后把自定义的链绑定到标准的链路中，因此此处DOCKER 是自定义的链。那我们现在就来看一下DOCKER这个自定义链上的规则。\n$ iptables -t nat -nvL DOCKER Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8088 to:172.17.0.2:80 此条规则就是对主机收到的目的端口为8088的tcp流量进行DNAT转换，将流量发往172.17.0.2:80，172.17.0.2地址是不是就是我们上面创建的Docker容器的ip地址，流量走到网桥上了，后面就走网桥的转发就ok了。 所以，外界只需访问192.168.136.133:8088就可以访问到容器中的服务了。\n数据包在出口方向走POSTROUTING链，我们查看一下规则：\n$ iptables -t nat -nvL POSTROUTING Chain POSTROUTING (policy ACCEPT 1099 packets, 67268 bytes) pkts bytes target prot opt in out source destination 86 5438 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 172.17.0.4 172.17.0.4 tcp dpt:80 大家注意MASQUERADE这个动作是什么意思，其实是一种更灵活的SNAT，把源地址转换成主机的出口ip地址，那解释一下这条规则的意思:\n这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。大概的过程就是ACK的包在容器里面发出来，会路由到网桥docker0，网桥根据宿主机的路由规则会转给宿主机网卡eth0，这时候包就从docker0网卡转到eth0网卡了，并从eth0网卡发出去，这时候这条规则就会生效了，把源地址换成了eth0的ip地址。\n注意一下，刚才这个过程涉及到了网卡间包的传递，那一定要打开主机的ip_forward转发服务，要不然包转不了，服务肯定访问不到。\n抓包演示 我们先想一下，我们要抓哪个网卡的包\n首先访问宿主机的8088端口，我们抓一下宿主机的eth0\n$ tcpdump -i eth0 port 8088 -w host.cap 然后最终包会流入容器内，那我们抓一下容器内的eth0网卡\n# 容器内安装一下tcpdump $ sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories $ apk add tcpdump $ tcpdump -i eth0 port 80 -w container.cap 到另一台机器访问一下，\n$ curl 172.21.32.6:8088/ 停止抓包，拷贝容器内的包到宿主机\n$ docker cp test:/root/container.cap /root/ 把抓到的内容拷贝到本地，使用wireshark进行分析。\n$ scp root@172.21.32.6:/root/*.cap /d/packages （wireshark合并包进行分析）\n进到容器内的包做DNAT，出去的包做SNAT，这样对外面来讲，根本就不知道机器内部是谁提供服务，其实这就和一个内网多个机器公用一个外网IP地址上网的效果是一样的，对吧，那这也属于NAT功能的一个常见的应用场景。\nHost模式 容器内部不会创建网络空间，共享宿主机的网络空间\n$ docker run --net host -d --name mysql mysql:5.7 Conatiner模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。\n## 启动测试容器，共享mysql的网络空间 $ docker run -ti --rm --net=container:mysql busybox sh / # ip a / # netstat -tlp|grep 3306 / # telnet localhost 3306 实用技巧 清理主机上所有退出的容器\n$ docker rm $(docker ps -aq) 调试或者排查容器启动错误\n## 若有时遇到容器启动失败的情况，可以先使用相同的镜像启动一个临时容器，先进入容器 $ docker exec -ti --rm \u0026lt;image_id\u0026gt; bash ## 进入容器后，手动执行该容器对应的ENTRYPOINT或者CMD命令，这样即使出错，容器也不会退出，因为bash作为1号进程，我们只要不退出容器，该容器就不会自动退出 本章小结 为了解决软件交付过程中的环境依赖，同时提供一种更加轻量的虚拟化技术，Docker出现了\n2013年诞生，15年开始迅速发展，从17.03月开始，使用时间日期管理版本，稳定版以每季度为准\nDocker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理\n使用yum部署docker，启动后通过操作docker这个命令行，自动调用docker daemon完成容器相关操作\n常用操作\nsystemctl start|stop|restart docker docker build | pull -\u0026gt; docker tag -\u0026gt; docker push docker run \u0026ndash;name my-demo -d -p 8080:80 -v /opt/data:/data demo:v20200327 docker cp /path/a.txt mycontainer:/opt docker exec -ti mycontainer /bin/sh docker logs -f mycontainer 通过dockerfile构建业务镜像，先使用基础镜像，然后通过一系列的指令把我们的业务应用所需要的运行环境和依赖都打包到镜像中，然后通过CMD或者ENTRYPOINT指令把镜像启动时的入口制定好，完成封装即可。有点类似于，先找来一个空的集装箱(基础镜像)，然后把项目依赖的服务都扔到集装箱中，然后设置好服务的启动入口，关闭箱门，即完成了业务镜像的制作。\n容器的实现依赖于内核模块提供的namespace和control-group的功能，通过namespace创建一块虚拟空间，空间内实现了各类资源(进程、网络、文件系统)的隔离，提供control-group实现了对隔离的空间的资源使用的限制。\ndocker镜像使用分层的方式进行存储，根据主机的存储驱动的不同，实现方式会不同，kernel在3.10.0-514以上自动支持overlay2 存储驱动，也是目前Docker推荐的方式。\n得益于分层存储的模式，多个容器可以通过copy-on-write的策略，在镜像的最上层加一个可写层，实现一个镜像快速启动多个容器的场景\ndocker的网络模式分为4种，最常用的为bridge和host模式。bridge模式通过docker0网桥，启动容器的时候通过创建一对虚拟网卡，将容器连接在桥上，同时维护了虚拟网卡与网桥端口的关系，实现容器间的通信。容器与宿主机之间的通信通过iptables端口映射的方式，docker利用iptables的PREROUTING和POSTROUTING的nat功能，实现了SNAT与DNAT，使得容器内部的服务被完美的保护起来。\n第二章 Kubernetes实践之旅 录屏！！！ 本章学习kubernetes的架构及工作流程，重点介绍如何使用Deployment管理Pod生命周期，实现服务不中断的滚动更新，通过服务发现来实现集群内部的服务间访问，并通过ingress-nginx实现外部使用域名访问集群内部的服务。同时介绍基于EFK如何搭建Kubernetes集群的日志收集系统。\n学完本章，我们的Django demo项目已经可以运行在k8s集群中，同时我们可以使用域名进行服务的访问。\n架构及核心组件介绍 使用kubeadm快速搭建集群 运行第一个Pod应用 Pod进阶 Pod控制器的使用 实现服务与Node绑定的几种方式 负载均衡与服务发现 使用Ingress实现集群服务的7层代理 Django项目k8s落地实践 基于EFK实现kubernetes集群的日志平台（扩展） 集群认证与授权 纯容器模式的问题 业务容器数量庞大，哪些容器部署在哪些节点，使用了哪些端口，如何记录、管理，需要登录到每台机器去管理？ 跨主机通信，多个机器中的容器之间相互调用如何做，iptables规则手动维护？ 跨主机容器间互相调用，配置如何写？写死固定IP+端口？ 如何实现业务高可用？多个容器对外提供服务如何实现负载均衡？ 容器的业务中断了，如何可以感知到，感知到以后，如何自动启动新的容器? 如何实现滚动升级保证业务的连续性？ \u0026hellip;\u0026hellip; 容器调度管理平台 Docker Swarm Mesos Google Kubernetes\n2017年开始Kubernetes凭借强大的容器集群管理功能, 逐步占据市场,目前在容器编排领域一枝独秀\nhttps://kubernetes.io/\n架构图 区分组件与资源\n核心组件 ETCD：分布式高性能键值数据库,存储整个集群的所有元数据\nApiServer: API服务器,集群资源访问控制入口,提供restAPI及安全访问控制\nScheduler：调度器,负责把业务容器调度到最合适的Node节点\nController Manager：控制器管理,确保集群资源按照期望的方式运行\nReplication Controller Node controller ResourceQuota Controller Namespace Controller ServiceAccount Controller Tocken Controller Service Controller Endpoints Controller kubelet：运行在每运行在每个节点上的主要的“节点代理”个节点上的主要的“节点代理”\npod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理. 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要 kubectl: 命令行接口，用于对 Kubernetes 集群运行命令 https://kubernetes.io/zh/docs/reference/kubectl/\nCNI实现: 通用网络接口, 我们使用flannel来作为k8s集群的网络插件, 实现跨节点通信\n工作流程 用户准备一个资源文件（记录了业务应用的名称、镜像地址等信息），通过调用APIServer执行创建Pod APIServer收到用户的Pod创建请求，将Pod信息写入到etcd中 调度器通过list-watch的方式，发现有新的pod数据，但是这个pod还没有绑定到某一个节点中 调度器通过调度算法，计算出最适合该pod运行的节点，并调用APIServer，把信息更新到etcd中 kubelet同样通过list-watch方式，发现有新的pod调度到本机的节点了，因此调用容器运行时，去根据pod的描述信息，拉取镜像，启动容器，同时生成事件信息 同时，把容器的信息、事件及状态也通过APIServer写入到etcd中 实践\u0026ndash;集群安装 录屏！！！ kubeadm https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/\n《Kubernetes安装手册（非高可用版）》\n核心组件 静态Pod的方式：\n## etcd、apiserver、controller-manager、kube-scheduler $ kubectl -n kube-system get po systemd服务方式：\n$ systemctl status kubelet kubectl：二进制命令行工具\n理解集群资源 组件是为了支撑k8s平台的运行，安装好的软件。\n资源是如何去使用k8s的能力的定义。比如，k8s可以使用Pod来管理业务应用，那么Pod就是k8s集群中的一类资源，集群中的所有资源可以提供如下方式查看：\n$ kubectl api-resources 如何理解namespace：\n命名空间，集群内一个虚拟的概念，类似于资源池的概念，一个池子里可以有各种资源类型，绝大多数的资源都必须属于某一个namespace。集群初始化安装好之后，会默认有如下几个namespace：\n$ kubectl get namespaces NAME STATUS AGE default Active 84m kube-node-lease Active 84m kube-public Active 84m kube-system Active 84m kubernetes-dashboard Active 71m 所有NAMESPACED的资源，在创建的时候都需要指定namespace，若不指定，默认会在default命名空间下 相同namespace下的同类资源不可以重名，不同类型的资源可以重名 不同namespace下的同类资源可以重名 通常在项目使用的时候，我们会创建带有业务含义的namespace来做逻辑上的整合 kubectl的使用 类似于docker，kubectl是命令行工具，用于与APIServer交互，内置了丰富的子命令，功能极其强大。 https://kubernetes.io/docs/reference/kubectl/overview/\n$ kubectl -h $ kubectl get -h $ kubectl create -h $ kubectl create namespace -h kubectl如何管理集群资源\n$ kubectl get po -v=7 实践\u0026ndash;使用k8s管理业务应用 最小调度单元 Pod 录屏！！！ docker调度的是容器，在k8s集群中，最小的调度单元是Pod（豆荚）\n为什么引入Pod 与容器引擎解耦\nDocker、Rkt。平台设计与引擎的具体的实现解耦\n多容器共享网络|存储|进程 空间, 支持的业务场景更加灵活\n使用yaml格式定义Pod myblog/one-pod/pod.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\t\u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;demo\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;component\u0026#34;: \u0026#34;myblog\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;172.21.32.6:5000/myblog\u0026#34;, \u0026#34;env\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;MYSQL_HOST\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;127.0.0.1\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;MYSQL_PASSWD\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;123456\u0026#34; } ], \u0026#34;ports\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: 8002 } ] }, { \u0026#34;name\u0026#34;: \u0026#34;mysql\u0026#34;, ... } ] } } apiVersion 含义 alpha 进入K8s功能的早期候选版本，可能包含Bug，最终不一定进入K8s beta 已经过测试的版本，最终会进入K8s，但功能、对象定义可能会发生变更。 stable 可安全使用的稳定版本 v1 stable 版本之后的首个版本，包含了更多的核心对象 apps/v1 使用最广泛的版本，像Deployment、ReplicaSets都已进入该版本 资源类型与apiVersion对照表\nKind apiVersion ClusterRoleBinding rbac.authorization.k8s.io/v1 ClusterRole rbac.authorization.k8s.io/v1 ConfigMap v1 CronJob batch/v1beta1 DaemonSet extensions/v1beta1 Node v1 Namespace v1 Secret v1 PersistentVolume v1 PersistentVolumeClaim v1 Pod v1 Deployment v1、apps/v1、apps/v1beta1、apps/v1beta2 Service v1 Ingress extensions/v1beta1 ReplicaSet apps/v1、apps/v1beta2 Job batch/v1 StatefulSet apps/v1、apps/v1beta1、apps/v1beta2 快速获得资源和版本\n$ kubectl explain pod $ kubectl explain Pod.apiVersion 创建和访问Pod ## 创建namespace, namespace是逻辑上的资源池 $ kubectl create namespace demo ## 使用指定文件创建Pod $ kubectl create -f demo-pod.yaml ## 查看pod，可以简写po ## 所有的操作都需要指定namespace，如果是在default命名空间下，则可以省略 $ kubectl -n demo get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m 10.244.1.146 k8s-slave1 ## 使用Pod Ip访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/ ## 进入容器,执行初始化, 不必到对应的主机执行docker exec $ kubectl -n demo exec -ti myblog -c myblog bash / # env / # python3 manage.py migrate $ kubectl -n demo exec -ti myblog -c mysql bash / # mysql -p123456 ## 再次访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/ Infra容器 登录k8s-slave1节点\n$ docker ps -a |grep myblog ## 发现有三个容器 ## 其中包含mysql和myblog程序以及Infra容器 ## 为了实现Pod内部的容器可以通过localhost通信，每个Pod都会启动Infra容器，然后Pod内部的其他容器的网络空间会共享该Infra容器的网络空间(Docker网络的container模式)，Infra容器只需要hang住网络空间，不需要额外的功能，因此资源消耗极低。 ## 登录master节点，查看pod内部的容器ip均相同，为pod ip $ kubectl -n demo exec -ti myblog -c myblog bash / # ifconfig $ kubectl -n demo exec -ti myblog -c mysql bash / # ifconfig pod容器命名: k8s_\u0026lt;container_name\u0026gt;_\u0026lt;pod_name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;random_string\u0026gt;\n查看pod详细信息 ## 查看pod调度节点及pod_ip $ kubectl -n demo get pods -o wide ## 查看完整的yaml $ kubectl -n demo get po myblog -o yaml ## 查看pod的明细信息及事件 $ kubectl -n demo describe pod myblog Troubleshooting and Debugging #进入Pod内的容器 $ kubectl -n \u0026lt;namespace\u0026gt; exec \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; -ti /bin/sh #查看Pod内容器日志,显示标准或者错误输出日志 $ kubectl -n \u0026lt;namespace\u0026gt; logs -f \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; 更新服务版本 $ kubectl apply -f demo-pod.yaml 删除Pod服务 #根据文件删除 $ kubectl delete -f demo-pod.yaml #根据pod_name删除 $ kubectl -n \u0026lt;namespace\u0026gt; delete pod \u0026lt;pod_name\u0026gt; Pod数据持久化 若删除了Pod，由于mysql的数据都在容器内部，会造成数据丢失，因此需要数据进行持久化。\n定点使用hostpath挂载，nodeSelector定点\nmyblog/one-pod/pod-with-volume.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; volumeMounts: - name: mysql-data mountPath: /var/lib/mysql 保存文件为pod-with-volume.yaml，执行创建\n## 若存在旧的同名服务，先删除掉，后创建 $ kubectl -n demo delete pod myblog ## 创建 $ kubectl create -f pod-with-volume.yaml ## 此时pod状态Pending $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE myblog 0/2 Pending 0 32s ## 查看原因，提示调度失败，因为节点不满足node selector $ kubectl -n demo describe po myblog Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 12s (x2 over 12s) default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector. ## 为节点打标签 $ kubectl label node k8s-slave1 component=mysql ## 再次查看，已经运行成功 $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m54s 10.244.1.150 k8s-slave1 ## 到k8s-slave1节点，查看/opt/mysql/data $ ll /opt/mysql/data/ total 188484 -rw-r----- 1 polkitd input 56 Mar 29 09:20 auto.cnf -rw------- 1 polkitd input 1676 Mar 29 09:20 ca-key.pem -rw-r--r-- 1 polkitd input 1112 Mar 29 09:20 ca.pem drwxr-x--- 2 polkitd input 8192 Mar 29 09:20 sys ... ## 执行migrate，创建数据库表，然后删掉pod，再次创建后验证数据是否存在 $ kubectl -n demo exec -ti myblog python3 manage.py migrate ## 访问服务，正常 $ curl 10.244.1.150:8002/blog/index/ ## 删除pod $ kubectl delete -f pod-with-volume.yaml ## 再次创建Pod $ kubectl create -f pod-with-volume.yaml ## 查看pod ip并访问服务 $ kubectl -n demo get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 7s 10.244.1.151 k8s-slave1 ## 未做migrate，服务正常 $ curl 10.244.1.151:8002/blog/index/ 使用PV+PVC连接分布式存储解决方案\nceph glusterfs nfs 服务健康检查 检测容器服务是否健康的手段，若不健康，会根据设置的重启策略（restartPolicy）进行操作，两种检测机制可以分别单独设置，若不设置，默认认为Pod是健康的。\n两种机制：\nLivenessProbe探针 用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。 ReadinessProbe探针 用于判断容器是否正常提供服务，即容器的Ready是否为True，是否可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。（剔除此pod不参与接收请求不会将流量转发给此Pod）。 三种类型：\nexec：通过执行命令来检查服务是否正常，回值为0则表示容器健康 httpGet方式：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康 tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康 示例：\n完整文件路径 myblog/one-pod/pod-with-healthcheck.yaml\ncontainers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 initialDelaySeconds：容器启动后第一次执行探测是需要等待多少秒。 periodSeconds：执行探测的频率。默认是10秒，最小1秒。 timeoutSeconds：探测超时时间。默认1秒，最小1秒。 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1，最小值是1。 failureThreshold：探测成功后，最少连续探测失败多少次 才被认定为失败。默认是3，最小值是1。 重启策略：\nPod的重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。 Pod的重启策略包括Always、OnFailure和Never，默认值为Always。\nAlways：当容器失败时，由kubelet自动重启该容器； OnFailure：当容器终止运行且退出码不为0时，有kubelet自动重启该容器； Never：不论容器运行状态如何，kubelet都不会重启该容器。 镜像拉取策略 spec: containers: - name: myblog image: 172.21.32.6:5000/demo/myblog imagePullPolicy: IfNotPresent 设置镜像的拉取策略，默认为IfNotPresent\nAlways，总是拉取镜像，即使本地有镜像也从仓库拉取 IfNotPresent ，本地有则使用本地镜像，本地没有则去仓库拉取 Never，只使用本地镜像，本地没有则报错 Pod资源限制 为了保证充分利用集群资源，且确保重要容器在运行周期内能够分配到足够的资源稳定运行，因此平台需要具备\nPod的资源限制的能力。 对于一个pod来说，资源最基础的2个的指标就是：CPU和内存。\nKubernetes提供了个采用requests和limits 两种类型参数对资源进行预分配和使用限制。\n完整文件路径：myblog/one-pod/pod-with-resourcelimits.yaml\n... containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m ... requests：\n容器使用的最小资源需求,作用于schedule阶段，作为容器调度时资源分配的判断依赖 只有当前节点上可分配的资源量 \u0026gt;= request 时才允许将容器调度到该节点 request参数不限制容器的最大可使用资源 requests.cpu被转成docker的\u0026ndash;cpu-shares参数，与cgroup cpu.shares功能相同 (无论宿主机有多少个cpu或者内核，\u0026ndash;cpu-shares选项都会按照比例分配cpu资源） requests.memory没有对应的docker参数，仅作为k8s调度依据 limits：\n容器能使用资源的最大值 设置为0表示对使用的资源不做限制, 可无限的使用 当pod 内存超过limit时，会被oom 当cpu超过limit时，不会被kill，但是会限制不超过limit值 limits.cpu会被转换成docker的–cpu-quota参数。与cgroup cpu.cfs_quota_us功能相同 limits.memory会被转换成docker的–memory参数。用来限制容器使用的最大内存 对于 CPU，我们知道计算机里 CPU 的资源是按“时间片”的方式来进行分配的，系统里的每一个操作都需要 CPU 的处理，所以，哪个任务要是申请的 CPU 时间片越多，那么它得到的 CPU 资源就越多。\n然后还需要了解下 CGroup 里面对于 CPU 资源的单位换算：\n1 CPU = 1000 millicpu（1 Core = 1000m） 这里的 m 就是毫、毫核的意思，Kubernetes 集群中的每一个节点可以通过操作系统的命令来确认本节点的 CPU 内核数量，然后将这个数量乘以1000，得到的就是节点总 CPU 总毫数。比如一个节点有四核，那么该节点的 CPU 总毫量为 4000m。\ndocker run命令和 CPU 限制相关的所有选项如下：\n选项 描述 --cpuset-cpus=\u0026quot;\u0026quot; 允许使用的 CPU 集，值可以为 0-3,0,1 -c,--cpu-shares=0 CPU 共享权值（相对权重） cpu-period=0 限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000] --cpu-quota=0 限制 CPU CFS 配额，必须不小于1ms，即 \u0026gt;= 1000，绝对限制 docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:16.04 /bin/bash 将 CFS 调度的周期设为 50000，将容器在每个周期内的 CPU 配额设置为 25000，表示该容器每 50ms 可以得到 50% 的 CPU 运行时间。\n注意：若内存使用超出限制，会引发系统的OOM机制，因CPU是可压缩资源，不会引发Pod退出或重建\nyaml优化 目前完善后的yaml，myblog/one-pod/pod-completed.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: myblog image: 172.21.32.6:5000/myblog env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;127.0.0.1\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql 为什么要优化\n考虑真实的使用场景，像数据库这类中间件，是作为公共资源，为多个项目提供服务，不适合和业务容器绑定在同一个Pod中，因为业务容器是经常变更的，而数据库不需要频繁迭代 yaml的环境变量中存在敏感信息（账号、密码），存在安全隐患 解决问题一，需要拆分yaml\nmyblog/two-pod/mysql.yaml\napiVersion: v1 kind: Pod metadata: name: mysql namespace: demo labels: component: mysql spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD value: \u0026#34;123456\u0026#34; - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql myblog.yaml\napiVersion: v1 kind: Pod metadata: name: myblog namespace: demo labels: component: myblog spec: containers: - name: myblog image: 172.21.32.6:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST # 指定root用户的用户名 value: \u0026#34;172.21.32.6\u0026#34; - name: MYSQL_PASSWD value: \u0026#34;123456\u0026#34; ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 创建测试\n## 先删除旧pod $ kubectl -n demo delete po myblog ## 分别创建mysql和myblog $ kubectl create -f mysql.yaml $ kubectl create -f myblog.yaml ## 查看pod，注意mysqlIP为宿主机IP，因为网络模式为host $ kubectl -n demo get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 1/1 Running 0 41s 10.244.1.152 k8s-slave1 mysql 1/1 Running 0 52s 192.168.136.131 k8s-slave1 ## 访问myblog服务正常 $ curl 10.244.1.152:8002/blog/index/ 解决问题二，环境变量中敏感信息带来的安全隐患\n为什么要统一管理环境变量\n环境变量中有很多敏感的信息，比如账号密码，直接暴漏在yaml文件中存在安全性问题 团队内部一般存在多个项目，这些项目直接存在配置相同环境变量的情况，因此可以统一维护管理 对于开发、测试、生产环境，由于配置均不同，每套环境部署的时候都要修改yaml，带来额外的开销 k8s提供两类资源，configMap和Secret，可以用来实现业务配置的统一管理， 允许将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性 。\nconfigMap，通常用来管理应用的配置文件或者环境变量，myblog/two-pod/configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: myblog namespace: demo data: MYSQL_HOST: \u0026#34;172.21.32.6\u0026#34; MYSQL_PORT: \u0026#34;3306\u0026#34; Secret，管理敏感类的信息，默认会base64编码存储，有三种类型\nService Account ：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；创建ServiceAccount后，Pod中指定serviceAccount后，自动创建该ServiceAccount对应的secret； Opaque ： base64编码格式的Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有docker registry的认证信息。 myblog/two-pod/secret.yaml\napiVersion: v1 kind: Secret metadata: name: myblog namespace: demo type: Opaque data: MYSQL_USER: cm9vdA==\t#注意加-n参数， echo -n root|base64 MYSQL_PASSWD: MTIzNDU2 创建并查看：\n$ kubectl create -f secret.yaml $ kubectl -n demo get secret 如果不习惯这种方式，可以通过如下方式：\n$ cat secret.txt MYSQL_USER=root MYSQL_PASSWD=123456 $ kubectl -n demo create secret generic myblog --from-env-file=secret.txt 修改后的mysql的yaml，资源路径：myblog/two-pod/mysql-with-config.yaml\n... spec: containers: - name: mysql image: 172.21.32.6:5000/mysql:5.7-utf8 env: - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; ... 修改后的myblog的yaml，资源路径：myblog/two-pod/myblog-with-config.yaml\nspec: containers: - name: myblog image: 172.21.32.6:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST valueFrom: configMapKeyRef: name: myblog key: MYSQL_HOST - name: MYSQL_PORT valueFrom: configMapKeyRef: name: myblog key: MYSQL_PORT - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD 在部署不同的环境时，pod的yaml无须再变化，只需要在每套环境中维护一套ConfigMap和Secret即可。但是注意configmap和secret不能跨namespace使用，且更新后，pod内的env不会自动更新，重建后方可更新。\n如何编写资源yaml 拿来主义，从机器中已有的资源中拿\n$ kubectl -n kube-system get po,deployment,ds 学会在官网查找， https://kubernetes.io/docs/home/\n从kubernetes-api文档中查找， https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#pod-v1-core\nkubectl explain 查看具体字段含义\npod状态与生命周期 Pod的状态如下表所示：\n状态值 描述 Pending API Server已经创建该Pod，等待调度器调度 ContainerCreating 镜像正在创建 Running Pod内容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态 Succeeded Pod内所有容器均已成功执行退出，且不再重启 Failed Pod内所有容器均已退出，但至少有一个容器退出为失败状态 CrashLoopBackOff Pod内有容器启动失败，比如配置文件丢失导致主进程启动失败 Unknown 由于某种原因无法获取该Pod的状态，可能由于网络通信不畅导致 生命周期示意图：\n启动和关闭示意：\napiVersion: v1 kind: Pod metadata: name: demo-start-stop namespace: demo labels: component: demo-start-stop spec: initContainers: - name: init image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): INIT \u0026gt;\u0026gt; /loap/timing\u0026#39;] volumeMounts: - mountPath: /loap name: timing containers: - name: main image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): START \u0026gt;\u0026gt; /loap/timing; sleep 10; echo $(date +%s): END \u0026gt;\u0026gt; /loap/timing;\u0026#39;] volumeMounts: - mountPath: /loap name: timing livenessProbe: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): LIVENESS \u0026gt;\u0026gt; /loap/timing\u0026#39;] readinessProbe: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): READINESS \u0026gt;\u0026gt; /loap/timing\u0026#39;] lifecycle: postStart: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): POST-START \u0026gt;\u0026gt; /loap/timing\u0026#39;] preStop: exec: command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): PRE-STOP \u0026gt;\u0026gt; /loap/timing\u0026#39;] volumes: - name: timing hostPath: path: /tmp/loap 创建pod测试：\n$ kubectl create -f demo-pod-start.yaml ## 查看demo状态 $ kubectl -n demo get po -o wide -w ## 查看调度节点的/tmp/loap/timing $ cat /tmp/loap/timing 1585424708: INIT 1585424746: START 1585424746: POST-START 1585424754: READINESS 1585424756: LIVENESS 1585424756: END 须主动杀掉 Pod 才会触发 pre-stop hook，如果是 Pod 自己 Down 掉，则不会执行 pre-stop hook\n小结 实现k8s平台与特定的容器运行时解耦，提供更加灵活的业务部署方式，引入了Pod概念 k8s使用yaml格式定义资源文件，yaml中Map与List的语法，与json做类比 通过kubectl create | get | exec | logs | delete 等操作k8s资源，必须指定namespace 每启动一个Pod，为了实现网络空间共享，会先创建Infra容器，并把其他容器网络加入该容器 通过livenessProbe和readinessProbe实现Pod的存活性和就绪健康检查 通过requests和limit分别限定容器初始资源申请与最高上限资源申请 通过Pod IP访问具体的Pod服务，实现是 Pod控制器 录屏！！！ 只使用Pod, 将会面临如下需求:\n业务应用启动多个副本 Pod重建后IP会变化，外部如何访问Pod服务 运行业务Pod的某个节点挂了，可以自动帮我把Pod转移到集群中的可用节点启动起来 我的业务应用功能是收集节点监控数据,需要把Pod运行在k8集群的各个节点上 Workload (工作负载) 控制器又称工作负载是用于实现管理pod的中间层，确保pod资源符合预期的状态，pod的资源出现故障时，会尝试 进行重启，当根据重启策略无效，则会重新新建pod的资源。\nReplicaSet: 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能 Deployment：工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置 DaemonSet：用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如ELK服务 Job：只要完成就立即退出，不需要重启或重建 Cronjob：周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 Deployment myblog/deployment/deploy-mysql.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: mysql namespace: demo spec: replicas: 1\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: mysql template: metadata: labels:\t#给Pod打label app: mysql spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.15:5000/mysql:5.7-utf8 ports: - containerPort: 3306 env: - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD - name: MYSQL_DATABASE value: \u0026#34;myblog\u0026#34; resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 5 periodSeconds: 10 livenessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 20 volumeMounts: - name: mysql-data mountPath: /var/lib/mysql deploy-myblog.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: myblog namespace: demo spec: replicas: 1\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: myblog template: metadata: labels:\t#给Pod打label app: myblog spec: containers: - name: myblog image: 172.21.32.15:5000/myblog imagePullPolicy: IfNotPresent env: - name: MYSQL_HOST valueFrom: configMapKeyRef: name: myblog key: MYSQL_HOST - name: MYSQL_PORT valueFrom: configMapKeyRef: name: myblog key: MYSQL_PORT - name: MYSQL_USER valueFrom: secretKeyRef: name: myblog key: MYSQL_USER - name: MYSQL_PASSWD valueFrom: secretKeyRef: name: myblog key: MYSQL_PASSWD ports: - containerPort: 8002 resources: requests: memory: 100Mi cpu: 50m limits: memory: 500Mi cpu: 100m livenessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒 periodSeconds: 15 # 执行探测的频率 timeoutSeconds: 2\t# 探测超时时间 readinessProbe: httpGet: path: /blog/index/ port: 8002 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 2 periodSeconds: 15 创建Deployment $ kubectl create -f deploy.yaml 查看Deployment # kubectl api-resources $ kubectl -n demo get deploy NAME READY UP-TO-DATE AVAILABLE AGE myblog 1/1 1 1 2m22s mysql 1/1 1 1 2d11h * `NAME` 列出了集群中 Deployments 的名称。 * `READY`显示当前正在运行的副本数/期望的副本数。 * `UP-TO-DATE`显示已更新以实现期望状态的副本数。 * `AVAILABLE`显示应用程序可供用户使用的副本数。 * `AGE` 显示应用程序运行的时间量。 # 查看pod $ kubectl -n demo get po NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 109s mysql-85f4f65f99-w6jkj 1/1 Running 0 2m28s 副本保障机制 controller实时检测pod状态，并保障副本数一直处于期望的值。\n## 删除pod，观察pod状态变化 $ kubectl -n demo delete pod myblog-7c96c9f76b-qbbg7 # 观察pod $ kubectl get pods -o wide ## 设置两个副本, 或者通过kubectl -n demo edit deploy myblog的方式，最好通过修改文件，然后apply的方式，这样yaml文件可以保持同步 $ kubectl -n demo scale deploy myblog --replicas=2 deployment.extensions/myblog scaled # 观察pod $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 11m myblog-7c96c9f76b-s6brm 1/1 Running 0 55s mysql-85f4f65f99-w6jkj 1/1 Running 0 11m Pod驱逐策略 K8S 有个特色功能叫 pod eviction，它在某些场景下如节点 NotReady，或者资源不足时，把 pod 驱逐至其它节点，这也是出于业务保护的角度去考虑的。\nKube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。停掉kubelet\npod-eviction-timeout：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod\nmemory.available：节点可用内存 nodefs.available：节点根盘可用存储空间 nodefs.inodesFree：节点inodes可用数量 imagefs.available：镜像存储盘的可用空间 imagefs.inodesFree：镜像存储盘的inodes可用数量 服务更新 修改dockerfile，重新打tag模拟服务更新。\n更新方式：\n修改yaml文件，使用kubectl -n demo apply -f deploy-myblog.yaml来应用更新 kubectl -n demo edit deploy myblog在线更新 kubectl set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record 修改文件测试：\n$ vi mybolg/blog/template/index.html $ docker build . -t 172.21.32.6:5000/myblog:v2 -f Dockerfile_optimized $ docker push 172.21.32.6:5000/myblog:v2 更新策略 ... spec: replicas: 2\t#指定Pod副本数 selector:\t#指定Pod的选择器 matchLabels: app: myblog strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate\t#指定更新方式为滚动更新，默认策略，通过get deploy yaml查看 ... 策略控制：\nmaxSurge：最大激增数, 指更新过程中, 最多可以比replicas预先设定值多出的pod数量, 可以为固定值或百分比,默认为desired Pods数的25%。计算时向上取整(比如3.4，取4)，更新过程中最多会有replicas + maxSurge个pod maxUnavailable： 指更新过程中, 最多有几个pod处于无法服务状态 , 可以为固定值或百分比，默认为desired Pods数的25%。计算时向下取整(比如3.6，取3) 在Deployment rollout时，需要保证Available(Ready) Pods数不低于 desired pods number - maxUnavailable; 保证所有的非异常状态Pods数不多于 desired pods number + maxSurge。\n以myblog为例，使用默认的策略，更新过程:\nmaxSurge 25%，2个实例，向上取整，则maxSurge为1，意味着最多可以有2+1=3个Pod，那么此时会新创建1个ReplicaSet，RS-new，把副本数置为1，此时呢，副本控制器就去创建这个新的Pod 同时，maxUnavailable是25%，副本数2*25%，向下取整，则为0，意味着，滚动更新的过程中，不能有少于2个可用的Pod，因此，旧的Replica（RS-old）会先保持不动，等RS-new管理的Pod状态Ready后，此时已经有3个Ready状态的Pod了，那么由于只要保证有2个可用的Pod即可，因此，RS-old的副本数会有2个变成1个，此时，会删掉一个旧的Pod 删掉旧的Pod的时候，由于总的Pod数量又变成2个了，因此，距离最大的3个还有1个Pod可以创建，所以，RS-new把管理的副本数由1改成2，此时又会创建1个新的Pod，等RS-new管理了2个Pod都ready后，那么就可以把RS-old的副本数由1置为0了，这样就完成了滚动更新 #查看滚动更新事件 $ kubectl -n demo describe deploy myblog ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 1 Normal ScalingReplicaSet 11s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 1 Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 2 Normal ScalingReplicaSet 6s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 0 $ kubectl get rs NAME DESIRED CURRENT READY AGE myblog-6cf56fc848 2 2 2 16h myblog-6fdcf98f9 0 0 0 16h 服务回滚 通过滚动升级的策略可以平滑的升级Deployment，若升级出现问题，需要最快且最好的方式回退到上一次能够提供正常工作的版本。为此K8S提供了回滚机制。\nrevision：更新应用时，K8S都会记录当前的版本号，即为revision，当升级出现问题时，可通过回滚到某个特定的revision，默认配置下，K8S只会保留最近的几个revision，可以通过Deployment配置文件中的spec.revisionHistoryLimit属性增加revision数量，默认是10。\n查看当前：\n$ kubectl -n demo rollout history deploy myblog ##CHANGE-CAUSE为空 $ kubectl delete -f deploy-myblog.yaml ## 方便演示到具体效果，删掉已有deployment 记录回滚：\n$ kubectl create -f deploy-myblog.yaml --record $ kubectl -n demo set image deploy myblog myblog=172.21.32.6:5000/myblog:v2 --record=true 查看deployment更新历史：\n$ kubectl -n demo rollout history deploy myblog deployment.extensions/myblog REVISION CHANGE-CAUSE 1 kubectl create --filename=deploy-myblog.yaml --record=true 2 kubectl set image deploy myblog myblog=172.21.32.6:5000/demo/myblog:v1 --record=true 回滚到具体的REVISION:\n$ kubectl -n demo rollout undo deploy myblog --to-revision=1 deployment.extensions/myblog rolled back # 访问应用测试 Kubernetes调度 录屏！！！ 为何要控制Pod应该如何调度 集群中有些机器的配置高（SSD，更好的内存等），我们希望核心的服务（比如说数据库）运行在上面 某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上 \u0026hellip;\u0026hellip; NodeSelector label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，POD 的调度可以根据节点的 label 进行特定的部署。\n查看节点的label：\n$ kubectl get nodes --show-labels 为节点打label：\n$ kubectl label node k8s-master disktype=ssd 当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。\n... spec: hostNetwork: true\t# 声明pod的网络模式为host模式，效果通docker run --net=host volumes: - name: mysql-data hostPath: path: /opt/mysql/data nodeSelector: # 使用节点选择器将Pod调度到指定label的节点 component: mysql containers: - name: mysql image: 172.21.32.6:5000/demo/mysql:5.7 ... nodeAffinity 节点亲和性 ， 比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合，不只是简单的相等匹配 。分为两种，软策略和硬策略。\npreferredDuringSchedulingIgnoredDuringExecution：软策略，如果你没有满足调度要求的节点的话，Pod就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有满足就忽略掉的策略。\nrequiredDuringSchedulingIgnoredDuringExecution ： 硬策略，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不会调度Pod。\n#要求 Pod 不能运行在128和132两个节点上，如果有个节点满足disktype=ssd的话就优先调度到这个节点上 ... spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog ports: - containerPort: 8002 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - 192.168.136.128 - 192.168.136.132 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disktype operator: In values: - ssd - sas ... 这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：\nIn：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod\n污点（Taints）与容忍（tolerations） 对于nodeAffinity无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度Pod。\nTaints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的。于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。\n比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：\n设置污点：\n$ kubectl taint node [node_name] key=value:[effect] 其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ] NoSchedule：一定不能被调度。 PreferNoSchedule：尽量不要调度。 NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。 示例：kubectl taint node k8s-master smoke=true:NoSchedule 去除污点：\n去除指定key及其effect： kubectl taint nodes [node_name] key:[effect]- #这里的key不用指定value 去除指定key所有的effect: kubectl taint nodes node_name key- 示例： kubectl taint node k8s-master smoke=true:NoSchedule kubectl taint node k8s-master smoke:NoExecute- kubectl taint node k8s-master smoke- 污点演示：\n## 给k8s-slave1打上污点，smoke=true:NoSchedule $ kubectl taint node k8s-slave1 smoke=true:NoSchedule $ kubectl taint node k8s-slave2 drunk=true:NoSchedule ## 扩容myblog的Pod，观察新Pod的调度情况 $ kuebctl -n demo scale deploy myblog --replicas=3 $ kubectl -n demo get po -w ## pending Pod容忍污点示例：myblog/deployment/deploy-myblog-taint.yaml\n... spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog tolerations: #设置容忍性 - key: \u0026#34;smoke\u0026#34; operator: \u0026#34;Equal\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; - key: \u0026#34;drunk\u0026#34; operator: \u0026#34;Equal\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; #意思是这个Pod要容忍的有污点的Node的key是smoke Equal true,效果是NoSchedule， #tolerations属性下各值必须使用引号，容忍的值都是设置Node的taints时给的值。 $ kubectl apply -f deploy-myblog-taint.yaml spec: containers: - name: demo image: 172.21.32.6:5000/demo/myblog tolerations: - operator: \u0026#34;Exists\u0026#34; Kubernetes服务访问之Service 录屏！！！ 通过以前的学习，我们已经能够通过Deployment来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两个问题：\nPod IP仅仅是集群内可见的虚拟IP，外部无法访问。 Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。 Service 负载均衡之Cluster IP service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP 。使用Service对象，通过selector进行标签选择，找到对应的Pod:\nmyblog/deployment/svc-myblog.yaml\napiVersion: v1 kind: Service metadata: name: myblog namespace: demo spec: ports: - port: 80 protocol: TCP targetPort: 8002 selector: app: myblog type: ClusterIP 操作演示：\n## 别名 $ alias kd=\u0026#39;kubectl -n demo\u0026#39; ## 创建服务 $ kd create -f svc-myblog.yaml $ kd get po --show-labels NAME READY STATUS RESTARTS AGE LABELS myblog-5c97d79cdb-jn7km 1/1 Running 0 6m5s app=myblog mysql-85f4f65f99-w6jkj 1/1 Running 0 176m app=mysql $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 7m50s $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002 Session Affinity: None Events: \u0026lt;none\u0026gt; ## 扩容myblog服务 $ kd scale deploy myblog --replicas=2 deployment.extensions/myblog scaled ## 再次查看 $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002,10.244.1.158:8002 Session Affinity: None Events: \u0026lt;none\u0026gt; Service与Pod如何关联:\nservice对象创建的同时，会创建同名的endpoints对象，若服务设置了readinessProbe, 当readinessProbe检测失败时，endpoints列表中会剔除掉对应的pod_ip，这样流量就不会分发到健康检测失败的Pod中\n$ kd get endpoints myblog NAME ENDPOINTS AGE myblog 10.244.0.68:8002,10.244.1.158:8002 7m Service Cluster-IP如何访问:\n$ kd get svc myblog NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 13m $ curl 10.99.174.93/blog/index/ 为mysql服务创建service：\napiVersion: v1 kind: Service metadata: name: mysql namespace: demo spec: ports: - port: 3306 protocol: TCP targetPort: 3306 selector: app: mysql type: ClusterIP 访问mysql：\n$ kd get svc mysql mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 3s $ curl 10.108.214.84:3306 目前使用hostNetwork部署，通过宿主机ip+port访问，弊端：\n服务使用hostNetwork，使得宿主机的端口大量暴漏，存在安全隐患 容易引发端口冲突 服务均属于k8s集群，尽可能使用k8s的网络访问，因此可以对目前myblog访问mysql的方式做改造：\n为mysql创建一个固定clusterIp的Service，把clusterIp配置在myblog的环境变量中 利用集群服务发现的能力，组件之间通过service name来访问 服务发现 在k8s集群中，组件之间可以通过定义的Service名称实现通信。\n演示服务发现：\n## 演示思路：在myblog的容器中直接通过service名称访问服务，观察是否可以访问通 # 先查看服务 $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 59m mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 35m # 进入myblog容器 $ kd exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# curl mysql:3306 5.7.29 )→ (mysql_native_password ot packets out of order [root@myblog-5c97d79cdb-j485f myblog]# curl myblog/blog/index/ 我的博客列表 虽然podip和clusterip都不固定，但是service name是固定的，而且具有完全的跨集群可移植性，因此组件之间调用的同时，完全可以通过service name去通信，这样避免了大量的ip维护成本，使得服务的yaml模板更加简单。因此可以对mysql和myblog的部署进行优化改造：\nmysql可以去掉hostNetwork部署，使得服务只暴漏在k8s集群内部网络 configMap中数据库地址可以换成Service名称，这样跨环境的时候，配置内容基本上可以保持不用变化 修改deploy-mysql.yaml\nspec: hostNetwork: true\t# 去掉此行 volumes: - name: mysql-data hostPath: path: /opt/mysql/data 修改configmap.yaml\napiVersion: v1 kind: ConfigMap metadata: name: myblog namespace: demo data: MYSQL_HOST: \u0026#34;mysql\u0026#34;\t# 此处替换为mysql MYSQL_PORT: \u0026#34;3306\u0026#34; 应用修改：\n$ kubectl apply -f configmap.yaml $ kubectl apply -f deploy-mysql.yaml ## 重建pod $ kubectl -n demo delete po mysql-7f747644b8-6npzn #去掉taint $ kubectl taint node k8s-slave1 smoke- $ kubectl taint node k8s-slave2 drunk- ## myblog不用动，会自动因健康检测不过而重启 服务发现实现：\nCoreDNS是一个Go语言实现的链式插件DNS服务端，是CNCF成员，是一个高性能、易扩展的DNS服务端。\n$ kubectl -n kube-system get po -o wide|grep dns coredns-d4475785-2w4hk 1/1 Running 0 4d22h 10.244.0.64 coredns-d4475785-s49hq 1/1 Running 0 4d22h 10.244.0.65 # 查看myblog的pod解析配置 $ kubectl -n demo exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# cat /etc/resolv.conf nameserver 10.96.0.10 search demo.svc.cluster.local svc.cluster.local cluster.local options ndots:5 ## 10.96.0.10 从哪来 $ kubectl -n kube-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 51d ## 启动pod的时候，会把kube-dns服务的cluster-ip地址注入到pod的resolve解析配置中，同时添加对应的namespace的search域。 因此跨namespace通过service name访问的话，需要添加对应的namespace名称， service_name.namespace_name $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 26h Service负载均衡之NodePort cluster-ip为虚拟地址，只能在k8s集群内部进行访问，集群外部如果访问内部服务，实现方式之一为使用NodePort方式。NodePort会默认在 30000-32767 ，不指定的会随机使用其中一个。\nmyblog/deployment/svc-myblog-nodeport.yaml\napiVersion: v1 kind: Service metadata: name: myblog-np namespace: demo spec: ports: - port: 80 protocol: TCP targetPort: 8002 selector: app: myblog type: NodePort 查看并访问服务：\n$ kd create -f svc-myblog-nodeport.yaml service/myblog-np created $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 102m myblog-np NodePort 10.105.228.101 \u0026lt;none\u0026gt; 80:30647/TCP 4s mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 77m #集群内每个节点的NodePort端口都会进行监听 $ curl 192.168.136.128:30647/blog/index/ 我的博客列表 $ curl 192.168.136.131:30647/blog/index/ 我的博客列表 ## 浏览器访问 思考：\nNodePort的端口监听如何转发到对应的Pod服务？\nCLUSTER-IP为虚拟IP，集群内如何通过虚拟IP访问到具体的Pod服务？\nkube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过创建流量路由规则来实现网络的转发。参照\n有三种模式：\nUser space, 让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发 ， 所有报文都走一遍用户态，性能不高，k8s v1.2版本后废弃。 Iptables， 当前默认模式，完全由 IPtables 来实现， 通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。 IPVS， 与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 k8s 1.8版本开始引入，1.11版本开始稳定，需要开启宿主机的ipvs模块。 IPtables模式示意图：\n$ iptables-save |grep -v myblog-np|grep \u0026#34;demo/myblog\u0026#34; -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-WQNGJ7YFZKCTKPZK $ iptables-save |grep KUBE-SVC-WQNGJ7YFZKCTKPZK -A KUBE-SVC-WQNGJ7YFZKCTKPZK -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SVC-WQNGJ7YFZKCTKPZK -j KUBE-SEP-7GWC3FN2JI5KLE47 $ iptables-save |grep KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SEP-GB5GNOM5CZH7ICXZ -p tcp -m tcp -j DNAT --to-destination 10.244.1.158:8002 $ iptables-save |grep KUBE-SEP-7GWC3FN2JI5KLE47 -A KUBE-SEP-7GWC3FN2JI5KLE47 -p tcp -m tcp -j DNAT --to-destination 10.244.1.159:8002 Kubernetes服务访问之Ingress 对于Kubernetes的Service，无论是Cluster-Ip和NodePort均是四层的负载，集群内的服务如何实现七层的负载均衡，这就需要借助于Ingress，Ingress控制器的实现方式有很多，比如nginx, Contour, Haproxy, trafik, Istio，我们以nginx的实现为例做演示。\nIngress-nginx是7层的负载均衡器 ，负责统一管理外部对k8s cluster中service的请求。主要包含：\ningress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）； ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件） 示意图： 实现逻辑 1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化 2）然后读取ingress规则(规则就是写明了哪个域名对应哪个service)，按照自定义的规则，生成一段nginx配置 3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器把生成的nginx配置写入/etc/nginx.conf文件中 4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。\n安装 官方文档\n$ wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml ## 或者使用myblog/deployment/ingress/mandatory.yaml ## 修改部署节点 $ grep -n5 nodeSelector mandatory.yaml 212- spec: 213- hostNetwork: true #添加为host模式 214- # wait up to five minutes for the drain of connections 215- terminationGracePeriodSeconds: 300 216- serviceAccountName: nginx-ingress-serviceaccount 217: nodeSelector: 218- ingress: \u0026#34;true\u0026#34;\t#替换此处，来决定将ingress部署在哪些机器 219- containers: 220- - name: nginx-ingress-controller 221- image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 222- args: 使用示例：myblog/deployment/ingress.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: myblog namespace: demo spec: rules: - host: myblog.devops.cn http: paths: - path: / backend: serviceName: myblog servicePort: 80 ingress-nginx动态生成upstream配置：\n... server_name myblog.devops.cn ; listen 80 ; listen [::]:80 ; listen 443 ssl http2 ; listen [::]:443 ssl http2 ; set $proxy_upstream_name \u0026#34;-\u0026#34;; ssl_certificate_by_lua_block { certificate.call() } location / { set $namespace \u0026#34;demo\u0026#34;; set $ingress_name \u0026#34;myblog\u0026#34;; ... 访问 域名解析服务，将 myblog.devops.cn解析到ingress的地址上。ingress是支持多副本的，高可用的情况下，生产的配置是使用lb服务（内网F5设备，公网elb、slb、clb，解析到各ingress的机器，如何域名指向lb地址）\n本机，添加如下hosts记录来演示效果。\n192.168.136.128 myblog.devops.cn 然后，访问 http://myblog.devops.cn/blog/index/\nHTTPS访问：\n#自签名证书 $ openssl req -x509 -nodes -days 2920 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=*.devops.cn/O=ingress-nginx\u0026#34; # 证书信息保存到secret对象中，ingress-nginx会读取secret对象解析出证书加载到nginx配置中 $ kubectl -n demo create secret tls https-secret --key tls.key --cert tls.crt secret/https-secret created 修改yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: myblog-tls namespace: demo spec: rules: - host: myblog.devops.cn http: paths: - path: / backend: serviceName: myblog servicePort: 80 tls: - hosts: - myblog.devops.cn secretName: https-secret 然后，访问 https://myblog.devops.cn/blog/index/\nKubernetes认证与授权 录屏！！！ APIService安全控制 Authentication：身份认证\n这个环节它面对的输入是整个http request，负责对来自client的请求进行身份校验，支持的方法包括: client证书验证（https双向验证）\nbasic auth\n普通token\njwt token(用于serviceaccount)\nAPIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证， 只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功；\n使用kubeadm引导启动的k8s集群的apiserver初始配置中，默认支持client证书验证和serviceaccount两种身份验证方式。 证书认证通过设置--client-ca-file根证书以及--tls-cert-file和--tls-private-key-file来开启。\n在这个环节，apiserver会通过client证书或 http header中的字段(比如serviceaccount的jwt token)来识别出请求的用户身份，包括”user”、”group”等，这些信息将在后面的authorization环节用到。\nAuthorization：鉴权，你可以访问哪些资源\n这个环节面对的输入是http request context中的各种属性，包括：user、group、request path（比如：/api/v1、/healthz、/version等）、 request verb(比如：get、list、create等)。\nAPIServer会将这些属性值与事先配置好的访问策略(access policy）相比较。APIServer支持多种authorization mode，包括Node、RBAC、Webhook等。\nAPIServer启动时，可以指定一种authorization mode，也可以指定多种authorization mode，如果是后者，只要Request通过了其中一种mode的授权， 那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，authorization-mode的默认配置是”Node,RBAC”。\nAdmission Control：准入控制，一个控制链(层层关卡)，偏集群安全控制、管理方面。为什么说是安全相关的机制？\n以NamespaceLifecycle为例， 该插件确保处于Termination状态的Namespace不再接收新的对象创建请求，并拒绝请求不存在的Namespace。该插件还可以防止删除系统保留的Namespace:default，kube-system，kube-public。 NodeRestriction， 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制 。 为什么我们执行命令kubectl命令，可以直接管理k8s集群资源？\nkubectl的认证授权 kubectl的日志调试级别：\n信息 描述 v=0 通常，这对操作者来说总是可见的。 v=1 当您不想要很详细的输出时，这个是一个合理的默认日志级别。 v=2 有关服务和重要日志消息的有用稳定状态信息，这些信息可能与系统中的重大更改相关。这是大多数系统推荐的默认日志级别。 v=3 关于更改的扩展信息。 v=4 调试级别信息。 v=6 显示请求资源。 v=7 显示 HTTP 请求头。 v=8 显示 HTTP 请求内容。 v=9 显示 HTTP 请求内容，并且不截断内容。 $ kubectl get nodes -v=7 I0329 20:20:08.633065 3979 loader.go:359] Config loaded from file /root/.kube/config I0329 20:20:08.633797 3979 round_trippers.go:416] GET https://192.168.136.128:6443/api/v1/nodes?limit=500 kubeadm init启动完master节点后，会默认输出类似下面的提示内容：\n... ... Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ... ... 这些信息是在告知我们如何配置kubeconfig文件。按照上述命令配置后，master节点上的kubectl就可以直接使用$HOME/.kube/config的信息访问k8s cluster了。 并且，通过这种配置方式，kubectl也拥有了整个集群的管理员(root)权限。\n很多K8s初学者在这里都会有疑问：\n当kubectl使用这种kubeconfig方式访问集群时，Kubernetes的kube-apiserver是如何对来自kubectl的访问进行身份验证(authentication)和授权(authorization)的呢？ 为什么来自kubectl的请求拥有最高的管理员权限呢？ 查看/root/.kube/config文件：\n前面提到过apiserver的authentication支持通过tls client certificate、basic auth、token等方式对客户端发起的请求进行身份校验， 从kubeconfig信息来看，kubectl显然在请求中使用了tls client certificate的方式，即客户端的证书。\n证书base64解码：\n$ echo xxxxxxxxxxxxxx |base64 -d \u0026gt; kubectl.crt 说明在认证阶段，apiserver会首先使用--client-ca-file配置的CA证书去验证kubectl提供的证书的有效性,基本的方式 ：\n$ openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt kubectl.crt: OK 除了认证身份，还会取出必要的信息供授权阶段使用，文本形式查看证书内容：\n$ openssl x509 -in kubectl.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 4736260165981664452 (0x41ba9386f52b74c4) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Feb 10 07:33:39 2020 GMT Not After : Feb 9 07:33:40 2021 GMT Subject: O=system:masters, CN=kubernetes-admin ... 认证通过后，提取出签发证书时指定的CN(Common Name),kubernetes-admin，作为请求的用户名 (User Name), 从证书中提取O(Organization)字段作为请求用户所属的组 (Group)，group = system:masters，然后传递给后面的授权模块。\nkubeadm在init初始引导集群启动过程中，创建了许多default的role、clusterrole、rolebinding和clusterrolebinding， 在k8s有关RBAC的官方文档中，我们看到下面一些default clusterrole列表:\n其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。 沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。\n我们查看一下这一binding：\n$ kubectl describe clusterrolebinding cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true Role: Kind: ClusterRole Name: cluster-admin Subjects: Kind Name Namespace ---- ---- --------- Group system:masters 我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起， 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。\n我们再来查看一下cluster-admin这个role的具体权限信息：\n$ kubectl describe clusterrole cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.* [] [] [*] [*] [] [*] 非资源类，如查看集群健康状态。\nRBAC Role-Based Access Control，基于角色的访问控制， apiserver启动参数添加\u0026ndash;authorization-mode=RBAC 来启用RBAC认证模式，kubeadm安装的集群默认已开启。官方介绍\n查看开启：\n# master节点查看apiserver进程 $ ps aux |grep apiserver RBAC模式引入了4个资源：\nRole，角色\n一个Role只能授权访问单个namespace\n## 示例定义一个名为pod-reader的角色，该角色具有读取default这个命名空间下的pods的权限 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] ## apiGroups: \u0026#34;\u0026#34;,\u0026#34;apps\u0026#34;, \u0026#34;autoscaling\u0026#34;, \u0026#34;batch\u0026#34;, kubectl api-versions ## resources: \u0026#34;services\u0026#34;, \u0026#34;pods\u0026#34;,\u0026#34;deployments\u0026#34;... kubectl api-resources ## verbs: \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;exec\u0026#34; ClusterRole\n一个ClusterRole能够授予和Role一样的权限，但是它是集群范围内的。\n## 定义一个集群角色，名为secret-reader，该角色可以读取所有的namespace中的secret资源 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: # \u0026#34;namespace\u0026#34; omitted since ClusterRoles are not namespaced name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] Rolebinding\n将role中定义的权限分配给用户和用户组。RoleBinding包含主题（users,groups,或service accounts）和授予角色的引用。对于namespace内的授权使用RoleBinding，集群范围内使用ClusterRoleBinding。\n## 定义一个角色绑定，将pod-reader这个role的权限授予给jane这个User，使得jane可以在读取default这个命名空间下的所有的pod数据 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: read-pods namespace: default subjects: - kind: User #这里可以是User,Group,ServiceAccount name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role #这里可以是Role或者ClusterRole,若是ClusterRole，则权限也仅限于rolebinding的内部 name: pod-reader # match the name of the Role or ClusterRole you wish to bind to apiGroup: rbac.authorization.k8s.io 注意：rolebinding既可以绑定role，也可以绑定clusterrole，当绑定clusterrole的时候，subject的权限也会被限定于rolebinding定义的namespace内部，若想跨namespace，需要使用clusterrolebinding\n## 定义一个角色绑定，将dave这个用户和secret-reader这个集群角色绑定，虽然secret-reader是集群角色，但是因为是使用rolebinding绑定的，因此dave的权限也会被限制在development这个命名空间内 apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \u0026#34;dave\u0026#34; to read secrets in the \u0026#34;development\u0026#34; namespace. # You need to already have a ClusterRole named \u0026#34;secret-reader\u0026#34;. kind: RoleBinding metadata: name: read-secrets # # The namespace of the RoleBinding determines where the permissions are granted. # This only grants permissions within the \u0026#34;development\u0026#34; namespace. namespace: development subjects: - kind: User name: dave # Name is case sensitive apiGroup: rbac.authorization.k8s.io - kind: ServiceAccount name: dave # Name is case sensitive namespace: demo roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 考虑一个场景： 如果集群中有多个namespace分配给不同的管理员，每个namespace的权限是一样的，就可以只定义一个clusterrole，然后通过rolebinding将不同的namespace绑定到管理员身上，否则就需要每个namespace定义一个Role，然后做一次rolebinding。\nClusterRolebingding\n允许跨namespace进行授权\napiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows anyone in the \u0026#34;manager\u0026#34; group to read secrets in any namespace. kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: Group name: manager # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io kubelet的认证授权 查看kubelet进程\n$ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2020-04-01 02:34:13 CST; 1 day 14h ago Docs: https://kubernetes.io/docs/ Main PID: 851 (kubelet) Tasks: 21 Memory: 127.1M CGroup: /system.slice/kubelet.service └─851 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf 查看/etc/kubernetes/kubelet.conf，解析证书：\n$ echo xxxxx |base64 -d \u0026gt;kubelet.crt $ openssl x509 -in kubelet.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 9059794385454520113 (0x7dbadafe23185731) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Feb 10 07:33:39 2020 GMT Not After : Feb 9 07:33:40 2021 GMT Subject: O=system:nodes, CN=system:node:master-1 得到我们期望的内容：\nSubject: O=system:nodes, CN=system:node:k8s-master 我们知道，k8s会把O作为Group来进行请求，因此如果有权限绑定给这个组，肯定在clusterrolebinding的定义中可以找得到。因此尝试去找一下绑定了system:nodes组的clusterrolebinding\n$ kubectl get clusterrolebinding|awk \u0026#39;NR\u0026gt;1{print $1}\u0026#39;|xargs kubectl get clusterrolebinding -oyaml|grep -n10 system:nodes 98- roleRef: 99- apiGroup: rbac.authorization.k8s.io 100- kind: ClusterRole 101- name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 102- subjects: 103- - apiGroup: rbac.authorization.k8s.io 104- kind: Group 105: name: system:nodes 106-- apiVersion: rbac.authorization.k8s.io/v1 107- kind: ClusterRoleBinding 108- metadata: 109- creationTimestamp: \u0026#34;2020-02-10T07:34:02Z\u0026#34; 110- name: kubeadm:node-proxier 111- resourceVersion: \u0026#34;213\u0026#34; 112- selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier $ kubectl describe clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeclient [] [] [create] 结局有点意外，除了system:certificates.k8s.io:certificatesigningrequests:selfnodeclient外，没有找到system相关的rolebindings，显然和我们的理解不一样。 尝试去找资料，发现了这么一段 :\nDefault ClusterRole Default ClusterRoleBinding Description system:kube-scheduler system:kube-scheduler user Allows access to the resources required by the schedulercomponent. system:volume-scheduler system:kube-scheduler user Allows access to the volume resources required by the kube-scheduler component. system:kube-controller-manager system:kube-controller-manager user Allows access to the resources required by the controller manager component. The permissions required by individual controllers are detailed in the controller roles. system:node None Allows access to resources required by the kubelet, including read access to all secrets, and write access to all pod status objects. You should use the Node authorizer and NodeRestriction admission plugin instead of the system:node role, and allow granting API access to kubelets based on the Pods scheduled to run on them. The system:node role only exists for compatibility with Kubernetes clusters upgraded from versions prior to v1.8. system:node-proxier system:kube-proxy user Allows access to the resources required by the kube-proxycomponent. 大致意思是说：之前会定义system:node这个角色，目的是为了kubelet可以访问到必要的资源，包括所有secret的读权限及更新pod状态的写权限。如果1.8版本后，是建议使用 Node authorizer and NodeRestriction admission plugin 来代替这个角色的。\n我们目前使用1.16，查看一下授权策略：\n$ ps axu|grep apiserver kube-apiserver --authorization-mode=Node,RBAC --enable-admission-plugins=NodeRestriction 查看一下官网对Node authorizer的介绍：\nNode authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.\nIn future releases, the node authorizer may add or remove permissions to ensure kubelets have the minimal set of permissions required to operate correctly.\nIn order to be authorized by the Node authorizer, kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;\nService Account 前面说，认证可以通过证书，也可以通过使用ServiceAccount（服务账户）的方式来做认证。大多数时候，我们在基于k8s做二次开发时都是选择通过serviceaccount的方式。我们之前访问dashboard的时候，是如何做的？\n## 新建一个名为admin的serviceaccount，并且把名为cluster-admin的这个集群角色的权限授予新建的serviceaccount apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kubernetes-dashboard --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: admin annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kubernetes-dashboard 我们查看一下：\n$ kubectl -n kubernetes-dashboard get sa admin -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2020-04-01T11:59:21Z\u0026#34; name: admin namespace: kubernetes-dashboard resourceVersion: \u0026#34;1988878\u0026#34; selfLink: /api/v1/namespaces/kubernetes-dashboard/serviceaccounts/admin uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f secrets: - name: admin-token-lfsrf 注意到serviceaccount上默认绑定了一个名为admin-token-lfsrf的secret，我们查看一下secret\n$ kubectl -n kubernetes-dashboard describe secret admin-token-lfsrf Name: admin-token-lfsrf Namespace: kubernetes-dashboard Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin kubernetes.io/service-account.uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 4 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A 开发的时候如何去调用k8s的api:\ncurl演示 $ curl -k -H \u0026#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A\u0026#34; https://62.234.214.206:6443/api/v1/namespaces/demo/pods?limit=500 postman 查看etcd数据 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看所有key值：\n$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get / --prefix --keys-only 查看具体的key对应的数据：\n$ ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb 基于EFK实现kubernetes集群的日志平台（扩展） 录屏！！！ EFK介绍 EFK工作示意\nElasticsearch\n一个开源的分布式、Restful 风格的搜索和数据分析引擎，它的底层是开源库Apache Lucene。它可以被下面这样准确地形容：\n一个分布式的实时文档存储，每个字段可以被索引与搜索； 一个分布式实时分析搜索引擎； 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。 Fluentd\n一个针对日志的收集、处理、转发系统。通过丰富的插件系统，可以收集来自于各种系统或应用的日志，转化为用户指定的格式后，转发到用户所指定的日志存储系统之中。\nFluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储、kafka等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下\n首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务 Kibana\nKibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。可以通过Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。也可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。\n部署es服务 部署分析 es生产环境是部署es集群，通常会使用statefulset进行部署，此例由于演示环境资源问题，部署为单点 数据存储挂载主机路径 es默认使用elasticsearch用户启动进程，es的数据目录是通过宿主机的路径挂载，因此目录权限被主机的目录权限覆盖，因此可以利用init container容器在es进程启动之前把目录的权限修改掉，注意init container要用特权模式启动。 部署并验证 efk/elasticsearch.yaml\napiVersion: apps/v1 kind: StatefulSet metadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: elasticsearch-logging version: v7.4.2 name: elasticsearch-logging namespace: logging spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: elasticsearch-logging version: v7.4.2 serviceName: elasticsearch-logging template: metadata: labels: k8s-app: elasticsearch-logging version: v7.4.2 spec: nodeSelector: log: \u0026#34;true\u0026#34;\t## 指定部署在哪个节点。需根据环境来修改 containers: - env: - name: NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: cluster.initial_master_nodes value: elasticsearch-logging-0 - name: ES_JAVA_OPTS value: \u0026#34;-Xms512m -Xmx512m\u0026#34; image: 172.21.32.6:5000/elasticsearch/elasticsearch:7.4.2 name: elasticsearch-logging ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-logging dnsConfig: options: - name: single-request-reopen initContainers: - command: - /sbin/sysctl - -w - vm.max_map_count=262144 image: alpine:3.6 imagePullPolicy: IfNotPresent name: elasticsearch-logging-init resources: {} securityContext: privileged: true - name: fix-permissions image: alpine:3.6 command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026#34;] securityContext: privileged: true volumeMounts: - name: elasticsearch-logging mountPath: /usr/share/elasticsearch/data volumes: - name: elasticsearch-logging hostPath: path: /esdata --- apiVersion: v1 kind: Service metadata: labels: k8s-app: elasticsearch-logging name: elasticsearch namespace: logging spec: ports: - port: 9200 protocol: TCP targetPort: db selector: k8s-app: elasticsearch-logging type: ClusterIP $ kubectl create namespace logging ## 给slave1节点打上label，将es服务调度到slave1节点 $ kubectl label node k8s-slave1 log=true ## 部署服务，可以先去部署es的节点把镜像下载到本地 $ kubectl create -f elasticsearch.yaml statefulset.apps/elasticsearch-logging created service/elasticsearch created ## 等待片刻，查看一下es的pod部署到了k8s-slave1节点，状态变为running $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE IP NODE elasticsearch-logging-0 1/1 Running 0 69m 10.244.1.104 k8s-slave1 # 然后通过curl命令访问一下服务，验证es是否部署成功 $ kubectl -n logging get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch ClusterIP 10.109.174.58 \u0026lt;none\u0026gt; 9200/TCP 71m $ curl 10.109.174.58:9200 { \u0026#34;name\u0026#34; : \u0026#34;elasticsearch-logging-0\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;docker-cluster\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;uic8xOyNSlGwvoY9DIBT1g\u0026#34;, \u0026#34;version\u0026#34; : { \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;, \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;, \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;, \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;, \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;, \u0026#34;build_snapshot\u0026#34; : false, \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;, \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;, \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34; }, \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; } 部署kibana 部署分析 kibana需要暴漏web页面给前端使用，因此使用ingress配置域名来实现对kibana的访问 kibana为无状态应用，直接使用Deployment来启动 kibana需要访问es，直接利用k8s服务发现访问此地址即可，http://elasticsearch:9200 部署并验证 资源文件 efk/kibana.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: logging labels: app: kibana spec: selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: 172.21.32.6:5000/kibana/kibana:7.4.2 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 --- apiVersion: v1 kind: Service metadata: name: kibana namespace: logging labels: app: kibana spec: ports: - port: 5601 protocol: TCP targetPort: 5601 type: ClusterIP selector: app: kibana --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: logging spec: rules: - host: kibana.devops.cn http: paths: - path: / backend: serviceName: kibana servicePort: 5601 $ kubectl create -f kibana.yaml deployment.apps/kibana created service/kibana created ingress/kibana created # 然后查看pod，等待状态变成running $ kubectl -n logging get po NAME READY STATUS RESTARTS AGE elasticsearch-logging-0 1/1 Running 0 88m kibana-944c57766-ftlcw 1/1 Running 0 15m ## 配置域名解析 kibana.devops.cn，并访问服务进行验证，若可以访问，说明连接es成功 部署fluentd 部署分析 fluentd为日志采集服务，kubernetes集群的每个业务节点都有日志产生，因此需要使用daemonset的模式进行部署 为进一步控制资源，会为daemonset指定一个选择表情，fluentd=true来做进一步过滤，只有带有此标签的节点才会部署fluentd 日志采集，需要采集哪些目录下的日志，采集后发送到es端，因此需要配置的内容比较多，我们选择使用configmap的方式把配置文件整个挂载出来 部署服务 配置文件，efk/fluentd-es-main.yaml\napiVersion: v1 data: fluent.conf: |- # This is the root config file, which only includes components of the actual configuration # # Do not collect fluentd\u0026#39;s own logs to avoid infinite loops. \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; @include /fluentd/etc/config.d/*.conf kind: ConfigMap metadata: labels: addonmanager.kubernetes.io/mode: Reconcile name: fluentd-es-config-main namespace: logging 配置文件，fluentd-config.yaml，注意点：\n数据源source的配置，k8s会默认把容器的标准和错误输出日志重定向到宿主机中 默认集成了 kubernetes_metadata_filter 插件，来解析日志格式，得到k8s相关的元数据，raw.kubernetes match输出到es端的flush配置 kind: ConfigMap apiVersion: v1 metadata: name: fluentd-config namespace: logging labels: addonmanager.kubernetes.io/mode: Reconcile data: system.conf: |- \u0026lt;system\u0026gt; root_dir /tmp/fluentd-buffers/ \u0026lt;/system\u0026gt; containers.input.conf: |- \u0026lt;source\u0026gt; @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos time_format %Y-%m-%dT%H:%M:%S.%NZ localtime tag raw.kubernetes.* format json read_from_head true \u0026lt;/source\u0026gt; # Detect exceptions in the log output and forward them as one log entry. \u0026lt;match raw.kubernetes.**\u0026gt; @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 \u0026lt;/match\u0026gt; forward.input.conf: |- # Takes the messages sent over TCP \u0026lt;source\u0026gt; @type forward \u0026lt;/source\u0026gt; output.conf: |- # Enriches records with Kubernetes metadata \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata \u0026lt;/filter\u0026gt; \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true host elasticsearch port 9200 logstash_format true request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; daemonset定义文件，fluentd.yaml，注意点：\n需要配置rbac规则，因为需要访问k8s api去根据日志查询元数据 需要将/var/log/containers/目录挂载到容器中 需要将fluentd的configmap中的配置文件挂载到容器内 想要部署fluentd的节点，需要添加fluentd=true的标签 efk/fluentd.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: logging labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \u0026#34;\u0026#34; resources: - \u0026#34;namespaces\u0026#34; - \u0026#34;pods\u0026#34; verbs: - \u0026#34;get\u0026#34; - \u0026#34;watch\u0026#34; - \u0026#34;list\u0026#34; --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: \u0026#34;true\u0026#34; addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: logging apiGroup: \u0026#34;\u0026#34; roleRef: kind: ClusterRole name: fluentd-es apiGroup: \u0026#34;\u0026#34; --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: fluentd-es name: fluentd-es namespace: logging spec: selector: matchLabels: k8s-app: fluentd-es template: metadata: labels: k8s-app: fluentd-es spec: containers: - env: - name: FLUENTD_ARGS value: --no-supervisor -q image: 172.21.32.6:5000/fluentd-es-root:v1.6.2-1.0 imagePullPolicy: IfNotPresent name: fluentd-es resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - mountPath: /var/log name: varlog - mountPath: /var/lib/docker/containers name: varlibdockercontainers readOnly: true - mountPath: /home/docker/containers name: varlibdockercontainershome readOnly: true - mountPath: /fluentd/etc/config.d name: config-volume - mountPath: /fluentd/etc/fluent.conf name: config-volume-main subPath: fluent.conf nodeSelector: fluentd: \u0026#34;true\u0026#34; securityContext: {} serviceAccount: fluentd-es serviceAccountName: fluentd-es volumes: - hostPath: path: /var/log type: \u0026#34;\u0026#34; name: varlog - hostPath: path: /var/lib/docker/containers type: \u0026#34;\u0026#34; name: varlibdockercontainers - hostPath: path: /home/docker/containers type: \u0026#34;\u0026#34; name: varlibdockercontainershome - configMap: defaultMode: 420 name: fluentd-config name: config-volume - configMap: defaultMode: 420 items: - key: fluent.conf path: fluent.conf name: fluentd-es-config-main name: config-volume-main ## 给slave1和slave2打上标签，进行部署fluentd日志采集服务 $ kubectl label node k8s-slave1 fluentd=true node/k8s-slave1 labeled $ kubectl label node k8s-slave2 fluentd=true node/k8s-slave2 labeled # 创建服务 $ kubectl create -f fluentd-es-config-main.yaml configmap/fluentd-es-config-main created $ kubectl create -f fluentd-configmap.yaml configmap/fluentd-config created $ kubectl create -f fluentd.yaml serviceaccount/fluentd-es created clusterrole.rbac.authorization.k8s.io/fluentd-es created clusterrolebinding.rbac.authorization.k8s.io/fluentd-es created daemonset.extensions/fluentd-es created ## 然后查看一下pod是否已经在k8s-slave1和k8s-slave2节点启动成功 $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE elasticsearch-logging-0 1/1 Running 0 123m fluentd-es-246pl 1/1 Running 0 2m2s fluentd-es-4e21w 1/1 Running 0 2m10s kibana-944c57766-ftlcw 1/1 Running 0 50m EFK功能验证 验证思路 k8s-slave1和slave2中启动服务，同时往标准输出中打印测试日志，到kibana中查看是否可以收集\n创建测试容器 apiVersion: v1 kind: Pod metadata: name: counter spec: nodeSelector: fluentd: \u0026#34;true\u0026#34; containers: - name: count image: alpine:3.6 args: [/bin/sh, -c, \u0026#39;i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34;; i=$((i+1)); sleep 1; done\u0026#39;] $ kubectl get po NAME READY STATUS RESTARTS AGE counter 1/1 Running 0 6s 配置kibana 登录kibana界面，按照截图的顺序操作：\n也可以通过其他元数据来过滤日志数据，比如可以单击任何日志条目以查看其他元数据，如容器名称，Kubernetes 节点，命名空间等，比如kubernetes.pod_name : counter\n到这里，我们就在 Kubernetes 集群上成功部署了 EFK ，要了解如何使用 Kibana 进行日志数据分析，可以参考 Kibana 用户指南文档：https://www.elastic.co/guide/en/kibana/current/index.html\n","permalink":"https://wandong1.github.io/post/%E5%9F%BA%E4%BA%8Edocker%E5%92%8Ckubernetes%E7%9A%84%E4%BC%81%E4%B8%9A%E7%BA%A7devops%E5%AE%9E%E8%B7%B5%E8%AE%AD%E7%BB%83%E8%90%A5/","summary":"基于Docker和Kubernetes的企业级DevOps实践训练营 课程准备 离线镜像包\n百度：https://pan.baidu.com/s/1N1AYGCYftYGn6L0QPMWIMw 提取码：ev2h\n天翼云：https://cloud.189.cn/t/ENjUbmRR7FNz\nCentOS7.4版本以上 虚拟机3台（4C+8G+50G），内网互通，可连外网\n课件文档\n《训练营课件》 《安装手册》 git仓库\nhttps://gitee.com/agagin/python-demo.git python demo项目\nhttps://gitee.com/agagin/demo-resources.git demo项目演示需要的资源文件\n关于本人 李永信\n2012-2017，云平台开发工程师，先后对接过Vmware、OpenStack、Docker平台\n2017-2019， 运维开发工程师，Docker+Kubernetes的Paas平台运维开发\n2019至今，DevOps工程师\n8年多的时间，积攒了一定的开发和运维经验，跟大家分享。\n课程安排 2020.4.11 Docker + kubernetes\n2020.4.18 DevOps平台实践\n2天的时间，节奏会相对快一些\n小调研：\nA : 只听过docker，几乎没有docker的使用经验 B：有一定的docker实践经验，不熟悉或者几乎没用过k8s C：对于docker和k8s都有一定的实践经验，想更多了解如何基于docker+k8s构建devops平台 D：其他 课程介绍 最近的三年多时间，关注容器圈的话应该会知道这么几个事情：\n容器技术持续火爆\nKubernetes(k8s)成为容器编排管理的标准\n国内外厂商均已开始了全面拥抱Kubernetes的转型， 无数中小型企业已经落地 Kubernetes，或正走落地的道路上 。基于目前的发展趋势可以预见，未来几年以kubernetes平台为核心的容器运维管理、DevOps等将迎来全面的发展。\n本着实践为核心的思想，本课程使用企业常见的基于Django + uwsgi + Nginx架构的Python Demo项目，分别讲述三个事情：\n项目的容器化\n教大家如何把公司的项目做成容器，并且运行在docker环境中\n使用Kubernetes集群来管理容器化的项目\n带大家一步一步部署k8s集群，并把容器化后的demo项目使用k8s来管理起来\n使用Jenkins和Kubernetes集成，实现demo项目的持续集成/持续交付(CI/CD)\n会使用k8s管理应用生命周期后，还差最后的环节，就是如何把开发、测试、部署的流程使用自动化工具整合起来，最后一部分呢，课程会教会大家如何优雅的使用gitlab+Jenkins+k8s构建企业级的DevOps平台\n流程示意 你将学到哪些 Docker相关\n如何使用Dockerfile快速构建镜像 Docker镜像、容器、仓库的常用操作 Docker容器的网络（Bridge下的SNAT、DNAT） Kubernetes相关\n集群的快速搭建 kubernetes的架构及工作流程 使用Pod控制器管理业务应用的生命周期 使用CoreDNS、Service和Ingress实现服务发现、负载均衡及四层、七层网络的访问 Kubernetes的认证授权体系 使用EFK构建集群业务应用的日志收集系统","title":""},{"content":"创建服务 docker volume create jenkins-data docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home jenkins/jenkins 查看首次登录密钥 docker logs jenkins # 或者 cat /var/jenkins_home/secrets/initialAdminPassword 修改国内插件源 # 进入jenkins_home/updates目录 sed -i \u0026#39;s/http:\\/\\/updates.jenkins.ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json 管理Jenkins-\u0026gt;系统配置\u0026ndash;\u0026gt;管理插件\u0026ndash;\u0026gt;分别搜索Git Parameter/Git/Pipeline/kubernetes/Config File Provider， 选中点击安装。\n• Git：拉取代码\n• Git Parameter：Git参数化构建\n• Pipeline：流水线\n• kubernetes：连接Kubernetes动态创建Slave代理\n• Config File Provider：存储配置文件\n• Extended Choice Parameter：扩展选择框参数，支持多选\npipeline { agent any stages { stage(\u0026#39;pull code\u0026#39;) { steps { git credentialsId: \u0026#39;fd53dd28-a24f-48ce-b6a3-edaefef0c61a\u0026#39;, url: \u0026#39;https://github.com/wandong1/wandong1.github.io.git\u0026#39; } } stage(\u0026#39;print hello\u0026#39;) { steps { sh \u0026#39;echo \u0026#34;hello world\u0026#34;\u0026#39; } } } } ","permalink":"https://wandong1.github.io/post/docker%E9%83%A8%E7%BD%B2jenkins/","summary":"创建服务 docker volume create jenkins-data docker run -d --name jenkins -p 80:8080 -p 50000:50000 -v jenkins-data:/var/jenkins_home jenkins/jenkins 查看首次登录密钥 docker logs jenkins # 或者 cat /var/jenkins_home/secrets/initialAdminPassword 修改国内插件源 # 进入jenkins_home/updates目录 sed -i \u0026#39;s/http:\\/\\/updates.jenkins.ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json 管理Jenkins-\u0026gt;系统配置\u0026ndash;\u0026gt;管理插件\u0026ndash;\u0026gt;分别搜索Git Parameter/Git/Pipeline/kubernetes/Config File Provider， 选中点击安装。\n• Git：拉取代码\n• Git Parameter：Git参数化构建\n• Pipeline：流水线\n• kubernetes：连接Kubernetes动态创建Slave代理\n• Config File Provider：存储配置文件\n• Extended Choice Parameter：扩展选择框参数，支持多选\npipeline { agent any stages { stage(\u0026#39;pull code\u0026#39;) { steps { git credentialsId: \u0026#39;fd53dd28-a24f-48ce-b6a3-edaefef0c61a\u0026#39;, url: \u0026#39;https://github.","title":"docker部署jenkins"},{"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment\n","permalink":"https://wandong1.github.io/post/hello-world/","summary":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment","title":"Hello World"},{"content":"安装工具 指定pip源安装\npip install rdbtools -i https://mirrors.aliyun.com/pypi/simple/ pip install python-lzf -i https://mirrors.aliyun.com/pypi/simple/ 生成内存报告 rdb -c memory dump.rdb \u0026gt; redis_memory_report.csv\n详细参考：https://www.cnblogs.com/xingxia/p/redis_rdb_tools.html\n","permalink":"https://wandong1.github.io/post/redis%E5%B7%A5%E5%85%B7%E4%B9%8Bredis_rdb_tools/","summary":"安装工具 指定pip源安装\npip install rdbtools -i https://mirrors.aliyun.com/pypi/simple/ pip install python-lzf -i https://mirrors.aliyun.com/pypi/simple/ 生成内存报告 rdb -c memory dump.rdb \u0026gt; redis_memory_report.csv\n详细参考：https://www.cnblogs.com/xingxia/p/redis_rdb_tools.html","title":"redis工具之redis_rdb_tools"},{"content":"tcpdump -i bond0 tcp and port 3029 and host 10.42.23.141 -n -nn -vvv -w redis2.cap data and tcp.dstport == 3034\nimport dpkt import socket from openpyxl import Workbook def analysis_of_redis_cap(cap_file,redis_port): wb = Workbook() ws = wb.active table_hed = [\u0026#39;源地址\u0026#39;,\u0026#39;目的地址\u0026#39;,\u0026#39;redis请求命令\u0026#39;] ws.append(table_hed) with open(cap_file,\u0026#39;rb\u0026#39;) as f: string_data = dpkt.pcap.Reader(f) for ts, buf in string_data: eth = dpkt.ethernet.Ethernet(buf) ip = eth.data tcp = ip.data # print(tcp.dport) if tcp.dport == int(redis_port): try: data_pre = tcp.data.decode(\u0026#39;utf-8\u0026#39;) data_pre = data_pre.split(\u0026#39;\\r\\n\u0026#39;) except Exception as res: data_pre = \u0026#39;\u0026#39; request_info = \u0026#39;\u0026#39; for i in data_pre: if i.startswith(\u0026#39;$\u0026#39;): pass else: request_info += i + \u0026#39; \u0026#39; if len(request_info) \u0026gt; 1: # print(socket.inet_ntoa(ip.src),tcp.sport) src_ip = \u0026#39;{}:{}\u0026#39;.format(socket.inet_ntoa(ip.src),tcp.sport) dest_ip = \u0026#39;{}:{}\u0026#39;.format(socket.inet_ntoa(ip.dst),tcp.dport) lst = [src_ip, dest_ip, request_info] ws.append(lst) # print(request_info) wb.save(\u0026#39;redis_{}查询请求.xlsx\u0026#39;.format(redis_port)) if __name__ == \u0026#39;__main__\u0026#39;: cap_file = r\u0026#39;redis3028.cap\u0026#39; analysis_of_redis_cap(cap_file,3028) ","permalink":"https://wandong1.github.io/post/redis%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90/","summary":"tcpdump -i bond0 tcp and port 3029 and host 10.42.23.141 -n -nn -vvv -w redis2.cap data and tcp.dstport == 3034\nimport dpkt import socket from openpyxl import Workbook def analysis_of_redis_cap(cap_file,redis_port): wb = Workbook() ws = wb.active table_hed = [\u0026#39;源地址\u0026#39;,\u0026#39;目的地址\u0026#39;,\u0026#39;redis请求命令\u0026#39;] ws.append(table_hed) with open(cap_file,\u0026#39;rb\u0026#39;) as f: string_data = dpkt.pcap.Reader(f) for ts, buf in string_data: eth = dpkt.ethernet.Ethernet(buf) ip = eth.data tcp = ip.data # print(tcp.dport) if tcp.dport == int(redis_port): try: data_pre = tcp.","title":"redis抓包分析"},{"content":"概述 Harbor是由VMWare公司开源的容器镜像仓库。事实上，Harbor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制，AD/LDAP集成以及审计日志等，足以满足基本企业需求。 官方：https://goharbor.io/ Github：https://github.com/goharbor/harbor\n部署先决条件 服务器硬件配置： •最低要求：CPU2核/内存4G/硬盘40GB •推荐：CPU4核/内存8G/硬盘160GB 软件： •Docker CE 17.06版本+ •Docker Compose1.18版本+ Harbor安装有2种方式： •在线安装：从Docker Hub下载Harbor相关镜像，因此安装软件包非常小 •离线安装：安装包包含部署的相关镜像，因此安装包比较大\nHarbor部署HTTP 1、先安装Docker和Docker Compose https://github.com/docker/compose/releases\nmv docker-compose-Linux-x86_64 /usr/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/bin/docker-compose 2、部署Harbor HTTP https://github.com/goharbor/harbor/releases\ntar zxvf harbor-offline-installer-v2.0.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml vi harbor.yml hostname: reg.myharbor.com https: # 先注释https相关配置 harbor_admin_password: Harbor12345 ./prepare ./install.sh # 查看已安装的依赖容器 docker-compose ps # 重启docker-compose docker-compose down docker-compose up -d web界面登录：IP：80端口访问， 用户名 admin/Harbor12345\nhttp部署方式基本使用 1、配置http镜像仓库可信任\nvi /etc/docker/daemon.json {\u0026#34;insecure-registries\u0026#34;:[\u0026#34;reg.ctnrs.com\u0026#34;]} systemctl restart docker 2、打标签\ndocker tag centos:7 reg.myharbor.com/library/centos:7 3、上传\ndocker push reg.myharbor.com/library/centos:7 4、下载\ndocker pull reg.myharbor.com/library/centos:7 打标格式：仓库地址/项目名称/镜像名称：版本号\nHarbor 部署HTTPS 1、生成SSL证书 certs.sh cfssl.sh\n# 需要根据harbor设置的域名来修改certs.sh中域名部分 mkdir /root/ssl;cd /root/ssl;sh cfssl.sh sh certs.sh 2、Harbor启用HTTPS\nvi harbor.yml 修改以下内容 https: port: 443 certificate: /root/harbor/ssl/reg.ctnrs.com.pem private_key: /root/harbor/ssl/reg.ctnrs.com-key.pem 3、重新配置并部署Harbor\n./prepare docker-compose down docker-compose up –d 4、将数字证书复制到Docker主机\n# 都以harbor最初设置的域名来命名 mkdir /etc/docker/certs.d/reg.myharbor.com cp reg.myharbor.com.pem /etc/docker/certs.d/reg.myharbor.com/reg.myharbor.com.crt 5、验证 docker login reg.myharbor.com 会自动跳转443的。 ","permalink":"https://wandong1.github.io/post/%E4%BC%81%E4%B8%9A%E7%BA%A7%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93harbor/","summary":"概述 Harbor是由VMWare公司开源的容器镜像仓库。事实上，Harbor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制，AD/LDAP集成以及审计日志等，足以满足基本企业需求。 官方：https://goharbor.io/ Github：https://github.com/goharbor/harbor\n部署先决条件 服务器硬件配置： •最低要求：CPU2核/内存4G/硬盘40GB •推荐：CPU4核/内存8G/硬盘160GB 软件： •Docker CE 17.06版本+ •Docker Compose1.18版本+ Harbor安装有2种方式： •在线安装：从Docker Hub下载Harbor相关镜像，因此安装软件包非常小 •离线安装：安装包包含部署的相关镜像，因此安装包比较大\nHarbor部署HTTP 1、先安装Docker和Docker Compose https://github.com/docker/compose/releases\nmv docker-compose-Linux-x86_64 /usr/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/bin/docker-compose 2、部署Harbor HTTP https://github.com/goharbor/harbor/releases\ntar zxvf harbor-offline-installer-v2.0.0.tgz cd harbor cp harbor.yml.tmpl harbor.yml vi harbor.yml hostname: reg.myharbor.com https: # 先注释https相关配置 harbor_admin_password: Harbor12345 ./prepare ./install.sh # 查看已安装的依赖容器 docker-compose ps # 重启docker-compose docker-compose down docker-compose up -d web界面登录：IP：80端口访问， 用户名 admin/Harbor12345\nhttp部署方式基本使用 1、配置http镜像仓库可信任\nvi /etc/docker/daemon.json {\u0026#34;insecure-registries\u0026#34;:[\u0026#34;reg.","title":"企业级镜像仓库harbor"}]